{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea1a9cfa-a6fe-4da7-858f-6cc19d2b8b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client creating using default project: gcp-ushi-digital-ds-qa\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd \n",
    "\n",
    "client = bigquery.Client()\n",
    "print(\"Client creating using default project: {}\".format(client.project))\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT * \n",
    "FROM `gcp-ushi-digital-ds-qa.digital_ds.hansi_recs_search_data`;\n",
    "\"\"\"\n",
    "query_job = client.query(query)\n",
    "df = query_job.to_dataframe()\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT * \n",
    "FROM `gcp-ushi-digital-ds-qa.np_digital_ds.hansi_srch_recs_5_core_products`;\n",
    "\"\"\"\n",
    "query_job = client.query(query)\n",
    "core_prd_df = query_job.to_dataframe()\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT * \n",
    "FROM `gcp-ushi-digital-ds-qa.np_digital_ds.hansi_product_text_metas`;\n",
    "\"\"\"\n",
    "query_job = client.query(query)\n",
    "product_df = query_job.to_dataframe()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c47210e3-18a0-4054-97ba-663804827d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 1228136/1987188 [01:10<00:54, 13860.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old title = Square D Terminal\t Aluminum Ground Bar Kit, new title = Square D Terminal  Aluminum Ground Bar Kit\n",
      "old title = Square D 7 Terminal\t Aluminum Ground Bar Kit, new title = Square D 7 Terminal  Aluminum Ground Bar Kit\n",
      "old title = Blackburn 2 Terminal\t CopperBar, new title = Blackburn 2 Terminal  CopperBar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 1256057/1987188 [01:12<00:52, 13946.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old title = Blackburn 1 Terminal\t CopperBar, new title = Blackburn 1 Terminal  CopperBar\n",
      "old title = Eaton Terminal\t Galvanized Ground Bar Kit, new title = Eaton Terminal  Galvanized Ground Bar Kit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 1299847/1987188 [01:15<00:39, 17620.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old title = Blackburn 1 Terminal\t Copper Rods, new title = Blackburn 1 Terminal  Copper Rods\n",
      "old title = Eaton 5 Terminal\t Galvanized Ground Bar Kit, new title = Eaton 5 Terminal  Galvanized Ground Bar Kit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 1408101/1987188 [01:21<00:33, 17071.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old title = Square D 18 Terminal\t Aluminum Ground Bar Kit, new title = Square D 18 Terminal  Aluminum Ground Bar Kit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 1436550/1987188 [01:23<00:30, 17762.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old title = Square D 3 Terminal\t Aluminum Ground Bar Kit, new title = Square D 3 Terminal  Aluminum Ground Bar Kit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 1558556/1987188 [01:30<00:24, 17751.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old title = Square D 4 Terminal\t Aluminum Ground Bar Kit, new title = Square D 4 Terminal  Aluminum Ground Bar Kit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 1579985/1987188 [01:31<00:22, 17866.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old title = Eaton 21 Terminal\t Galvanized Ground Bar Kit, new title = Eaton 21 Terminal  Galvanized Ground Bar Kit\n",
      "old title = Eaton 14 Terminal\t Galvanized Ground Bar Kit, new title = Eaton 14 Terminal  Galvanized Ground Bar Kit\n",
      "old title = Blackburn 1 Terminal\t Copper Rods, new title = Blackburn 1 Terminal  Copper Rods\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1987188/1987188 [01:54<00:00, 17338.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of products = 1987188, products without title = 182, without bulletins = 778\n",
      "pid_to_title = Korky Red 3-in Flush Valve Seal for American Standard, Kohler \n",
      " pid_to_text = Korky Red 3-in Flush Valve Seal for American Standard, Kohler Replaces American Standard Seal: 738651-0070A Exclusive longest lasting red rubber resists chlorine Made in the USA Includes: (1) Kohler tower style seal, (1) American Standard&#174; tower style seal Replaces Kohler Seal: GP1059291 5 Year warranty Easy installation\n"
     ]
    }
   ],
   "source": [
    "# read all pids \n",
    "from tqdm import tqdm\n",
    "\n",
    "prdid_to_title = {}\n",
    "prdid_to_text = {}\n",
    "prdids = []\n",
    "no_bulletin_product = set()\n",
    "no_title_product = set()\n",
    "\n",
    "for idx, row in tqdm(product_df.iterrows(), total=len(product_df)):\n",
    "    product_id = row[\"product_id\"]\n",
    "    if row[\"product_name\"] == None:\n",
    "        no_title_product.add(product_id)\n",
    "        title = \"no title\"\n",
    "    else:\n",
    "        old_title = row[\"product_name\"]\n",
    "        title = old_title.replace(\"\\t\", \" \")\n",
    "        if old_title != title:\n",
    "            print(\"old title = {}, new title = {}\".format(old_title, title))\n",
    "    if row[\"bulletins\"] == None:\n",
    "        no_bulletin_product.add(product_id)\n",
    "        description = \"no description\"\n",
    "    else:\n",
    "        description = row[\"bulletins\"].replace(\"\\t\", \" \")\n",
    "        description = description.replace(\"\\n\", \" \")\n",
    "    \n",
    "    prdid_to_title[product_id] = title\n",
    "    prdid_to_text[product_id] = title + \" \" + description\n",
    "    prdids.append(product_id)\n",
    "\n",
    "assert len(prdids) == len(set(prdids))\n",
    "print(\"number of products = {}, products without title = {}, without bulletins = {}\".format(len(prdids), len(no_title_product), len(no_bulletin_product)))\n",
    "print(\"pid_to_title = {} \\n pid_to_text = {}\".format(prdid_to_title[product_id], prdid_to_text[product_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a27fc90a-c49f-4d6b-ab9a-aa8ac1557814",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 759677/759677 [01:38<00:00, 7676.11it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "from copy import deepcopy\n",
    "\n",
    "core_products = list(core_prd_df.ivm)\n",
    "core_products = {p: 1 for p in core_products} # hashmap \n",
    "\n",
    "all_products = {p: 1 for p in prdids} # hashmap\n",
    "\n",
    "remove_rec_examples = 0\n",
    "remove_search_examples = 0\n",
    "total_rec_examples = 0\n",
    "total_search_examples = 0\n",
    "remove_rows = 0\n",
    "remove_ivms = set()\n",
    "total_ivms = set()\n",
    "filtered_df = []\n",
    "for idx, df_row in tqdm(df.iterrows(), total=len(df)):\n",
    "    row = deepcopy(df_row)\n",
    "    new_rec_records = []\n",
    "    for rec_record in row[\"rec_record\"]:\n",
    "        total_rec_examples += 1\n",
    "        anchor, ivms = rec_record[\"anchor\"], rec_record[\"interacted_ivms\"]\n",
    "        if anchor not in all_products:\n",
    "            remove_rec_examples += 1\n",
    "            continue\n",
    "        new_ivms = []\n",
    "        for ivm in ivms:\n",
    "            total_ivms.add(ivm)\n",
    "            if ivm not in core_products:\n",
    "                remove_ivms.add(ivm)\n",
    "                continue\n",
    "            if ivm not in all_products:\n",
    "                remove_ivms.add(ivm)\n",
    "                continue\n",
    "            new_ivms.append(ivm)\n",
    "        if len(new_ivms) == 0:\n",
    "            remove_rec_examples += 1\n",
    "        else:\n",
    "            assert anchor in all_products\n",
    "            new_rec_records.append({\"anchor\": anchor, \"interacted_ivms\": new_ivms})\n",
    "            \n",
    "    new_search_records = []\n",
    "    for search_record in row[\"search_record\"]:\n",
    "        total_search_examples += 1\n",
    "        query, ivms = search_record[\"query\"], search_record[\"interacted_ivms\"]\n",
    "        new_ivms = []\n",
    "        for ivm in ivms:\n",
    "            total_ivms.add(ivm)\n",
    "            if ivm not in core_products:\n",
    "                remove_ivms.add(ivm)\n",
    "                continue\n",
    "            if ivm not in all_products:\n",
    "                remove_ivms.add(ivm)\n",
    "                continue\n",
    "            new_ivms.append(ivm)\n",
    "        if len(new_ivms) == 0:\n",
    "            remove_search_examples += 1\n",
    "        else:\n",
    "            new_search_records.append({\"query\": query, \"interacted_ivms\": new_ivms})\n",
    "            \n",
    "    row[\"rec_record\"] = new_rec_records\n",
    "    row[\"search_record\"] = new_search_records\n",
    "    if len(new_rec_records) == 0 and len(new_search_records) == 0:\n",
    "        remove_rows += 1\n",
    "        continue\n",
    "    filtered_df.append(row)\n",
    "    \n",
    "filtered_df = pd.DataFrame(data=filtered_df).sort_values(by=\"feed_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeef6b82-15a1-435f-8ac0-7a00b29bf79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove rec_example = 65827 (0.061), remove search example = 81082 (0.069)\n",
      "remove rows = 7970 (0.010), remove ivms = 66302 (0.294)\n",
      "original total rec example = 1087786, search example = 1179024, ivms = 225539\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "print(\"remove rec_example = {} ({:.3f}), remove search example = {} ({:.3f})\".format(remove_rec_examples, remove_rec_examples/total_rec_examples,\n",
    "                                                                                    remove_search_examples, remove_search_examples/total_search_examples))\n",
    "print(\"remove rows = {} ({:.3f}), remove ivms = {} ({:.3f})\".format(remove_rows, remove_rows/len(df),\n",
    "                                                                   len(remove_ivms), len(remove_ivms)/len(total_ivms)))\n",
    "print(\"original total rec example = {}, search example = {}, ivms = {}\".format(total_rec_examples, total_search_examples, len(total_ivms)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4819253e-e2d7-4b94-92e6-ab3cb0c2fd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 751707/751707 [01:17<00:00, 9692.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec_example = 1267877, search_example = 1420720\n",
      "# remove_imvs since not in all_products = 0.059\n"
     ]
    }
   ],
   "source": [
    "# create training examples \n",
    "rec_examples = []\n",
    "search_examples = []\n",
    "for idx, df_row in tqdm(filtered_df.iterrows(), total=len(filtered_df)):\n",
    "    row = deepcopy(df_row)\n",
    "    for rec_record in row[\"rec_record\"]:\n",
    "        anchor, ivms = rec_record[\"anchor\"], rec_record[\"interacted_ivms\"]\n",
    "        for ivm in ivms:\n",
    "            assert ivm in core_products and ivm in all_products and anchor in all_products, anchor\n",
    "            rec_examples.append({\"feed_date\": row[\"feed_date\"], \"feed_type\": row[\"feed_type\"], \"anchor\": anchor, \"product_id\": ivm})\n",
    "    \n",
    "    for search_record in row[\"search_record\"]:\n",
    "        query, ivms = search_record[\"query\"], search_record[\"interacted_ivms\"]\n",
    "        for ivm in ivms:\n",
    "            assert ivm in core_products and ivm in all_products\n",
    "            search_examples.append({\"feed_date\": row[\"feed_date\"], \"feed_type\": row[\"feed_type\"], \"query\": query, \"product_id\": ivm})\n",
    "\n",
    "print(\"rec_example = {}, search_example = {}\".format(len(rec_examples), len(search_examples)))\n",
    "print(\"# remove_imvs since not in all_products = {:.3f}\".format((len(total_ivms) - len(total_ivms & set(prdids))) / len(total_ivms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9659cb9-3caa-49c2-a936-29cf2ee20cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_date = 2022-03-05, end_date = 2022-05-06, val_start_date = 2022-04-24, test_start_date\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "unique_dates = np.unique(filtered_df.feed_date)\n",
    "start_date, end_date = unique_dates[0], unique_dates[-1]\n",
    "val_start_date = unique_dates[int(len(unique_dates)*0.8)]\n",
    "test_start_date = unique_dates[int(len(unique_dates)*0.9)]\n",
    "print(\"start_date = {}, end_date = {}, val_start_date = {}, test_start_date\".format(start_date, end_date, val_start_date, test_start_date))\n",
    "len(unique_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4f8fab6-6133-4b3b-9fc2-27d0ef337121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len pid_to_text = 1987188 pid_to_title = 1987188\n",
      "start_date = 2022-03-05, end_date = 2022-05-06, val_start_date = 2022-04-24, test_start_date\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000992/1000992 [00:02<00:00, 387949.79it/s]\n",
      "100%|██████████| 1118512/1118512 [00:03<00:00, 310884.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# write files \n",
    "import os \n",
    "import pickle\n",
    "\n",
    "import ujson\n",
    "import numpy as np\n",
    "\n",
    "base_dir = \"/home/jupyter/jointly_rec_and_search/datasets/jointly_rec_and_search/\"\n",
    "\n",
    "prdid_to_pid = {prdid: pid for pid, prdid in enumerate(prdids)}\n",
    "query_to_qid = {}\n",
    "for search_exp in search_examples:\n",
    "    cand_qid = len(query_to_qid)\n",
    "    query = search_exp[\"query\"]\n",
    "    if query in query_to_qid:\n",
    "        continue\n",
    "    query_to_qid[query] = cand_qid\n",
    "    \n",
    "# sanity check\n",
    "pid_to_prdid = {pid: prdid for prdid, pid in prdid_to_pid.items()}\n",
    "qid_to_query = {qid: query for query, qid in query_to_qid.items()}\n",
    "pid_to_text = {}\n",
    "pid_to_title = {}\n",
    "assert len(pid_to_prdid) == len(prdid_to_pid) and len(query_to_qid) == len(qid_to_query)\n",
    "with open(os.path.join(base_dir, \"all_queries.tsv\"), \"w\") as fout:\n",
    "    for qid, query in qid_to_query.items():\n",
    "        fout.write(f\"{qid}\\t{query}\\n\")\n",
    "with open(os.path.join(base_dir, \"pid_to_product_id.tsv\"), \"w\") as fout:\n",
    "    for pid, product_id in pid_to_prdid.items():\n",
    "        fout.write(f\"{pid}\\t{product_id}\\n\")\n",
    "with open(os.path.join(base_dir, \"pid_to_product_id.pkl\"), \"wb\") as fout:\n",
    "    pickle.dump(pid_to_prdid, fout)\n",
    "with open(os.path.join(base_dir, \"collection.tsv\"), \"w\") as fout:\n",
    "    with open(os.path.join(base_dir, \"collection_title.tsv\"), \"w\") as fout2:\n",
    "        for pid, product_id in pid_to_prdid.items():\n",
    "            text = prdid_to_text[product_id]\n",
    "            title = prdid_to_title[product_id]\n",
    "            fout.write(f\"{pid}\\t{text}\\n\")\n",
    "            fout2.write(f\"{pid}\\t{title}\\n\")\n",
    "            pid_to_text[pid] = text\n",
    "            pid_to_title[pid] = title\n",
    "        \n",
    "print(\"len pid_to_text = {} pid_to_title = {}\".format(len(pid_to_text), len(pid_to_title)))\n",
    "# split train, val, test set \n",
    "unique_dates = np.unique(filtered_df.feed_date)\n",
    "start_date, end_date = unique_dates[0], unique_dates[-1]\n",
    "val_start_date = unique_dates[int(len(unique_dates)*0.8)]\n",
    "test_start_date = unique_dates[int(len(unique_dates)*0.9)]\n",
    "print(\"start_date = {}, end_date = {}, val_start_date = {}, test_start_date\".format(start_date, end_date, val_start_date, test_start_date))\n",
    "\n",
    "train_rec_examples, val_rec_examples, test_rec_examples = [], [], []\n",
    "for rec_row in rec_examples:\n",
    "    rec_exp = {\"feed_date\": rec_row[\"feed_date\"], \"feed_type\": rec_row[\"feed_type\"], \"anchor_pid\": prdid_to_pid[rec_row[\"anchor\"]],\n",
    "                \"pid\": prdid_to_pid[rec_row[\"product_id\"]]}\n",
    "    if rec_exp[\"feed_date\"] < val_start_date:\n",
    "        rec_exp[\"feed_date\"] = rec_exp[\"feed_date\"].strftime(\"%Y-%m-%d\")\n",
    "        train_rec_examples.append(rec_exp)\n",
    "    elif rec_exp[\"feed_date\"] >= val_start_date and rec_exp[\"feed_date\"] < test_start_date:\n",
    "        rec_exp[\"feed_date\"] = rec_exp[\"feed_date\"].strftime(\"%Y-%m-%d\")\n",
    "        val_rec_examples.append(rec_exp)\n",
    "    else:\n",
    "        assert rec_exp[\"feed_date\"] >= test_start_date\n",
    "        rec_exp[\"feed_date\"] = rec_exp[\"feed_date\"].strftime(\"%Y-%m-%d\")\n",
    "        test_rec_examples.append(rec_exp)\n",
    "assert len(train_rec_examples) + len(val_rec_examples) + len(test_rec_examples) == len(rec_examples), (len(train_rec_examples), len(val_rec_examples), len(test_rec_examples))\n",
    "\n",
    "\n",
    "train_search_examples, val_search_examples, test_search_examples = [], [], []\n",
    "for search_row in search_examples:\n",
    "    search_exp = {\"feed_date\": search_row[\"feed_date\"], \"feed_type\": search_row[\"feed_type\"], \"qid\": query_to_qid[search_row[\"query\"]],\n",
    "                 \"pid\": prdid_to_pid[search_row[\"product_id\"]]}\n",
    "    if search_exp[\"feed_date\"] < val_start_date:\n",
    "        search_exp[\"feed_date\"] = search_exp[\"feed_date\"].strftime(\"%Y-%m-%d\")\n",
    "        train_search_examples.append(search_exp)\n",
    "    elif search_exp[\"feed_date\"] >= val_start_date and search_exp[\"feed_date\"] < test_start_date:\n",
    "        search_exp[\"feed_date\"] = search_exp[\"feed_date\"].strftime(\"%Y-%m-%d\")\n",
    "        val_search_examples.append(search_exp)\n",
    "    else:\n",
    "        assert search_exp[\"feed_date\"] >= test_start_date\n",
    "        search_exp[\"feed_date\"] = search_exp[\"feed_date\"].strftime(\"%Y-%m-%d\")\n",
    "        test_search_examples.append(search_exp)\n",
    "assert len(train_search_examples) + len(val_search_examples) + len(test_search_examples) == len(search_examples), (len(train_search_examples),len(val_search_examples),len(test_search_examples))\n",
    "\n",
    "anchor_pid_dict = set()\n",
    "anchor_qrels = {}\n",
    "with open(os.path.join(base_dir, \"train_rec_examples.json\"), \"w\") as fout:\n",
    "    with open(os.path.join(base_dir, \"train_anchors.tsv\"), \"w\") as fout2:\n",
    "        with open(os.path.join(base_dir, \"train_anchor_qrels.tsv\"), \"w\") as fout3:\n",
    "            with open(os.path.join(base_dir, \"train_anchor_qrels_clicknum.tsv\"), \"w\") as fout4:\n",
    "                for rec_exp in tqdm(train_rec_examples):\n",
    "                    fout.write(ujson.dumps(rec_exp) + \"\\n\")\n",
    "\n",
    "                    anchor_pid = rec_exp[\"anchor_pid\"]\n",
    "                    if anchor_pid in anchor_pid_dict:\n",
    "                        pass \n",
    "                    else:\n",
    "                        anchor_pid_dict.add(anchor_pid)\n",
    "                        title = pid_to_title[anchor_pid]\n",
    "                        fout2.write(f\"{anchor_pid}\\t{title}\\n\")\n",
    "\n",
    "                    if anchor_pid not in anchor_qrels:\n",
    "                        anchor_qrels[anchor_pid] = {pid: 1}\n",
    "                    else:\n",
    "                        if pid not in anchor_qrels[anchor_pid]:\n",
    "                            anchor_qrels[anchor_pid][pid] = 1\n",
    "                        else:\n",
    "                            anchor_qrels[anchor_pid][pid] += 1\n",
    "                \n",
    "                for anchor_pid in anchor_qrels:\n",
    "                    for pid in anchor_qrels[anchor_pid]:\n",
    "                        click_num = anchor_qrels[anchor_pid][pid]\n",
    "                        fout3.write(f\"{anchor_pid}\\tQ0\\t{pid}\\t{1}\\n\")\n",
    "                        fout4.write(f\"{anchor_pid}\\t{pid}\\t{click_num}\\n\")\n",
    "\n",
    "anchor_pid_dict = set()\n",
    "anchor_qrels = {}\n",
    "with open(os.path.join(base_dir, \"val_rec_examples.json\"), \"w\") as fout:\n",
    "    with open(os.path.join(base_dir, \"val_anchors.tsv\"), \"w\") as fout2:\n",
    "        with open(os.path.join(base_dir, \"val_anchor_qrels.tsv\"), \"w\") as fout3:\n",
    "            with open(os.path.join(base_dir, \"val_anchor_qrels_clicknum.tsv\"), \"w\") as fout4:\n",
    "                for rec_exp in val_rec_examples:\n",
    "                    fout.write(ujson.dumps(rec_exp) + \"\\n\")\n",
    "\n",
    "                    anchor_pid = rec_exp[\"anchor_pid\"]\n",
    "                    if anchor_pid in anchor_pid_dict:\n",
    "                        pass\n",
    "                    else:\n",
    "                        anchor_pid_dict.add(anchor_pid)\n",
    "                        title = pid_to_title[anchor_pid]\n",
    "                        fout2.write(f\"{anchor_pid}\\t{title}\\n\")\n",
    "\n",
    "                    if anchor_pid not in anchor_qrels:\n",
    "                        anchor_qrels[anchor_pid] = {pid: 1}\n",
    "                    else:\n",
    "                        if pid not in anchor_qrels[anchor_pid]:\n",
    "                            anchor_qrels[anchor_pid][pid] = 1\n",
    "                        else:\n",
    "                            anchor_qrels[anchor_pid][pid] += 1\n",
    "                \n",
    "                for anchor_pid in anchor_qrels:\n",
    "                    for pid in anchor_qrels[anchor_pid]:\n",
    "                        click_num = anchor_qrels[anchor_pid][pid]\n",
    "                        fout3.write(f\"{anchor_pid}\\tQ0\\t{pid}\\t{1}\\n\")\n",
    "                        fout4.write(f\"{anchor_pid}\\t{pid}\\t{click_num}\\n\")\n",
    "                \n",
    "anchor_pid_dict = set()\n",
    "anchor_qrels = {}\n",
    "with open(os.path.join(base_dir, \"test_rec_examples.json\"), \"w\") as fout:\n",
    "    with open(os.path.join(base_dir, \"test_anchors.tsv\"), \"w\") as fout2:\n",
    "        with open(os.path.join(base_dir, \"test_anchor_qrels.tsv\"), \"w\") as fout3:\n",
    "            with open(os.path.join(base_dir, \"test_anchor_qrels_clicknum.tsv\"), \"w\") as fout4:\n",
    "                for rec_exp in test_rec_examples:\n",
    "                    fout.write(ujson.dumps(rec_exp) + \"\\n\")\n",
    "\n",
    "                    anchor_pid = rec_exp[\"anchor_pid\"]\n",
    "                    if anchor_pid in anchor_pid_dict:\n",
    "                        pass\n",
    "                    else:\n",
    "                        anchor_pid_dict.add(anchor_pid)\n",
    "                        title = pid_to_title[anchor_pid]\n",
    "                        fout2.write(f\"{anchor_pid}\\t{title}\\n\")\n",
    "\n",
    "                    if anchor_pid not in anchor_qrels:\n",
    "                            anchor_qrels[anchor_pid] = {pid: 1}\n",
    "                    else:\n",
    "                        if pid not in anchor_qrels[anchor_pid]:\n",
    "                            anchor_qrels[anchor_pid][pid] = 1\n",
    "                        else:\n",
    "                            anchor_qrels[anchor_pid][pid] += 1\n",
    "\n",
    "                for anchor_pid in anchor_qrels:\n",
    "                    for pid in anchor_qrels[anchor_pid]:\n",
    "                        click_num = anchor_qrels[anchor_pid][pid]\n",
    "                        fout3.write(f\"{anchor_pid}\\tQ0\\t{pid}\\t{1}\\n\")\n",
    "                        fout4.write(f\"{anchor_pid}\\t{pid}\\t{click_num}\\n\")\n",
    "\n",
    "qid_dict = set()\n",
    "qid_qrels = {}\n",
    "with open(os.path.join(base_dir, \"train_search_examples.json\"), \"w\") as fout:\n",
    "    with open(os.path.join(base_dir,\"train_queries.tsv\"), \"w\") as fout2:\n",
    "        with open(os.path.join(base_dir,\"train_query_qrels.tsv\"), \"w\") as fout3:\n",
    "            with open(os.path.join(base_dir,\"train_query_qrels_clicknum.tsv\"), \"w\") as fout4:\n",
    "                for search_exp in tqdm(train_search_examples):\n",
    "                    fout.write(ujson.dumps(search_exp) + \"\\n\")\n",
    "\n",
    "                    qid = search_exp[\"qid\"]\n",
    "                    if qid in qid_dict:\n",
    "                        pass\n",
    "                    else:\n",
    "                        qid_dict.add(qid)\n",
    "                        query = qid_to_query[qid]\n",
    "                        fout2.write(f\"{qid}\\t{query}\\n\")\n",
    "\n",
    "                    pid = search_exp[\"pid\"]\n",
    "                    fout3.write(f\"{qid}\\tQ0\\t{pid}\\t{1}\\n\")\n",
    "                    \n",
    "                    if qid not in qid_qrels:\n",
    "                        qid_qrels[qid] = {pid: 1}\n",
    "                    else:\n",
    "                        if pid not in qid_qrels[qid]:\n",
    "                            qid_qrels[qid][pid] = 1\n",
    "                        else:\n",
    "                            qid_qrels[qid][pid] += 1\n",
    "                \n",
    "                for qid in qid_qrels:\n",
    "                    for pid in qid_qrels[qid]:\n",
    "                        click_num = qid_qrels[qid][pid]\n",
    "                        fout3.write(f\"{qid}\\tQ0\\t{pid}\\t{1}\\n\")\n",
    "                        fout4.write(f\"{qid}\\t{pid}\\t{click_num}\\n\")\n",
    "                \n",
    "qid_dict = set()\n",
    "qid_qrels = {}\n",
    "with open(os.path.join(base_dir, \"val_search_examples.json\"), \"w\") as fout:\n",
    "    with open(os.path.join(base_dir,\"val_queries.tsv\"), \"w\") as fout2:\n",
    "        with open(os.path.join(base_dir,\"val_query_qrels.tsv\"), \"w\") as fout3:\n",
    "            with open(os.path.join(base_dir,\"val_query_qrels_clicknum.tsv\"), \"w\") as fout4:\n",
    "                for search_exp in val_search_examples:\n",
    "                    fout.write(ujson.dumps(search_exp) + \"\\n\")\n",
    "\n",
    "                    qid = search_exp[\"qid\"]\n",
    "                    if qid in qid_dict:\n",
    "                        pass\n",
    "                    else:\n",
    "                        qid_dict.add(qid)\n",
    "                        query = qid_to_query[qid]\n",
    "                        fout2.write(f\"{qid}\\t{query}\\n\")\n",
    "\n",
    "                    if qid not in qid_qrels:\n",
    "                        qid_qrels[qid] = {pid: 1}\n",
    "                    else:\n",
    "                        if pid not in qid_qrels[qid]:\n",
    "                            qid_qrels[qid][pid] = 1\n",
    "                        else:\n",
    "                            qid_qrels[qid][pid] += 1\n",
    "                \n",
    "                for qid in qid_qrels:\n",
    "                    for pid in qid_qrels[qid]:\n",
    "                        click_num = qid_qrels[qid][pid]\n",
    "                        fout3.write(f\"{qid}\\tQ0\\t{pid}\\t{1}\\n\")\n",
    "                        fout4.write(f\"{qid}\\t{pid}\\t{click_num}\\n\")\n",
    "                \n",
    "qid_dict = set()\n",
    "qid_qrels = {}\n",
    "with open(os.path.join(base_dir, \"test_search_examples.json\"), \"w\") as fout:\n",
    "    with open(os.path.join(base_dir,\"test_queries.tsv\"), \"w\") as fout2:\n",
    "        with open(os.path.join(base_dir,\"test_query_qrels.tsv\"), \"w\") as fout3:\n",
    "            with open(os.path.join(base_dir,\"test_query_qrels_clicknum.tsv\"), \"w\") as fout4:\n",
    "                for search_exp in test_search_examples:\n",
    "                    fout.write(ujson.dumps(search_exp) + \"\\n\")\n",
    "\n",
    "                    qid = search_exp[\"qid\"]\n",
    "                    if qid in qid_dict:\n",
    "                        pass\n",
    "                    else:\n",
    "                        qid_dict.add(qid)\n",
    "                        query = qid_to_query[qid]\n",
    "                        fout2.write(f\"{qid}\\t{query}\\n\")\n",
    "\n",
    "                    if qid not in qid_qrels:\n",
    "                        qid_qrels[qid] = {pid: 1}\n",
    "                    else:\n",
    "                        if pid not in qid_qrels[qid]:\n",
    "                            qid_qrels[qid][pid] = 1\n",
    "                        else:\n",
    "                            qid_qrels[qid][pid] += 1\n",
    "                \n",
    "                for qid in qid_qrels:\n",
    "                    for pid in qid_qrels[qid]:\n",
    "                        click_num = qid_qrels[qid][pid]\n",
    "                        fout3.write(f\"{qid}\\tQ0\\t{pid}\\t{1}\\n\")\n",
    "                        fout4.write(f\"{qid}\\t{pid}\\t{click_num}\\n\")\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e4909d8-46b3-4c33-9ba6-b0372e865801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000992 /home/jupyter/jointly_rec_and_search/datasets/jointly_rec_and_search/train_rec_examples.json\n",
      "{\"feed_date\":\"2022-03-05\",\"feed_type\":\"desktop\",\"anchor_pid\":1908294,\"pid\":1691387}\n",
      "{\"feed_date\":\"2022-03-05\",\"feed_type\":\"desktop\",\"anchor_pid\":1684969,\"pid\":1810591}\n",
      "{\"feed_date\":\"2022-03-05\",\"feed_type\":\"desktop\",\"anchor_pid\":1745463,\"pid\":1545588}\n",
      "{\"feed_date\":\"2022-04-23\",\"feed_type\":\"desktop\",\"anchor_pid\":1726171,\"pid\":1884870}\n",
      "{\"feed_date\":\"2022-04-23\",\"feed_type\":\"desktop\",\"anchor_pid\":1726171,\"pid\":1957647}\n",
      "{\"feed_date\":\"2022-04-23\",\"feed_type\":\"mobile\",\"anchor_pid\":945617,\"pid\":945616}\n",
      "====================================================================================================\n",
      "130335 /home/jupyter/jointly_rec_and_search/datasets/jointly_rec_and_search/val_rec_examples.json\n",
      "{\"feed_date\":\"2022-04-24\",\"feed_type\":\"desktop\",\"anchor_pid\":983902,\"pid\":732894}\n",
      "{\"feed_date\":\"2022-04-24\",\"feed_type\":\"desktop\",\"anchor_pid\":1891227,\"pid\":1891228}\n",
      "{\"feed_date\":\"2022-04-24\",\"feed_type\":\"mobile\",\"anchor_pid\":770457,\"pid\":888654}\n",
      "{\"feed_date\":\"2022-04-29\",\"feed_type\":\"desktop\",\"anchor_pid\":775879,\"pid\":711109}\n",
      "{\"feed_date\":\"2022-04-29\",\"feed_type\":\"desktop\",\"anchor_pid\":1091770,\"pid\":899287}\n",
      "{\"feed_date\":\"2022-04-29\",\"feed_type\":\"desktop\",\"anchor_pid\":1722156,\"pid\":1722164}\n",
      "====================================================================================================\n",
      "136550 /home/jupyter/jointly_rec_and_search/datasets/jointly_rec_and_search/test_rec_examples.json\n",
      "{\"feed_date\":\"2022-04-30\",\"feed_type\":\"desktop\",\"anchor_pid\":1959706,\"pid\":1673201}\n",
      "{\"feed_date\":\"2022-04-30\",\"feed_type\":\"desktop\",\"anchor_pid\":851060,\"pid\":797950}\n",
      "{\"feed_date\":\"2022-04-30\",\"feed_type\":\"desktop\",\"anchor_pid\":851060,\"pid\":963651}\n",
      "{\"feed_date\":\"2022-05-06\",\"feed_type\":\"desktop\",\"anchor_pid\":1065574,\"pid\":1100537}\n",
      "{\"feed_date\":\"2022-05-06\",\"feed_type\":\"desktop\",\"anchor_pid\":1726533,\"pid\":1939574}\n",
      "{\"feed_date\":\"2022-05-06\",\"feed_type\":\"mobile\",\"anchor_pid\":239721,\"pid\":229201}\n",
      "====================================================================================================\n",
      "1118512 /home/jupyter/jointly_rec_and_search/datasets/jointly_rec_and_search/train_search_examples.json\n",
      "{\"feed_date\":\"2022-03-05\",\"feed_type\":\"desktop\",\"qid\":0,\"pid\":1691409}\n",
      "{\"feed_date\":\"2022-03-05\",\"feed_type\":\"desktop\",\"qid\":0,\"pid\":1691391}\n",
      "{\"feed_date\":\"2022-03-05\",\"feed_type\":\"desktop\",\"qid\":1,\"pid\":1684969}\n",
      "{\"feed_date\":\"2022-04-23\",\"feed_type\":\"desktop\",\"qid\":305836,\"pid\":1834268}\n",
      "{\"feed_date\":\"2022-04-23\",\"feed_type\":\"desktop\",\"qid\":84289,\"pid\":1974179}\n",
      "{\"feed_date\":\"2022-04-23\",\"feed_type\":\"mobile\",\"qid\":1210,\"pid\":1556558}\n",
      "====================================================================================================\n",
      "149020 /home/jupyter/jointly_rec_and_search/datasets/jointly_rec_and_search/val_search_examples.json\n",
      "{\"feed_date\":\"2022-04-24\",\"feed_type\":\"desktop\",\"qid\":4982,\"pid\":1337707}\n",
      "{\"feed_date\":\"2022-04-24\",\"feed_type\":\"desktop\",\"qid\":4982,\"pid\":1480441}\n",
      "{\"feed_date\":\"2022-04-24\",\"feed_type\":\"desktop\",\"qid\":4982,\"pid\":1797629}\n",
      "{\"feed_date\":\"2022-04-29\",\"feed_type\":\"desktop\",\"qid\":718,\"pid\":1091770}\n",
      "{\"feed_date\":\"2022-04-29\",\"feed_type\":\"desktop\",\"qid\":10572,\"pid\":1722164}\n",
      "{\"feed_date\":\"2022-04-29\",\"feed_type\":\"desktop\",\"qid\":3119,\"pid\":1499944}\n",
      "====================================================================================================\n",
      "153188 /home/jupyter/jointly_rec_and_search/datasets/jointly_rec_and_search/test_search_examples.json\n",
      "{\"feed_date\":\"2022-04-30\",\"feed_type\":\"desktop\",\"qid\":8153,\"pid\":1959706}\n",
      "{\"feed_date\":\"2022-04-30\",\"feed_type\":\"desktop\",\"qid\":336718,\"pid\":791476}\n",
      "{\"feed_date\":\"2022-04-30\",\"feed_type\":\"desktop\",\"qid\":336718,\"pid\":963651}\n",
      "{\"feed_date\":\"2022-05-06\",\"feed_type\":\"desktop\",\"qid\":32733,\"pid\":1013305}\n",
      "{\"feed_date\":\"2022-05-06\",\"feed_type\":\"mobile\",\"qid\":3573,\"pid\":1794333}\n",
      "{\"feed_date\":\"2022-05-06\",\"feed_type\":\"mobile\",\"qid\":3573,\"pid\":1794368}\n",
      "====================================================================================================\n",
      "all search examples = 1420720\n",
      "1987188 /home/jupyter/jointly_rec_and_search/datasets/jointly_rec_and_search/collection.tsv\n",
      "0\tKobalt 7-in 6.5-Amp Wet Tabletop Tile Saw 7-in wet tabletop tile saw is designed to rip cut 18-in tile and diagonally cut 12-in tile with a 1-1/4-in max depth of cut Includes a 7-inch blade and 2 AAA batteries for the laser - an optional stand is sold separately, online only Aluminum extension table supports larger tile projects and increases rip capacity up to 26-in Built-in laser guide provides precise alignment Table tilts for accurate 22.5° and 45° bevel cuts Miter gauge slides for precise cuts up to 45°, so you can begin at your desired angle without worrying about creeping during cuts 20-in aluminum rip fence slides and locks securely to ensure accurate and clean cuts Includes 7-in blade and 2 AAA batteries for the laser 6.5-amp motor generates up to 3,450-RPM to easily cut ceramic, porcelain, natural stone and glass/mosaic tile\n",
      "1\tSo Phresh So Phresh X-Large Leak Guard Quilted Potty Pads, Count of 10 Pads feature attractant making for great dog training pads for puppies X-Large Leak Guard Dog Pads by So Phresh Non-skid backing ensures dog pad stays where it has been placed to protect your floor Five ultra-absorbent layers soak hold up to five cups of liquid to help protect your floors Quick-dry technology in the top layer of these dog pee pads helps prevent tracking of wet paws\n",
      "2\tReliaBilt 150 Series 31.5-in x 35.5-in x 3.25-in Jamb Between The Glass Vinyl New Construction White Single Hung Window Half The most popular of the ReliaBilt new construction single hung windows, the Series 150 showcases a brick mould exterior to complement any home&#8217;s style The bottom sash tilts in and operates easily with both an integrated, full-length, slim-line lift rail, and block and tackle balance system Window comes standard in white and includes a half screen; additional sizes, factory mulling, glass, wood jamb extensions, grid and colors are available; see your local sales associate for available custom options DP-50 rated (Design Pressure rating) on windows up to 36-in W x 74-in H All ReliaBilt windows are manufactured exclusively for Lowe's and made proudly in the USA Features energy-efficient Low-E glass to help reflect the sun&#8217;s heat back away from your home in the warm summer months and to help hold the warmth inside your home during the colder winter months Designed for new construction and remodeling applications, the window installs easily with a pre-punched, integrated nail fin and J-channel included (Series 160 available without J-channel) Standard manufacturer&#8217;s Limited Lifetime Warranty included on all vinyl materials and all parts under normal use, as well as a 25-year prorated warranty against seal failure (see actual warranty for details) Features a 5/8-in contoured grid pattern between the glass panes to enhance the style and appearance of your home without creating the extra cleaning complexities of grids outside the panes\n",
      "1987185\tSloan 952 Series Flush Valve Brass finish ADA complaint non-hold-open feature type actuator Bak-Chek angle stop 1 in IPS wheel handle Double slip elbow flush connection and spud coupling for 1-1/2-in concealed back spud For urinal applications - Royal series hydraulic actuator Adjustable tail piece Manual flush valve, concealed hydraulically operated Length: 2 inches to 10-3/4 inches This closet flush valve operates at 1.6 gallons per flush\n",
      "1987186\tMansfield 3-in Dual Flush Tower - Summit DF 3-in flush valve 1.1 gpf and 1.6 gpf low water consumption Fits Mansfield Summit dual flush toilets (tank model 3386) Dual flush\n",
      "1987187\tKorky Red 3-in Flush Valve Seal for American Standard, Kohler Replaces American Standard Seal: 738651-0070A Exclusive longest lasting red rubber resists chlorine Made in the USA Includes: (1) Kohler tower style seal, (1) American Standard&#174; tower style seal Replaces Kohler Seal: GP1059291 5 Year warranty Easy installation\n",
      "====================================================================================================\n",
      "1987188 /home/jupyter/jointly_rec_and_search/datasets/jointly_rec_and_search/collection_title.tsv\n",
      "0\tKobalt 7-in 6.5-Amp Wet Tabletop Tile Saw\n",
      "1\tSo Phresh So Phresh X-Large Leak Guard Quilted Potty Pads, Count of 10\n",
      "2\tReliaBilt 150 Series 31.5-in x 35.5-in x 3.25-in Jamb Between The Glass Vinyl New Construction White Single Hung Window Half\n",
      "1987185\tSloan 952 Series Flush Valve\n",
      "1987186\tMansfield 3-in Dual Flush Tower - Summit DF\n",
      "1987187\tKorky Red 3-in Flush Valve Seal for American Standard, Kohler\n",
      "====================================================================================================\n",
      "367750 /home/jupyter/jointly_rec_and_search/datasets/jointly_rec_and_search/all_queries.tsv\n",
      "0\t2x2 pressure treated\n",
      "1\tkitchenaid dishwasher\n",
      "2\tfurniture sliders\n",
      "367747\twing nut 3/8\n",
      "367748\theavy cement pots\n",
      "367749\tevolver black ste3el sunken base post\n",
      "====================================================================================================\n",
      "305837 /home/jupyter/jointly_rec_and_search/datasets/jointly_rec_and_search/train_queries.tsv\n",
      "0\t2x2 pressure treated\n",
      "1\tkitchenaid dishwasher\n",
      "2\tfurniture sliders\n",
      "305834\t30 bathroom vanity with sink\n",
      "305835\t4 foot welded wire rolled fencing\n",
      "305836\tdouble door bottom freezer\n",
      "====================================================================================================\n",
      "60305 /home/jupyter/jointly_rec_and_search/datasets/jointly_rec_and_search/val_queries.tsv\n",
      "4982\ttree stakes\n",
      "85659\t20-20-20 plant food\n",
      "51794\ttree fertilizer\n",
      "59006\tfranklin stud finder\n",
      "336717\tber apint\n",
      "223131\tcontractor clipboard\n",
      "====================================================================================================\n",
      "62032 /home/jupyter/jointly_rec_and_search/datasets/jointly_rec_and_search/test_queries.tsv\n",
      "8153\twashing machine outlet box\n",
      "336718\toutdoor pest control spray\n",
      "124\tbathroom vanity\n",
      "367748\theavy cement pots\n",
      "367749\tevolver black ste3el sunken base post\n",
      "32733\tround outdoor table\n",
      "====================================================================================================\n",
      "1739780 /home/jupyter/jointly_rec_and_search/datasets/jointly_rec_and_search/train_query_qrels.tsv\n",
      "0\tQ0\t1691409\t1\n",
      "0\tQ0\t1691391\t1\n",
      "1\tQ0\t1684969\t1\n",
      "305835\tQ0\t1329353\t1\n",
      "305835\tQ0\t1398835\t1\n",
      "305836\tQ0\t1834268\t1\n",
      "====================================================================================================\n",
      "60305 /home/jupyter/jointly_rec_and_search/datasets/jointly_rec_and_search/val_query_qrels.tsv\n",
      "4982\tQ0\t1834268\t1\n",
      "85659\tQ0\t1834268\t1\n",
      "51794\tQ0\t1834268\t1\n",
      "59006\tQ0\t1834268\t1\n",
      "336717\tQ0\t1834268\t1\n",
      "223131\tQ0\t1834268\t1\n",
      "====================================================================================================\n",
      "62032 /home/jupyter/jointly_rec_and_search/datasets/jointly_rec_and_search/test_query_qrels.tsv\n",
      "8153\tQ0\t1834268\t1\n",
      "336718\tQ0\t1834268\t1\n",
      "124\tQ0\t1834268\t1\n",
      "367748\tQ0\t1834268\t1\n",
      "367749\tQ0\t1834268\t1\n",
      "32733\tQ0\t1834268\t1\n",
      "====================================================================================================\n",
      "621268 /home/jupyter/jointly_rec_and_search/datasets/jointly_rec_and_search/train_query_qrels_clicknum.tsv\n",
      "0\t1691409\t16\n",
      "0\t1691391\t23\n",
      "0\t1722136\t6\n",
      "305835\t1329353\t1\n",
      "305835\t1398835\t1\n",
      "305836\t1834268\t1\n",
      "====================================================================================================\n",
      "60305 /home/jupyter/jointly_rec_and_search/datasets/jointly_rec_and_search/val_query_qrels_clicknum.tsv\n",
      "4982\t1834268\t17\n",
      "85659\t1834268\t4\n",
      "51794\t1834268\t3\n",
      "59006\t1834268\t1\n",
      "336717\t1834268\t1\n",
      "223131\t1834268\t1\n",
      "====================================================================================================\n",
      "62032 /home/jupyter/jointly_rec_and_search/datasets/jointly_rec_and_search/test_query_qrels_clicknum.tsv\n",
      "8153\t1834268\t5\n",
      "336718\t1834268\t2\n",
      "124\t1834268\t153\n",
      "367748\t1834268\t1\n",
      "367749\t1834268\t1\n",
      "32733\t1834268\t1\n",
      "====================================================================================================\n",
      "143550 /home/jupyter/jointly_rec_and_search/datasets/jointly_rec_and_search/train_anchors.tsv\n",
      "1908294\tSevere Weather 6-in x 6-in x 12-ft #2 Pressure Treated Lumber\n",
      "1684969\tKitchenAid FREEFLEX Third Rack 44-Decibel Top Control 24-in Built-In Dishwasher (Black Stainless Steel with Printshield) ENERGY STAR\n",
      "1745463\tScotch 4-Pack 5 In Round Felt Hard Surface Furniture Slider\n",
      "1362203\tQualGear Pro-Av Projector Mount Kit with a Vaulted Ceiling Adapter, 1.5-in NPT Pipe 3-in, Black\n",
      "1362196\tQualGear Universal Low-Profile Projector Ceiling Mount, Black\n",
      "1726171\tEasy Kleen 24-in 4000 PSI Scrubbing Broom for Gas and Electric Pressure Washers\n",
      "====================================================================================================\n",
      "45388 /home/jupyter/jointly_rec_and_search/datasets/jointly_rec_and_search/val_anchors.tsv\n",
      "983902\tGarden Safe Brand Diatomaceous Earth 4-lb Insect Killer\n",
      "1891227\tUSG 4.5-Gallon Premixed Lightweight Drywall Joint Compound\n",
      "770457\tPermaBASE 3-ft x 5-ft x 1/2-in Cement Water Resistant Backer Board\n",
      "1072492\tallen + roth Cushioned bench 52.75-in W x 34-in H Black Steel Traditional Bench\n",
      "711109\tevekare Comfort grip grab bar Black Wall Mount (Ada Compliant) Grab Bar (550-lb Weight Capacity)\n",
      "775879\tevekare Comfort grip grab bar Black Wall Mount (Ada Compliant) Grab Bar (550-lb Weight Capacity)\n",
      "====================================================================================================\n",
      "46587 /home/jupyter/jointly_rec_and_search/datasets/jointly_rec_and_search/test_anchors.tsv\n",
      "1959706\tOatey Quarter Turn Ball Valve PEX Washing Machine Outlet Box\n",
      "851060\tORTHO Insect Mite and Disease 3-in-1-Gallon Insect; Disease and Mite Control Trigger Spray\n",
      "1850056\tallen + roth Port 27-in Navy Bathroom Vanity Cabinet\n",
      "1275191\tBon Tool 10 ft. Aluminum Alloy Straightedge Manual Concrete Screed\n",
      "1958086\tDesign Toscano S/2 Thornbury Window Trellises\n",
      "1767606\tProgress Lighting 6-in White Shower Recessed Light Trim\n",
      "====================================================================================================\n",
      "143550 /home/jupyter/jointly_rec_and_search/datasets/jointly_rec_and_search/train_anchor_qrels.tsv\n",
      "1908294\tQ0\t1987187\t1\n",
      "1684969\tQ0\t1987187\t1\n",
      "1745463\tQ0\t1987187\t1\n",
      "1362203\tQ0\t1987187\t1\n",
      "1362196\tQ0\t1987187\t1\n",
      "1726171\tQ0\t1987187\t1\n",
      "====================================================================================================\n",
      "45388 /home/jupyter/jointly_rec_and_search/datasets/jointly_rec_and_search/val_anchor_qrels.tsv\n",
      "983902\tQ0\t1987187\t1\n",
      "1891227\tQ0\t1987187\t1\n",
      "770457\tQ0\t1987187\t1\n",
      "1072492\tQ0\t1987187\t1\n",
      "711109\tQ0\t1987187\t1\n",
      "775879\tQ0\t1987187\t1\n",
      "====================================================================================================\n",
      "46587 /home/jupyter/jointly_rec_and_search/datasets/jointly_rec_and_search/test_anchor_qrels.tsv\n",
      "1959706\tQ0\t1987187\t1\n",
      "851060\tQ0\t1987187\t1\n",
      "1850056\tQ0\t1987187\t1\n",
      "1275191\tQ0\t1987187\t1\n",
      "1958086\tQ0\t1987187\t1\n",
      "1767606\tQ0\t1987187\t1\n",
      "====================================================================================================\n",
      "143550 /home/jupyter/jointly_rec_and_search/datasets/jointly_rec_and_search/train_anchor_qrels_clicknum.tsv\n",
      "1908294\t1987187\t480\n",
      "1684969\t1987187\t30\n",
      "1745463\t1987187\t8\n",
      "1362203\t1987187\t2\n",
      "1362196\t1987187\t2\n",
      "1726171\t1987187\t2\n",
      "====================================================================================================\n",
      "45388 /home/jupyter/jointly_rec_and_search/datasets/jointly_rec_and_search/val_anchor_qrels_clicknum.tsv\n",
      "983902\t1987187\t5\n",
      "1891227\t1987187\t8\n",
      "770457\t1987187\t2\n",
      "1072492\t1987187\t1\n",
      "711109\t1987187\t1\n",
      "775879\t1987187\t1\n",
      "====================================================================================================\n",
      "46587 /home/jupyter/jointly_rec_and_search/datasets/jointly_rec_and_search/test_anchor_qrels_clicknum.tsv\n",
      "1959706\t1987187\t2\n",
      "851060\t1987187\t8\n",
      "1850056\t1987187\t1\n",
      "1275191\t1987187\t1\n",
      "1958086\t1987187\t1\n",
      "1767606\t1987187\t1\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "path = os.path.join(base_dir, \"train_rec_examples.json\")\n",
    "! wc -l $path\n",
    "! head -n 3 $path\n",
    "! tail -n 3 $path\n",
    "print(\"=\"*100)\n",
    "path = os.path.join(base_dir, \"val_rec_examples.json\")\n",
    "! wc -l $path\n",
    "! head -n 3 $path\n",
    "! tail -n 3 $path\n",
    "print(\"=\"*100)\n",
    "path = os.path.join(base_dir, \"test_rec_examples.json\")\n",
    "! wc -l $path\n",
    "! head -n 3 $path\n",
    "! tail -n 3 $path\n",
    "print(\"=\"*100)\n",
    "path = os.path.join(base_dir, \"train_search_examples.json\")\n",
    "! wc -l $path\n",
    "! head -n 3 $path\n",
    "! tail -n 3 $path\n",
    "print(\"=\"*100)\n",
    "path = os.path.join(base_dir, \"val_search_examples.json\")\n",
    "! wc -l $path\n",
    "! head -n 3 $path\n",
    "! tail -n 3 $path\n",
    "print(\"=\"*100)\n",
    "path = os.path.join(base_dir, \"test_search_examples.json\")\n",
    "! wc -l $path\n",
    "! head -n 3 $path\n",
    "! tail -n 3 $path\n",
    "print(\"=\"*100)\n",
    "print(\"all search examples = {}\".format(len(search_examples)))\n",
    "path = os.path.join(base_dir, \"collection.tsv\")\n",
    "! wc -l $path\n",
    "! head -n 3 $path\n",
    "! tail -n 3 $path\n",
    "print(\"=\"*100)\n",
    "path = os.path.join(base_dir, \"collection_title.tsv\")\n",
    "! wc -l $path\n",
    "! head -n 3 $path\n",
    "! tail -n 3 $path\n",
    "print(\"=\"*100)\n",
    "path = os.path.join(base_dir, \"all_queries.tsv\")\n",
    "! wc -l $path\n",
    "! head -n 3 $path\n",
    "! tail -n 3 $path\n",
    "print(\"=\"*100)\n",
    "\n",
    "list_fns = [\"train_queries.tsv\", \"val_queries.tsv\", \"test_queries.tsv\"]\n",
    "for fn in list_fns:\n",
    "    path = os.path.join(base_dir, fn)\n",
    "    ! wc -l $path\n",
    "    ! head -n 3 $path\n",
    "    ! tail -n 3 $path\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "list_fns = [\"train_query_qrels.tsv\", \"val_query_qrels.tsv\", \"test_query_qrels.tsv\", \n",
    "            \"train_query_qrels_clicknum.tsv\", \"val_query_qrels_clicknum.tsv\", \"test_query_qrels_clicknum.tsv\"]\n",
    "for fn in list_fns:\n",
    "    path = os.path.join(base_dir, fn)\n",
    "    ! wc -l $path\n",
    "    ! head -n 3 $path\n",
    "    ! tail -n 3 $path\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "list_fns = [\"train_anchors.tsv\", \"val_anchors.tsv\", \"test_anchors.tsv\"]\n",
    "for fn in list_fns:\n",
    "    path = os.path.join(base_dir, fn)\n",
    "    ! wc -l $path\n",
    "    ! head -n 3 $path\n",
    "    ! tail -n 3 $path\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "list_fns = [\"train_anchor_qrels.tsv\", \"val_anchor_qrels.tsv\", \"test_anchor_qrels.tsv\",\n",
    "           \"train_anchor_qrels_clicknum.tsv\", \"val_anchor_qrels_clicknum.tsv\", \"test_anchor_qrels_clicknum.tsv\"]\n",
    "for fn in list_fns:\n",
    "    path = os.path.join(base_dir, fn)\n",
    "    ! wc -l $path\n",
    "    ! head -n 3 $path\n",
    "    ! tail -n 3 $path\n",
    "    print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce826f1-c569-4cb5-81d5-d7e604248e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "unique_dates = np.unique(filtered_df.feed_date)\n",
    "unique_dates[int(len(unique_dates)*0.9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb47bfef-c382-40d1-892a-3bec9d141ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(query_to_qid) == len(qid_to_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e843d6-038c-4b5f-8537-94cd1c5cce0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f4537c-e811-4bfd-9fc2-813cb069299b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-9.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
