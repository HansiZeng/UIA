{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of sim_rec train and test = 1,017,800, 10,000\n",
      "length of compl_rec train and test = 67,310, 10,000\n",
      "length of search train and test = 13,726,249, 10,000\n",
      "number of entites = 3,214,651\n",
      "test users for each data are subset of their corresponding train users.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pickle \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "in_dir = \"/home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/\"\n",
    "    \n",
    "train_sim_data, train_compl_data, train_search_data = None, None, None\n",
    "data_fns = [\n",
    "    os.path.join(in_dir, \"train_sim_recs.csv\"),\n",
    "    os.path.join(in_dir, \"train_compl_recs.csv\"),\n",
    "    os.path.join(in_dir, \"train_searchs.csv\"),\n",
    "]\n",
    "datas = []\n",
    "for fn in data_fns:\n",
    "    datas.append(pd.read_csv(fn, index_col=0))\n",
    "train_sim_data, train_compl_data, train_search_data = datas\n",
    "\n",
    "\n",
    "datas = []\n",
    "test_sim_data, test_compl_data, test_search_data = None, None, None\n",
    "selected_dir = os.path.join(in_dir, \"selected_test_user\")\n",
    "data_fns = [\n",
    "    os.path.join(selected_dir, \"selected_sim_data.test.pkl\"),\n",
    "    os.path.join(selected_dir, \"selected_compl_data.test.pkl\"),\n",
    "    os.path.join(selected_dir, \"selected_search_data.test.pkl\"),\n",
    "]\n",
    "for fn in data_fns:\n",
    "    with open(fn, \"rb\") as fin:\n",
    "        datas.append(pickle.load(fin))\n",
    "test_sim_data, test_compl_data, test_search_data = datas\n",
    "datas = None\n",
    "\n",
    "root_dir=\"/home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/\"\n",
    "eid_to_text = {}\n",
    "with open(os.path.join(root_dir, \"all_entities.tsv\")) as fin:\n",
    "    for line in fin:\n",
    "        eid, text = line.strip().split(\"\\t\")\n",
    "        eid_to_text[int(eid)] = text\n",
    "\n",
    "print(\"length of sim_rec train and test = {:,}, {:,}\".format(len(train_sim_data), len(test_sim_data)))\n",
    "print(\"length of compl_rec train and test = {:,}, {:,}\".format(len(train_compl_data), len(test_compl_data)))\n",
    "print(\"length of search train and test = {:,}, {:,}\".format(len(train_search_data), len(test_search_data)))\n",
    "print(\"number of entites = {:,}\".format(len(eid_to_text)))\n",
    "\n",
    "assert set(test_sim_data.uid).issubset(set(train_sim_data.uid)) \\\n",
    "and set(test_compl_data.uid).issubset(set(train_compl_data.uid)) \\\n",
    "and set(test_search_data.uid).issubset(set(train_search_data.uid))\n",
    "print(\"test users for each data are subset of their corresponding train users.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "search_sequential: 100%|██████████| 815832/815832 [09:31<00:00, 1428.49it/s]\n",
      "sim_rec_sequential: 100%|██████████| 81664/81664 [00:57<00:00, 1415.14it/s]\n",
      "compl_rec_sequential: 100%|██████████| 12628/12628 [00:10<00:00, 1188.69it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "import ujson \n",
    "\n",
    "out_dir = os.path.join(in_dir, \"sequential_train_test/\")\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir) \n",
    "\n",
    "seq_examples_list = []\n",
    "prefixes_to_datas= {\n",
    "    os.path.join(out_dir, \"search_sequential\"): (train_search_data, test_search_data, \"urels.search.test.tsv\"),\n",
    "    os.path.join(out_dir, \"sim_rec_sequential\"): (train_sim_data, test_sim_data, \"urels.sim.test.tsv\"),\n",
    "    os.path.join(out_dir, \"compl_rec_sequential\"): (train_compl_data, test_compl_data, \"urels.compl.test.tsv\"),\n",
    "}\n",
    "\n",
    "for prefix, (train_data, test_data, urel_path) in prefixes_to_datas.items():\n",
    "    train_seq_examples = []\n",
    "    test_seq_examples = []\n",
    "    test_uid_to_pospids = {}\n",
    "    for uid, group in tqdm(train_data.groupby(\"uid\"), desc=prefix.split(\"/\")[-1]):\n",
    "        if \"search_sequential\" in prefix:\n",
    "            qids = list(group.qid)\n",
    "            group_rel_pids = group.rel_pids \n",
    "        elif \"sim_rec_sequential\" in prefix:\n",
    "            qids = list(group.aid)\n",
    "            group_rel_pids = group.sim_pids\n",
    "        elif \"compl_rec_sequential\" in prefix:\n",
    "            qids = list(group.aid)\n",
    "            group_rel_pids = group.compl_pids\n",
    "        else:\n",
    "            raise ValueError(f\"{prefix} not valid.\")\n",
    "        \n",
    "        rel_pids = []\n",
    "        for xs in group_rel_pids:\n",
    "            rel_pids.append(random.sample(eval(xs), k=1)[0]) # only sample 1 relpid \n",
    "        assert len(qids) == len(rel_pids) == len(group)\n",
    "\n",
    "        uid = int(uid)\n",
    "        qids = [int(x) for x in qids]\n",
    "        rel_pids = [int(x) for x in rel_pids]\n",
    "\n",
    "        query_ids = qids[1:]\n",
    "        context_key_ids = qids[:-1]\n",
    "        context_value_ids = rel_pids[:-1]\n",
    "        target_value_ids = rel_pids[1:]\n",
    "        assert len(query_ids) == len(context_key_ids) == len(context_value_ids) == len(target_value_ids)\n",
    "\n",
    "        example = {\"uid\": uid, \"query_ids\": query_ids, \"context_key_ids\": context_key_ids, \"context_value_ids\": context_value_ids,\n",
    "                    \"target_value_ids\": target_value_ids}\n",
    "        train_seq_examples.append(example)\n",
    "\n",
    "        # for test\n",
    "        test_row = test_data[test_data.uid == uid]\n",
    "        if len(test_row) == 0:\n",
    "            continue\n",
    "        assert len(test_row) == 1, test_row\n",
    "        \n",
    "        if \"search_sequential\" in prefix:\n",
    "            test_qid = int(test_row.iloc[0].qid)\n",
    "        elif \"sim_rec_sequential\" in prefix:\n",
    "            test_qid = int(test_row.iloc[0].aid)\n",
    "        elif \"compl_rec_sequential\" in prefix:\n",
    "            qids = list(group.aid)\n",
    "            test_qid = int(test_row.iloc[0].aid)\n",
    "        else:\n",
    "            raise ValueError(f\"{prefix} not valid.\") \n",
    "\n",
    "        test_query_ids = qids[1:] + [test_qid]\n",
    "        test_context_key_ids = qids \n",
    "        test_context_value_ids = rel_pids\n",
    "        assert len(test_query_ids) == len(test_context_key_ids) == len(test_context_value_ids), (len(test_query_ids), \n",
    "                                                                                len(test_context_key_ids), len(test_context_value_ids))\n",
    "\n",
    "        example = {\"uid\": uid, \"query_ids\": test_query_ids, \"context_key_ids\": test_context_key_ids, \"context_value_ids\": test_context_value_ids}\n",
    "        test_seq_examples.append(example)\n",
    "\n",
    "        if \"search_sequential\" in prefix:\n",
    "            test_uid_to_pospids[uid] = test_row.iloc[0].rel_pids\n",
    "        elif \"sim_rec_sequential\" in prefix:\n",
    "            test_uid_to_pospids[uid] = test_row.iloc[0].sim_pids\n",
    "        elif \"compl_rec_sequential\" in prefix:\n",
    "            test_uid_to_pospids[uid] = test_row.iloc[0].compl_pids\n",
    "        else:\n",
    "            raise ValueError(f\"{prefix} not valid.\")\n",
    "        \n",
    "\n",
    "    with open(prefix + \".train.json\", \"w\") as fout:\n",
    "        for line in train_seq_examples:\n",
    "            fout.write(ujson.dumps(line) + \"\\n\")\n",
    "    with open(prefix + \".test.json\", \"w\") as fout:\n",
    "        for line in test_seq_examples:\n",
    "            fout.write(ujson.dumps(line) + \"\\n\")\n",
    "    with open(os.path.join(out_dir, urel_path), \"w\") as fout:\n",
    "        for uid, pos_pids in test_uid_to_pospids.items():\n",
    "            for pos_pid in pos_pids:\n",
    "                fout.write(f\"{uid}\\tQ0\\t{pos_pid}\\t{1}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ignore hids = 6644\n"
     ]
    }
   ],
   "source": [
    "def create_neg_value_ids(query_ids, pos_value_ids, miss_qids, sampler=None):\n",
    "    assert type(sampler) == dict\n",
    "    assert len(query_ids) == len(pos_value_ids)\n",
    "    neg_value_ids = []\n",
    "    for qid, pos_vid in zip(query_ids, pos_value_ids):\n",
    "        if qid not in sampler:\n",
    "            miss_qids.add(qid)\n",
    "            neg_vid = random.sample(range(2_000_000), k=1)[0]\n",
    "            while neg_vid == pos_vid:\n",
    "                neg_vid = random.sample(range(2_000_000), k=1)[0]\n",
    "            neg_value_ids.append(neg_vid)\n",
    "        else:\n",
    "            neg_vid = random.sample(sampler[qid], k=1)[0]\n",
    "            while neg_vid == pos_vid:\n",
    "                neg_vid = random.sample(range(2_000_000), k=1)[0]\n",
    "            neg_value_ids.append(neg_vid)\n",
    "    \n",
    "    assert len(neg_value_ids) == len(pos_value_ids)\n",
    "    \n",
    "    return neg_value_ids\n",
    "\n",
    "run_path = os.path.join(in_dir, \"runs/bm25.all.run\")\n",
    "df = pd.read_csv(run_path, sep=\" \", names=[\"hid\", \"q0\", \"tid\", \"rank\", \"score\", \"model_name\"])\n",
    "bm25_hid_to_tids = {}\n",
    "ignore_hids = set()\n",
    "for hid, group in df.groupby(\"hid\"):\n",
    "    cand_tids = list(group.tid.values)\n",
    "    if len(cand_tids) < 10:\n",
    "        ignore_hids.add(int(hid))\n",
    "    else:\n",
    "        bm25_hid_to_tids[int(hid)] = [int(x) for x in cand_tids]\n",
    "        \n",
    "print(\"number of ignore hids = {}\".format(len(ignore_hids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "search_sequential.train.json: 100%|██████████| 815832/815832 [00:09<00:00, 81939.01it/s]\n",
      "search_sequential.test.json: 100%|██████████| 10000/10000 [00:00<00:00, 212564.63it/s]\n",
      "sim_rec_sequential.train.json: 100%|██████████| 81664/81664 [00:01<00:00, 68595.75it/s]\n",
      "sim_rec_sequential.test.json: 100%|██████████| 10000/10000 [00:00<00:00, 216358.49it/s]\n",
      "compl_rec_sequential.train.json: 100%|██████████| 12628/12628 [00:00<00:00, 85009.76it/s]\n",
      "compl_rec_sequential.test.json: 100%|██████████| 10000/10000 [00:00<00:00, 226892.06it/s]\n",
      "search_sequential.train.json: 100%|██████████| 815832/815832 [00:19<00:00, 42028.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bm25 suffix: search_sequential.train.json's miss_hids = 25114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "search_sequential.test.json: 100%|██████████| 10000/10000 [00:00<00:00, 213876.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bm25 suffix: search_sequential.test.json's miss_hids = 25114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sim_rec_sequential.train.json: 100%|██████████| 81664/81664 [00:00<00:00, 84232.30it/s]\n",
      "sim_rec_sequential.test.json: 100%|██████████| 10000/10000 [00:00<00:00, 223325.79it/s]\n",
      "compl_rec_sequential.train.json: 100%|██████████| 12628/12628 [00:00<00:00, 50141.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bm25 suffix: compl_rec_sequential.train.json's miss_hids = 25114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "compl_rec_sequential.test.json: 100%|██████████| 10000/10000 [00:00<00:00, 184721.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bm25 suffix: compl_rec_sequential.test.json's miss_hids = 25114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "search_sequential.train.json: 100%|██████████| 815832/815832 [00:14<00:00, 58134.43it/s]\n",
      "search_sequential.test.json: 100%|██████████| 10000/10000 [00:00<00:00, 179544.54it/s]\n",
      "sim_rec_sequential.train.json: 100%|██████████| 81664/81664 [00:01<00:00, 52554.24it/s]\n",
      "sim_rec_sequential.test.json: 100%|██████████| 10000/10000 [00:00<00:00, 156434.42it/s]\n",
      "compl_rec_sequential.train.json: 100%|██████████| 12628/12628 [00:00<00:00, 82154.26it/s]\n",
      "compl_rec_sequential.test.json: 100%|██████████| 10000/10000 [00:00<00:00, 200473.38it/s]\n",
      "search_sequential.train.json: 100%|██████████| 815832/815832 [00:32<00:00, 25125.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bm25 suffix: search_sequential.train.json's miss_hids = 33608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "search_sequential.test.json: 100%|██████████| 10000/10000 [00:00<00:00, 176123.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bm25 suffix: search_sequential.test.json's miss_hids = 33608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sim_rec_sequential.train.json: 100%|██████████| 81664/81664 [00:01<00:00, 58994.27it/s]\n",
      "sim_rec_sequential.test.json: 100%|██████████| 10000/10000 [00:00<00:00, 174601.16it/s]\n",
      "compl_rec_sequential.train.json: 100%|██████████| 12628/12628 [00:00<00:00, 44999.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bm25 suffix: compl_rec_sequential.train.json's miss_hids = 33608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "compl_rec_sequential.test.json: 100%|██████████| 10000/10000 [00:00<00:00, 221557.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bm25 suffix: compl_rec_sequential.test.json's miss_hids = 33608\n"
     ]
    }
   ],
   "source": [
    "import ujson\n",
    "from tqdm import tqdm \n",
    "import random\n",
    "\n",
    "SIM_RELATION = \"is_similar_to\"\n",
    "COMPL_RELATION = \"is_complementary_to\"\n",
    "REL_RELATION = \"is_relevant_to\"\n",
    "\n",
    "in_dir = \"/home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/\"\n",
    "out_dir = os.path.join(in_dir, \"sequential_train_test/\")\n",
    "train_search_examples, test_search_examples, train_sim_rec_examples, test_sim_rec_examples, train_compl_rec_examples, \\\n",
    "test_compl_rec_examples = [],[],[],[],[],[]\n",
    "fn_to_example = {\n",
    "    os.path.join(out_dir, \"search_sequential.train.json\"): train_search_examples,\n",
    "    os.path.join(out_dir, \"search_sequential.test.json\"): test_search_examples,\n",
    "    os.path.join(out_dir, \"sim_rec_sequential.train.json\"): train_sim_rec_examples,\n",
    "    os.path.join(out_dir, \"sim_rec_sequential.test.json\"): test_sim_rec_examples,\n",
    "    os.path.join(out_dir, \"compl_rec_sequential.train.json\"): train_compl_rec_examples,\n",
    "    os.path.join(out_dir, \"compl_rec_sequential.test.json\"): test_compl_rec_examples,\n",
    "}\n",
    "\n",
    "for fn, data_examples in fn_to_example.items():\n",
    "    with open(fn) as fin:\n",
    "        for line in fin:\n",
    "            data_examples.append(ujson.loads(line))\n",
    "            \n",
    "miss_hids = set()\n",
    "\n",
    "suffix_to_examples = {\n",
    "    \"search_sequential.train.json\": (train_search_examples,REL_RELATION),\n",
    "    \"search_sequential.test.json\": (test_search_examples,REL_RELATION) ,\n",
    "    \"sim_rec_sequential.train.json\": (train_sim_rec_examples,SIM_RELATION),\n",
    "    \"sim_rec_sequential.test.json\": (test_sim_rec_examples,SIM_RELATION),\n",
    "    \"compl_rec_sequential.train.json\": (train_compl_rec_examples,COMPL_RELATION),\n",
    "    \"compl_rec_sequential.test.json\": (test_compl_rec_examples,COMPL_RELATION),\n",
    "}\n",
    "\n",
    "history_lengths = [4, 8]\n",
    "for hist_len in history_lengths:\n",
    "    for dest_signature in [\"hlen_{}_randneg\".format(hist_len), \"hlen_{}_bm25\".format(hist_len)]:\n",
    "        dest_dir = os.path.join(out_dir, dest_signature)\n",
    "        if not os.path.exists(dest_dir):\n",
    "            os.mkdir(dest_dir)\n",
    "        for suffix, (data_examples, relation) in suffix_to_examples.items():\n",
    "            dest_fn = os.path.join(dest_dir, suffix)\n",
    "            with open(dest_fn, \"w\") as fout:\n",
    "                for example in tqdm(data_examples, desc=suffix):\n",
    "                    if \"train.json\" in dest_fn:\n",
    "                        start_idx = max(0, len(example[\"query_ids\"])-hist_len)\n",
    "                    elif \"test.json\":\n",
    "                        start_idx = max(0, len(example[\"query_ids\"])-hist_len-1)\n",
    "                    else:\n",
    "                        raise ValueError(f\"{suffix} is not valid.\")\n",
    "\n",
    "                    query_ids = example[\"query_ids\"][start_idx:]\n",
    "                    context_key_ids = example[\"context_key_ids\"][start_idx:]\n",
    "                    context_value_ids = example[\"context_value_ids\"][start_idx:]\n",
    "                    if \"train.json\" in dest_fn:\n",
    "                        target_value_ids = example[\"target_value_ids\"][start_idx:]\n",
    "                        if \"randneg\" in dest_signature:\n",
    "                            neg_value_ids = random.sample(range(2_000_000), k=len(target_value_ids))\n",
    "                        elif \"bm25\" in dest_signature:\n",
    "                            if \"sim_rec_sequential\" in suffix:\n",
    "                                neg_value_ids = random.sample(range(2_000_000), k=len(target_value_ids))\n",
    "                            elif \"search_sequential\" in suffix or \"compl_rec_sequential\" in suffix:\n",
    "                                neg_value_ids = create_neg_value_ids(query_ids=query_ids, \n",
    "                                                                     pos_value_ids=target_value_ids, \n",
    "                                                                     miss_qids=miss_hids, \n",
    "                                                                     sampler=bm25_hid_to_tids)\n",
    "                                \n",
    "                        else:\n",
    "                            raise ValueError(f\"dest signature: {dest_signature} is not valid.\")\n",
    "                        dest_example = {\"uid\": example[\"uid\"], \"query_ids\": query_ids, \"context_key_ids\": context_key_ids,\n",
    "                                    \"context_value_ids\": context_value_ids, \n",
    "                                    \"target_value_ids\": target_value_ids, \"neg_value_ids\": neg_value_ids, \"relation\": relation}\n",
    "                    elif \"test.json\" in dest_fn:\n",
    "                        dest_example = {\"uid\": example[\"uid\"], \"query_ids\": query_ids, \"context_key_ids\": context_key_ids,\n",
    "                                    \"context_value_ids\": context_value_ids, \"relation\": relation}\n",
    "                    else:\n",
    "                        raise ValueError(f\"{suffix} is not valid.\")\n",
    "                    fout.write(ujson.dumps(dest_example) + \"\\n\")\n",
    "            if \"bm25\" in dest_signature:\n",
    "                if \"search_sequential\" in suffix or \"compl_rec_sequential\" in suffix:\n",
    "                    print(\"bm25 suffix: {}'s miss_hids = {}\".format(suffix, len(miss_hids)))\n",
    "\n",
    "\n",
    "root_dir=\"/home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/\"\n",
    "eid_to_text = {}\n",
    "with open(os.path.join(root_dir, \"all_entities.tsv\")) as fin:\n",
    "    for line in fin:\n",
    "        eid, text = line.strip().split(\"\\t\")\n",
    "        eid_to_text[int(eid)] = text\n",
    "\n",
    "dest_dir = os.path.join(out_dir, \"without_context/\")\n",
    "if not os.path.exists(dest_dir):\n",
    "    os.mkdir(dest_dir)\n",
    "fn_to_example = {\n",
    "    os.path.join(dest_dir, \"uid_anchors.test.sim.tsv\"): (test_sim_rec_examples, SIM_RELATION),\n",
    "    os.path.join(dest_dir, \"uid_anchors.test.compl.tsv\"): (test_compl_rec_examples, COMPL_RELATION),\n",
    "    os.path.join(dest_dir, \"uid_queries.test.search.tsv\"): (test_search_examples, REL_RELATION)\n",
    "}\n",
    "for fn, (test_examples, relation) in fn_to_example.items():\n",
    "    with open(fn, \"w\") as fout:\n",
    "        for example in test_examples:\n",
    "            uid, query = example[\"uid\"], eid_to_text[example[\"query_ids\"][-1]]\n",
    "            fout.write(f\"{uid}\\t{query}\\t{relation}\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "815832 /home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/sequential_train_test/hlen_4_bm25/search_sequential.train.json\n",
      "{\"uid\":0,\"query_ids\":[2938044,2441693,2382801,2676693],\"context_key_ids\":[2791482,2938044,2441693,2382801],\"context_value_ids\":[1558005,246217,438838,479119],\"target_value_ids\":[246217,438838,479119,264598],\"neg_value_ids\":[1554910,1363026,1865963,640542],\"relation\":\"is_relevant_to\"}\n",
      "{\"uid\":1,\"query_ids\":[3000257,2736901,3000257,2736901],\"context_key_ids\":[2461556,3000257,2736901,3000257],\"context_value_ids\":[618773,2065684,1698588,1846797],\"target_value_ids\":[2065684,1698588,1846797,1698588],\"neg_value_ids\":[1851723,2256340,1424601,1445720],\"relation\":\"is_relevant_to\"}\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor fn in os.listdir(out_dir):\\n    if \"search\" not in fn or \"small\" in fn:\\n        continue \\n    fn = os.path.join(out_dir, fn)\\n    \\n    ! wc -l $fn \\n    ! head -n 2 $fn\\n    print(100*\"=\")\\n\\nprint(100*\"=.\")\\n\\ncheck_dir = os.path.join(out_dir, \"without_context\")\\n\\nfor fn in os.listdir(check_dir):\\n    fn = os.path.join(check_dir, fn)\\n    \\n    ! wc -l $fn \\n    ! head -n 2 $fn\\n    print(100*\"=\")\\n'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dir=\"/home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/sequential_train_test/\"\n",
    "check_dir = os.path.join(out_dir, \"hlen_4_bm25\")\n",
    "\n",
    "for fn in os.listdir(check_dir):\n",
    "    fn = os.path.join(check_dir, fn)\n",
    "    if \".test.json\" in fn:\n",
    "        continue\n",
    "    if \"search_sequential\" not in fn:\n",
    "        continue\n",
    "    ! wc -l $fn \n",
    "    ! head -n 2 $fn\n",
    "    print(100*\"=\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "for fn in os.listdir(out_dir):\n",
    "    if \"search\" not in fn or \"small\" in fn:\n",
    "        continue \n",
    "    fn = os.path.join(out_dir, fn)\n",
    "    \n",
    "    ! wc -l $fn \n",
    "    ! head -n 2 $fn\n",
    "    print(100*\"=\")\n",
    "\n",
    "print(100*\"=.\")\n",
    "\n",
    "check_dir = os.path.join(out_dir, \"without_context\")\n",
    "\n",
    "for fn in os.listdir(check_dir):\n",
    "    fn = os.path.join(check_dir, fn)\n",
    "    \n",
    "    ! wc -l $fn \n",
    "    ! head -n 2 $fn\n",
    "    print(100*\"=\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uid': 148, 'query_ids': [3120745, 2299257, 3110146, 3169024, 2582666, 3191904, 2875418, 2978514, 3156648], 'context_key_ids': [2504195, 3120745, 2299257, 3110146, 3169024, 2582666, 3191904, 2875418, 2978514], 'context_value_ids': [818764, 538461, 1146204, 1738420, 926207, 727621, 2167844, 2217383, 663385]}\n",
      "query: venom steel gloves\n",
      "target_item: Venom Steel Unisex 2-Layer Rip Resistant Nitrile Multipurpose Gloves, One Size Fits All (25-Pairs) ; Work Gloves\n",
      "neg_item: Buckle-Down Venom Face Icon Black/Reds/White Black Dog Collar, Medium (26- 40 Lbs.) ; Pet Collars & Harnesses\n",
      "\n",
      "===========================================================================\n",
      "query: edger attachment\n",
      "target_item: TrimmerPlus LE720 Edger Attachment ; String Trimmer Attachments\n",
      "neg_item: Badger Link-on Cultivator Attachment Tiller Attachment ; String Trimmer Attachments\n",
      "\n",
      "===========================================================================\n",
      "query: drill bit extensions\n",
      "target_item: Southwire 3/16-in Round 54-in Drill Bit Extension ; Drill Bit Extensions\n",
      "neg_item: Drill America 6-in Cobalt Twist Drill Bit ; Twist Drill Bits\n",
      "\n",
      "===========================================================================\n",
      "query: air compressors\n",
      "target_item: CRAFTSMAN 6-Gallon Single Stage Portable Corded Electric Pancake Air Compressor ; Air Compressors\n",
      "neg_item: Hitachi 4-Gallon Single Stage Portable Electric Twin Stack Air Compressor ; Air Compressors\n",
      "\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "uid = 148\n",
    "for example in test_search_examples:\n",
    "    if example[\"uid\"] == uid:\n",
    "        print(example)\n",
    "query_ids = [2938044,2441693,2382801,2676693]\n",
    "target_vids = [246217,438838,479119,264598]\n",
    "neg_vids = [1554910,1363026,1865963,640542]\n",
    "\n",
    "for qid, tvid, nvid in zip(query_ids, target_vids, neg_vids):\n",
    "    print(\"query: {}\\ntarget_item: {}\\nneg_item: {}\\n\".format(eid_to_text[qid], eid_to_text[tvid], eid_to_text[nvid]))\n",
    "    print(75*\"=\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (1838640968.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_7829/1838640968.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    root_dir=\"/home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/\u001b[0m\n\u001b[0m                                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "root_dir=\"/home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/\n",
    "with open(os.path.join(root_dir, \"all_entities.tsv\")) as fin:\n",
    "    for line in fin:\n",
    "        eid, text = line.strip().split(\"\\t\")\n",
    "        eid_to_text[int(eid)] = text\n",
    "\n",
    "query_ids = [2417516,2374200, 2576318, 2836633 ]\n",
    "context_value_ids = [202549,1582436,1400531,520284]\n",
    "target_value_ids = [1582436,1400531,520284,27714]\n",
    "\n",
    "for qid, context_pid, target_pid in zip(query_ids, context_value_ids, target_value_ids):\n",
    "    print(\"qid: {}\\n ctxpid : {} \\n relpid : {}\".format(eid_to_text[qid],eid_to_text[context_pid], eid_to_text[target_pid]))\n",
    "    print(\"=\"*75)\n",
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compl_rec_sequential.test.json\t search_sequential.train.json\n",
      "compl_rec_sequential.train.json  sim_rec_sequential.test.json\n",
      "hlen_4_bm25\t\t\t sim_rec_sequential.train.json\n",
      "hlen_4_randneg\t\t\t urels.compl.test.tsv\n",
      "hlen_8_bm25\t\t\t urels.search.test.tsv\n",
      "hlen_8_randneg\t\t\t urels.sim.test.tsv\n",
      "search_sequential.test.json\t without_context\n"
     ]
    }
   ],
   "source": [
    "! ls /home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/sequential_train_test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compl_rec_sequential.test.json\t search_sequential.train.json\n",
      "compl_rec_sequential.train.json  sim_rec_sequential.test.json\n",
      "search_sequential.test.json\t sim_rec_sequential.train.json\n"
     ]
    }
   ],
   "source": [
    "! ls /home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/sequential_train_test/hlen_4_bm25/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-9.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9c9569641937ce4addf5496cbe81a89e9d276a9857e7a9967fd6589fdce30733"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
