{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of sim_rec train and test = 1,017,800, 81,664\n",
      "length of compl_rec train and test = 67,310, 12,628\n",
      "length of search train and test = 13,726,249, 815,832\n",
      "number of entites = 3,214,651\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pickle \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "in_dir = \"/work/hzeng_umass_edu/ir-research/joint_modeling_search_and_rec/datasets/unified_kgc/unified_user/\"\n",
    "    \n",
    "train_sim_data, test_sim_data, train_compl_data, test_compl_data, train_search_data, test_search_data = None, None, None, None, None, None\n",
    "fns = [\n",
    "    os.path.join(in_dir, \"train_sim_recs.csv\"),\n",
    "    os.path.join(in_dir, \"test_sim_recs.csv\"),\n",
    "    os.path.join(in_dir, \"train_compl_recs.csv\"),\n",
    "    os.path.join(in_dir, \"test_compl_recs.csv\"),\n",
    "    os.path.join(in_dir, \"train_searchs.csv\"),\n",
    "    os.path.join(in_dir, \"test_searchs.csv\"),\n",
    "]\n",
    "\n",
    "datas = []\n",
    "for fn in fns:\n",
    "    datas.append(pd.read_csv(fn, index_col=0))\n",
    "train_sim_data, test_sim_data, train_compl_data, test_compl_data, train_search_data, test_search_data = datas \n",
    "datas = None \n",
    "\n",
    "root_dir=\"/work/hzeng_umass_edu/ir-research/joint_modeling_search_and_rec/datasets/unified_kgc/\"\n",
    "eid_to_text = {}\n",
    "with open(os.path.join(root_dir, \"all_entities.tsv\")) as fin:\n",
    "    for line in fin:\n",
    "        eid, text = line.strip().split(\"\\t\")\n",
    "        eid_to_text[int(eid)] = text\n",
    "\n",
    "print(\"length of sim_rec train and test = {:,}, {:,}\".format(len(train_sim_data), len(test_sim_data)))\n",
    "print(\"length of compl_rec train and test = {:,}, {:,}\".format(len(train_compl_data), len(test_compl_data)))\n",
    "print(\"length of search train and test = {:,}, {:,}\".format(len(train_search_data), len(test_search_data)))\n",
    "print(\"number of entites = {:,}\".format(len(eid_to_text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "search_sequential: 100%|██████████| 815832/815832 [17:02<00:00, 797.78it/s]\n",
      "sim_rec_sequential: 100%|██████████| 81664/81664 [01:08<00:00, 1197.46it/s]\n",
      "compl_rec_sequential: 100%|██████████| 12628/12628 [00:09<00:00, 1367.92it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "import ujson \n",
    "\n",
    "out_dir = os.path.join(in_dir, \"sequential_train_test/\")\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir) \n",
    "\n",
    "seq_examples_list = []\n",
    "prefixes_to_datas= {\n",
    "    os.path.join(out_dir, \"search_sequential\"): (train_search_data, test_search_data, \"urels.search.test.tsv\"),\n",
    "    os.path.join(out_dir, \"sim_rec_sequential\"): (train_sim_data, test_sim_data, \"urels.sim.test.tsv\"),\n",
    "    os.path.join(out_dir, \"compl_rec_sequential\"): (train_compl_data, test_compl_data, \"urels.compl.test.tsv\"),\n",
    "}\n",
    "\n",
    "for prefix, (train_data, test_data, urel_path) in prefixes_to_datas.items():\n",
    "    train_seq_examples = []\n",
    "    test_seq_examples = []\n",
    "    test_uid_to_pospids = {}\n",
    "    for uid, group in tqdm(train_data.groupby(\"uid\"), desc=prefix.split(\"/\")[-1]):\n",
    "        if \"search_sequential\" in prefix:\n",
    "            qids = list(group.qid)\n",
    "            group_rel_pids = group.rel_pids \n",
    "        elif \"sim_rec_sequential\" in prefix:\n",
    "            qids = list(group.aid)\n",
    "            group_rel_pids = group.sim_pids\n",
    "        elif \"compl_rec_sequential\" in prefix:\n",
    "            qids = list(group.aid)\n",
    "            group_rel_pids = group.compl_pids\n",
    "        else:\n",
    "            raise ValueError(f\"{prefix} not valid.\")\n",
    "        \n",
    "        rel_pids = []\n",
    "        for xs in group_rel_pids:\n",
    "            rel_pids.append(random.sample(eval(xs), k=1)[0]) # only sample 1 relpid \n",
    "        assert len(qids) == len(rel_pids) == len(group)\n",
    "\n",
    "        uid = int(uid)\n",
    "        qids = [int(x) for x in qids]\n",
    "        rel_pids = [int(x) for x in rel_pids]\n",
    "\n",
    "        query_ids = qids[1:]\n",
    "        context_key_ids = qids[:-1]\n",
    "        context_value_ids = rel_pids[:-1]\n",
    "        target_value_ids = rel_pids[1:]\n",
    "        assert len(query_ids) == len(context_key_ids) == len(context_value_ids) == len(target_value_ids)\n",
    "\n",
    "        example = {\"uid\": uid, \"query_ids\": query_ids, \"context_key_ids\": context_key_ids, \"context_value_ids\": context_value_ids,\n",
    "                    \"target_value_ids\": target_value_ids}\n",
    "        train_seq_examples.append(example)\n",
    "\n",
    "        # for test\n",
    "        test_row = test_data[test_data.uid == uid]\n",
    "        assert len(test_row) == 1, test_row\n",
    "        \n",
    "        if \"search_sequential\" in prefix:\n",
    "            test_qid = int(test_row.iloc[0].qid)\n",
    "        elif \"sim_rec_sequential\" in prefix:\n",
    "            test_qid = int(test_row.iloc[0].aid)\n",
    "        elif \"compl_rec_sequential\" in prefix:\n",
    "            qids = list(group.aid)\n",
    "            test_qid = int(test_row.iloc[0].aid)\n",
    "        else:\n",
    "            raise ValueError(f\"{prefix} not valid.\") \n",
    "\n",
    "        test_query_ids = qids[1:] + [test_qid]\n",
    "        test_context_key_ids = qids \n",
    "        test_context_value_ids = rel_pids\n",
    "        assert len(test_query_ids) == len(test_context_key_ids) == len(test_context_value_ids), (len(test_query_ids), \n",
    "                                                                                len(test_context_key_ids), len(test_context_value_ids))\n",
    "\n",
    "        example = {\"uid\": uid, \"query_ids\": test_query_ids, \"context_key_ids\": test_context_key_ids, \"context_value_ids\": test_context_value_ids}\n",
    "        test_seq_examples.append(example)\n",
    "\n",
    "        if \"search_sequential\" in prefix:\n",
    "            test_uid_to_pospids[uid] = eval(test_row.iloc[0].rel_pids)\n",
    "        elif \"sim_rec_sequential\" in prefix:\n",
    "            test_uid_to_pospids[uid] = eval(test_row.iloc[0].sim_pids)\n",
    "        elif \"compl_rec_sequential\" in prefix:\n",
    "            test_uid_to_pospids[uid] = eval(test_row.iloc[0].compl_pids)\n",
    "        else:\n",
    "            raise ValueError(f\"{prefix} not valid.\")\n",
    "        \n",
    "\n",
    "    with open(prefix + \".train.json\", \"w\") as fout:\n",
    "        for line in train_seq_examples:\n",
    "            fout.write(ujson.dumps(line) + \"\\n\")\n",
    "    with open(prefix + \".test.json\", \"w\") as fout:\n",
    "        for line in test_seq_examples:\n",
    "            fout.write(ujson.dumps(line) + \"\\n\")\n",
    "    with open(os.path.join(out_dir, urel_path), \"w\") as fout:\n",
    "        for uid, pos_pids in test_uid_to_pospids.items():\n",
    "            for pos_pid in pos_pids:\n",
    "                fout.write(f\"{uid}\\tQ0\\t{pos_pid}\\t{1}\\n\")\n",
    "\n",
    "    if len(test_seq_examples) >= 10_000:\n",
    "        with open(prefix + \".small.test.json\", \"w\") as fout:\n",
    "            sampled_test_examples = random.sample(test_seq_examples, k=10_000)\n",
    "            for line in sampled_test_examples:\n",
    "                fout.write(ujson.dumps(line) + \"\\n\")\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "search_sequential.train.json: 100%|██████████| 815832/815832 [00:07<00:00, 106204.60it/s]\n",
      "search_sequential.test.json: 100%|██████████| 815832/815832 [00:03<00:00, 228900.57it/s]\n",
      "sim_rec_sequential.train.json: 100%|██████████| 81664/81664 [00:00<00:00, 95562.21it/s]\n",
      "sim_rec_sequential.test.json: 100%|██████████| 81664/81664 [00:00<00:00, 199381.49it/s]\n",
      "compl_rec_sequential.train.json: 100%|██████████| 12628/12628 [00:00<00:00, 102831.99it/s]\n",
      "compl_rec_sequential.test.json: 100%|██████████| 12628/12628 [00:00<00:00, 204934.27it/s]\n",
      "search_sequential.small.test.json: 100%|██████████| 10000/10000 [00:00<00:00, 168303.33it/s]\n",
      "sim_rec_sequential.small.test.json: 100%|██████████| 10000/10000 [00:00<00:00, 185160.18it/s]\n",
      "search_sequential.train.json: 100%|██████████| 815832/815832 [00:10<00:00, 74662.04it/s]\n",
      "search_sequential.test.json: 100%|██████████| 815832/815832 [00:04<00:00, 181987.35it/s]\n",
      "sim_rec_sequential.train.json: 100%|██████████| 81664/81664 [00:01<00:00, 71910.47it/s]\n",
      "sim_rec_sequential.test.json: 100%|██████████| 81664/81664 [00:00<00:00, 192168.29it/s]\n",
      "compl_rec_sequential.train.json: 100%|██████████| 12628/12628 [00:00<00:00, 92465.74it/s]\n",
      "compl_rec_sequential.test.json: 100%|██████████| 12628/12628 [00:00<00:00, 204889.08it/s]\n",
      "search_sequential.small.test.json: 100%|██████████| 10000/10000 [00:00<00:00, 181340.02it/s]\n",
      "sim_rec_sequential.small.test.json: 100%|██████████| 10000/10000 [00:00<00:00, 187823.44it/s]\n",
      "search_sequential.train.json: 100%|██████████| 815832/815832 [00:12<00:00, 63308.43it/s]\n",
      "search_sequential.test.json: 100%|██████████| 815832/815832 [00:04<00:00, 182415.12it/s]\n",
      "sim_rec_sequential.train.json: 100%|██████████| 81664/81664 [00:01<00:00, 69615.33it/s]\n",
      "sim_rec_sequential.test.json: 100%|██████████| 81664/81664 [00:00<00:00, 186978.08it/s]\n",
      "compl_rec_sequential.train.json: 100%|██████████| 12628/12628 [00:00<00:00, 96681.97it/s]\n",
      "compl_rec_sequential.test.json: 100%|██████████| 12628/12628 [00:00<00:00, 209893.05it/s]\n",
      "search_sequential.small.test.json: 100%|██████████| 10000/10000 [00:00<00:00, 180853.67it/s]\n",
      "sim_rec_sequential.small.test.json: 100%|██████████| 10000/10000 [00:00<00:00, 167868.18it/s]\n",
      "search_sequential.train.json: 100%|██████████| 815832/815832 [00:13<00:00, 59908.76it/s]\n",
      "search_sequential.test.json: 100%|██████████| 815832/815832 [00:04<00:00, 171368.83it/s]\n",
      "sim_rec_sequential.train.json: 100%|██████████| 81664/81664 [00:01<00:00, 59688.27it/s]\n",
      "sim_rec_sequential.test.json: 100%|██████████| 81664/81664 [00:00<00:00, 155999.46it/s]\n",
      "compl_rec_sequential.train.json: 100%|██████████| 12628/12628 [00:00<00:00, 87192.89it/s]\n",
      "compl_rec_sequential.test.json: 100%|██████████| 12628/12628 [00:00<00:00, 192066.05it/s]\n",
      "search_sequential.small.test.json: 100%|██████████| 10000/10000 [00:00<00:00, 137294.89it/s]\n",
      "sim_rec_sequential.small.test.json: 100%|██████████| 10000/10000 [00:00<00:00, 142949.30it/s]\n"
     ]
    }
   ],
   "source": [
    "import ujson\n",
    "from tqdm import tqdm \n",
    "import random\n",
    "\n",
    "SIM_RELATION = \"is_similar_to\"\n",
    "COMPL_RELATION = \"is_complementary_to\"\n",
    "REL_RELATION = \"is_relevant_to\"\n",
    "\n",
    "in_dir = \"/work/hzeng_umass_edu/ir-research/joint_modeling_search_and_rec/datasets/unified_kgc/unified_user/\"\n",
    "out_dir = os.path.join(in_dir, \"sequential_train_test/\")\n",
    "train_search_examples, test_search_examples, train_sim_rec_examples, test_sim_rec_examples, train_compl_rec_examples, \\\n",
    "test_compl_rec_examples, test_small_search_examples, test_small_sim_rec_examples = [],[],[],[],[],[],[],[]\n",
    "fn_to_example = {\n",
    "    os.path.join(out_dir, \"search_sequential.train.json\"): train_search_examples,\n",
    "    os.path.join(out_dir, \"search_sequential.test.json\"): test_search_examples,\n",
    "    os.path.join(out_dir, \"sim_rec_sequential.train.json\"): train_sim_rec_examples,\n",
    "    os.path.join(out_dir, \"sim_rec_sequential.test.json\"): test_sim_rec_examples,\n",
    "    os.path.join(out_dir, \"compl_rec_sequential.train.json\"): train_compl_rec_examples,\n",
    "    os.path.join(out_dir, \"compl_rec_sequential.test.json\"): test_compl_rec_examples,\n",
    "\n",
    "    os.path.join(out_dir, \"search_sequential.small.test.json\"): test_small_search_examples,\n",
    "    os.path.join(out_dir, \"sim_rec_sequential.small.test.json\"): test_small_sim_rec_examples,\n",
    "}\n",
    "\n",
    "for fn, data_examples in fn_to_example.items():\n",
    "    with open(fn) as fin:\n",
    "        for line in fin:\n",
    "            data_examples.append(ujson.loads(line))\n",
    "\n",
    "suffix_to_examples = {\n",
    "    \"search_sequential.train.json\": (train_search_examples,REL_RELATION),\n",
    "    \"search_sequential.test.json\": (test_search_examples,REL_RELATION) ,\n",
    "    \"sim_rec_sequential.train.json\": (train_sim_rec_examples,SIM_RELATION),\n",
    "    \"sim_rec_sequential.test.json\": (test_sim_rec_examples,SIM_RELATION),\n",
    "    \"compl_rec_sequential.train.json\": (train_compl_rec_examples,COMPL_RELATION),\n",
    "    \"compl_rec_sequential.test.json\": (test_compl_rec_examples,COMPL_RELATION),\n",
    "\n",
    "    \"search_sequential.small.test.json\": (test_small_search_examples, REL_RELATION) ,\n",
    "    \"sim_rec_sequential.small.test.json\": (test_small_sim_rec_examples,SIM_RELATION),\n",
    "}\n",
    "\n",
    "history_lengths = [4, 8, 12, 16]\n",
    "for hist_len in history_lengths:\n",
    "    dest_dir = os.path.join(out_dir, \"hlen_{}_randneg/\".format(hist_len))\n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.mkdir(dest_dir)\n",
    "    for suffix, (data_examples, relation) in suffix_to_examples.items():\n",
    "        dest_fn = os.path.join(dest_dir, suffix)\n",
    "        with open(dest_fn, \"w\") as fout:\n",
    "            for example in tqdm(data_examples, desc=suffix):\n",
    "                if \"train.json\" in dest_fn:\n",
    "                    start_idx = max(0, len(example[\"query_ids\"])-hist_len)\n",
    "                elif \"test.json\":\n",
    "                    start_idx = max(0, len(example[\"query_ids\"])-hist_len-1)\n",
    "                else:\n",
    "                    raise ValueError(f\"{suffix} is not valid.\")\n",
    "\n",
    "                query_ids = example[\"query_ids\"][start_idx:]\n",
    "                context_key_ids = example[\"context_key_ids\"][start_idx:]\n",
    "                context_value_ids = example[\"context_value_ids\"][start_idx:]\n",
    "                if \"train.json\" in dest_fn:\n",
    "                    target_value_ids = example[\"target_value_ids\"][start_idx:]\n",
    "                    neg_value_ids = random.sample(range(2_000_000), k=len(target_value_ids))\n",
    "                    dest_example = {\"uid\": example[\"uid\"], \"query_ids\": query_ids, \"context_key_ids\": context_key_ids,\n",
    "                                \"context_value_ids\": context_value_ids, \n",
    "                                \"target_value_ids\": target_value_ids, \"neg_value_ids\": neg_value_ids, \"relation\": relation}\n",
    "                elif \"test.json\" in dest_fn:\n",
    "                    dest_example = {\"uid\": example[\"uid\"], \"query_ids\": query_ids, \"context_key_ids\": context_key_ids,\n",
    "                                \"context_value_ids\": context_value_ids, \"relation\": relation}\n",
    "                else:\n",
    "                    raise ValueError(f\"{suffix} is not valid.\")\n",
    "                fout.write(ujson.dumps(dest_example) + \"\\n\")\n",
    "\n",
    "\n",
    "root_dir=\"/work/hzeng_umass_edu/ir-research/joint_modeling_search_and_rec/datasets/unified_kgc/\"\n",
    "eid_to_text = {}\n",
    "with open(os.path.join(root_dir, \"all_entities.tsv\")) as fin:\n",
    "    for line in fin:\n",
    "        eid, text = line.strip().split(\"\\t\")\n",
    "        eid_to_text[int(eid)] = text\n",
    "\n",
    "dest_dir = os.path.join(out_dir, \"without_context/\")\n",
    "if not os.path.exists(dest_dir):\n",
    "    os.mkdir(dest_dir)\n",
    "fn_to_example = {\n",
    "    os.path.join(dest_dir, \"uid_anchors.test.sim.small.tsv\"): (test_small_sim_rec_examples, SIM_RELATION),\n",
    "    os.path.join(dest_dir, \"uid_anchors.test.compl.tsv\"): (test_compl_rec_examples, COMPL_RELATION),\n",
    "    os.path.join(dest_dir, \"uid_queries.test.search.small.tsv\"): (test_small_search_examples, REL_RELATION)\n",
    "}\n",
    "for fn, (test_examples, relation) in fn_to_example.items():\n",
    "    with open(fn, \"w\") as fout:\n",
    "        for example in test_examples:\n",
    "            uid, query = example[\"uid\"], eid_to_text[example[\"query_ids\"][-1]]\n",
    "            fout.write(f\"{uid}\\t{query}\\t{relation}\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12628 /work/hzeng_umass_edu/ir-research/joint_modeling_search_and_rec/datasets/unified_kgc/unified_user/sequential_train_test/without_context/uid_anchors.test.compl.tsv\n",
      "6\tGeneral Shale Providence series 50-Pack Carbon 1/2-in x 8-in Tumbled Ceramic Brick Look Wall Tile ; Tile\tis_complementary_to\n",
      "92\tLegrand Plastic RCA to F-Type Wall Jack ; Audio & Video Wall Jacks\tis_complementary_to\n",
      "====================================================================================================\n",
      "10000 /work/hzeng_umass_edu/ir-research/joint_modeling_search_and_rec/datasets/unified_kgc/unified_user/sequential_train_test/without_context/uid_anchors.test.sim.small.tsv\n",
      "376229\tLG Smart Wi-Fi Enabled 4.5-cu ft High Efficiency Stackable Steam Cycle Front-Load Washer (Graphite Steel) ENERGY STAR ; Front-Load Washers\tis_similar_to\n",
      "490831\tClosetMaid BrightWood 5-ft to 10-ft W x 6.85-ft H White Wood Closet Kit ; Wood Closet Kits\tis_similar_to\n",
      "====================================================================================================\n",
      "10000 /work/hzeng_umass_edu/ir-research/joint_modeling_search_and_rec/datasets/unified_kgc/unified_user/sequential_train_test/without_context/uid_queries.test.search.small.tsv\n",
      "35926\tsoil\tis_relevant_to\n",
      "146571\tbathroom exhaust fan motor\tis_relevant_to\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "check_dir = os.path.join(out_dir, \"hlen_4_randneg\")\n",
    "\n",
    "for fn in os.listdir(check_dir):\n",
    "    if \"search\" not in fn or \"small\" in fn:\n",
    "        continue \n",
    "    fn = os.path.join(check_dir, fn)\n",
    "    \n",
    "    ! wc -l $fn \n",
    "    ! head -n 2 $fn\n",
    "    print(100*\"=\")\n",
    "\n",
    "for fn in os.listdir(out_dir):\n",
    "    if \"search\" not in fn or \"small\" in fn:\n",
    "        continue \n",
    "    fn = os.path.join(out_dir, fn)\n",
    "    \n",
    "    ! wc -l $fn \n",
    "    ! head -n 2 $fn\n",
    "    print(100*\"=\")\n",
    "\n",
    "print(100*\"=.\")\n",
    "\"\"\"\n",
    "check_dir = os.path.join(out_dir, \"without_context\")\n",
    "\n",
    "for fn in os.listdir(check_dir):\n",
    "    fn = os.path.join(check_dir, fn)\n",
    "    \n",
    "    ! wc -l $fn \n",
    "    ! head -n 2 $fn\n",
    "    print(100*\"=\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uid': 146571, 'query_ids': [2981289, 2658084, 2619697, 2438241, 2438241, 3012079, 2494185, 2730445, 2282185, 2650790, 3012079, 3012079, 3059960, 3170758, 2809002], 'context_key_ids': [2467143, 2981289, 2658084, 2619697, 2438241, 2438241, 3012079, 2494185, 2730445, 2282185, 2650790, 3012079, 3012079, 3059960, 3170758], 'context_value_ids': [420438, 1470982, 1430343, 200653, 2251545, 1302684, 1921262, 1276207, 1572750, 656803, 804622, 1545826, 1921262, 1461566, 555962]}\n",
      "bathroom exhaust fan motor\n"
     ]
    }
   ],
   "source": [
    "uid = 146571\n",
    "for example in test_search_examples:\n",
    "    if example[\"uid\"] == uid:\n",
    "        print(example)\n",
    "print(eid_to_text[2809002])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qid: sputnik light\n",
      " ctxpid : ReliaBilt 30001 Series 31.75-in x 15.75-in x 3-in Jamb Tilting Vinyl Replacement White Basement Hopper Window ; Basement Hopper Windows \n",
      " relpid : allen + roth Grayford 9-Light Brushed Nickel Mid-century Sputnik Pendant Light ; Pendant Lighting\n",
      "===========================================================================\n",
      "qid: pantry cabinet\n",
      " ctxpid : allen + roth Grayford 9-Light Brushed Nickel Mid-century Sputnik Pendant Light ; Pendant Lighting \n",
      " relpid : Project Source 18-in W x 84-in H x 23.75-in D Natural Unfinished Oak Door Pantry Fully Assembled Stock Cabinet (Square Door Style) ; Kitchen Cabinets\n",
      "===========================================================================\n",
      "qid: linoleum sheet flooring\n",
      " ctxpid : Project Source 18-in W x 84-in H x 23.75-in D Natural Unfinished Oak Door Pantry Fully Assembled Stock Cabinet (Square Door Style) ; Kitchen Cabinets \n",
      " relpid : Armstrong Flooring Pickwick Landing I 12-ft W Cut-to-Length Bear Path Oak Dark Brown Wood Look Low-Gloss Finish Sheet Vinyl ; Sheet Vinyl (Cut-to-Length)\n",
      "===========================================================================\n",
      "qid: shop heater\n",
      " ctxpid : Armstrong Flooring Pickwick Landing I 12-ft W Cut-to-Length Bear Path Oak Dark Brown Wood Look Low-Gloss Finish Sheet Vinyl ; Sheet Vinyl (Cut-to-Length) \n",
      " relpid : Dr. Infrared Heater 5600-Watt Infrared Portable Electric Garage Heater with Thermostat ; Electric Garage Heaters\n",
      "===========================================================================\n",
      "hi\n"
     ]
    }
   ],
   "source": [
    "root_dir=\"/work/hzeng_umass_edu/ir-research/joint_modeling_search_and_rec/datasets/unified_kgc/\"\n",
    "eid_to_text = {}\n",
    "with open(os.path.join(root_dir, \"all_entities.tsv\")) as fin:\n",
    "    for line in fin:\n",
    "        eid, text = line.strip().split(\"\\t\")\n",
    "        eid_to_text[int(eid)] = text\n",
    "\n",
    "query_ids = [2417516,2374200, 2576318, 2836633 ]\n",
    "context_value_ids = [202549,1582436,1400531,520284]\n",
    "target_value_ids = [1582436,1400531,520284,27714]\n",
    "\n",
    "for qid, context_pid, target_pid in zip(query_ids, context_value_ids, target_value_ids):\n",
    "    print(\"qid: {}\\n ctxpid : {} \\n relpid : {}\".format(eid_to_text[qid],eid_to_text[context_pid], eid_to_text[target_pid]))\n",
    "    print(\"=\"*75)\n",
    "print(\"hi\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('matchmaker')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9c9569641937ce4addf5496cbe81a89e9d276a9857e7a9967fd6589fdce30733"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
