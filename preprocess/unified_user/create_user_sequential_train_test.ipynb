{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of sim_rec train and test = 1,017,800, 10,000\n",
      "length of compl_rec train and test = 67,310, 10,000\n",
      "length of search train and test = 13,726,249, 10,000\n",
      "number of entites = 3,214,651\n",
      "test users for each data are subset of their corresponding train users.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pickle \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "in_dir = \"/home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/\"\n",
    "    \n",
    "train_sim_data, train_compl_data, train_search_data = None, None, None\n",
    "data_fns = [\n",
    "    os.path.join(in_dir, \"train_sim_recs.csv\"),\n",
    "    os.path.join(in_dir, \"train_compl_recs.csv\"),\n",
    "    os.path.join(in_dir, \"train_searchs.csv\"),\n",
    "]\n",
    "datas = []\n",
    "for fn in data_fns:\n",
    "    datas.append(pd.read_csv(fn, index_col=0))\n",
    "train_sim_data, train_compl_data, train_search_data = datas\n",
    "\n",
    "\n",
    "datas = []\n",
    "test_sim_data, test_compl_data, test_search_data = None, None, None\n",
    "selected_dir = os.path.join(in_dir, \"selected_test_user\")\n",
    "data_fns = [\n",
    "    os.path.join(selected_dir, \"selected_sim_data.test.pkl\"),\n",
    "    os.path.join(selected_dir, \"selected_compl_data.test.pkl\"),\n",
    "    os.path.join(selected_dir, \"selected_search_data.test.pkl\"),\n",
    "]\n",
    "for fn in data_fns:\n",
    "    with open(fn, \"rb\") as fin:\n",
    "        datas.append(pickle.load(fin))\n",
    "test_sim_data, test_compl_data, test_search_data = datas\n",
    "datas = None\n",
    "\n",
    "root_dir=\"/home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/\"\n",
    "eid_to_text = {}\n",
    "with open(os.path.join(root_dir, \"all_entities.tsv\")) as fin:\n",
    "    for line in fin:\n",
    "        eid, text = line.strip().split(\"\\t\")\n",
    "        eid_to_text[int(eid)] = text\n",
    "\n",
    "print(\"length of sim_rec train and test = {:,}, {:,}\".format(len(train_sim_data), len(test_sim_data)))\n",
    "print(\"length of compl_rec train and test = {:,}, {:,}\".format(len(train_compl_data), len(test_compl_data)))\n",
    "print(\"length of search train and test = {:,}, {:,}\".format(len(train_search_data), len(test_search_data)))\n",
    "print(\"number of entites = {:,}\".format(len(eid_to_text)))\n",
    "\n",
    "assert set(test_sim_data.uid).issubset(set(train_sim_data.uid)) \\\n",
    "and set(test_compl_data.uid).issubset(set(train_compl_data.uid)) \\\n",
    "and set(test_search_data.uid).issubset(set(train_search_data.uid))\n",
    "print(\"test users for each data are subset of their corresponding train users.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "search_sequential: 100%|██████████| 815832/815832 [09:31<00:00, 1428.49it/s]\n",
      "sim_rec_sequential: 100%|██████████| 81664/81664 [00:57<00:00, 1415.14it/s]\n",
      "compl_rec_sequential: 100%|██████████| 12628/12628 [00:10<00:00, 1188.69it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "import ujson \n",
    "\n",
    "out_dir = os.path.join(in_dir, \"sequential_train_test/\")\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir) \n",
    "\n",
    "seq_examples_list = []\n",
    "prefixes_to_datas= {\n",
    "    os.path.join(out_dir, \"search_sequential\"): (train_search_data, test_search_data, \"urels.search.test.tsv\"),\n",
    "    os.path.join(out_dir, \"sim_rec_sequential\"): (train_sim_data, test_sim_data, \"urels.sim.test.tsv\"),\n",
    "    os.path.join(out_dir, \"compl_rec_sequential\"): (train_compl_data, test_compl_data, \"urels.compl.test.tsv\"),\n",
    "}\n",
    "\n",
    "for prefix, (train_data, test_data, urel_path) in prefixes_to_datas.items():\n",
    "    train_seq_examples = []\n",
    "    test_seq_examples = []\n",
    "    test_uid_to_pospids = {}\n",
    "    for uid, group in tqdm(train_data.groupby(\"uid\"), desc=prefix.split(\"/\")[-1]):\n",
    "        if \"search_sequential\" in prefix:\n",
    "            qids = list(group.qid)\n",
    "            group_rel_pids = group.rel_pids \n",
    "        elif \"sim_rec_sequential\" in prefix:\n",
    "            qids = list(group.aid)\n",
    "            group_rel_pids = group.sim_pids\n",
    "        elif \"compl_rec_sequential\" in prefix:\n",
    "            qids = list(group.aid)\n",
    "            group_rel_pids = group.compl_pids\n",
    "        else:\n",
    "            raise ValueError(f\"{prefix} not valid.\")\n",
    "        \n",
    "        rel_pids = []\n",
    "        for xs in group_rel_pids:\n",
    "            rel_pids.append(random.sample(eval(xs), k=1)[0]) # only sample 1 relpid \n",
    "        assert len(qids) == len(rel_pids) == len(group)\n",
    "\n",
    "        uid = int(uid)\n",
    "        qids = [int(x) for x in qids]\n",
    "        rel_pids = [int(x) for x in rel_pids]\n",
    "\n",
    "        query_ids = qids[1:]\n",
    "        context_key_ids = qids[:-1]\n",
    "        context_value_ids = rel_pids[:-1]\n",
    "        target_value_ids = rel_pids[1:]\n",
    "        assert len(query_ids) == len(context_key_ids) == len(context_value_ids) == len(target_value_ids)\n",
    "\n",
    "        example = {\"uid\": uid, \"query_ids\": query_ids, \"context_key_ids\": context_key_ids, \"context_value_ids\": context_value_ids,\n",
    "                    \"target_value_ids\": target_value_ids}\n",
    "        train_seq_examples.append(example)\n",
    "\n",
    "        # for test\n",
    "        test_row = test_data[test_data.uid == uid]\n",
    "        if len(test_row) == 0:\n",
    "            continue\n",
    "        assert len(test_row) == 1, test_row\n",
    "        \n",
    "        if \"search_sequential\" in prefix:\n",
    "            test_qid = int(test_row.iloc[0].qid)\n",
    "        elif \"sim_rec_sequential\" in prefix:\n",
    "            test_qid = int(test_row.iloc[0].aid)\n",
    "        elif \"compl_rec_sequential\" in prefix:\n",
    "            qids = list(group.aid)\n",
    "            test_qid = int(test_row.iloc[0].aid)\n",
    "        else:\n",
    "            raise ValueError(f\"{prefix} not valid.\") \n",
    "\n",
    "        test_query_ids = qids[1:] + [test_qid]\n",
    "        test_context_key_ids = qids \n",
    "        test_context_value_ids = rel_pids\n",
    "        assert len(test_query_ids) == len(test_context_key_ids) == len(test_context_value_ids), (len(test_query_ids), \n",
    "                                                                                len(test_context_key_ids), len(test_context_value_ids))\n",
    "\n",
    "        example = {\"uid\": uid, \"query_ids\": test_query_ids, \"context_key_ids\": test_context_key_ids, \"context_value_ids\": test_context_value_ids}\n",
    "        test_seq_examples.append(example)\n",
    "\n",
    "        if \"search_sequential\" in prefix:\n",
    "            test_uid_to_pospids[uid] = test_row.iloc[0].rel_pids\n",
    "        elif \"sim_rec_sequential\" in prefix:\n",
    "            test_uid_to_pospids[uid] = test_row.iloc[0].sim_pids\n",
    "        elif \"compl_rec_sequential\" in prefix:\n",
    "            test_uid_to_pospids[uid] = test_row.iloc[0].compl_pids\n",
    "        else:\n",
    "            raise ValueError(f\"{prefix} not valid.\")\n",
    "        \n",
    "\n",
    "    with open(prefix + \".train.json\", \"w\") as fout:\n",
    "        for line in train_seq_examples:\n",
    "            fout.write(ujson.dumps(line) + \"\\n\")\n",
    "    with open(prefix + \".test.json\", \"w\") as fout:\n",
    "        for line in test_seq_examples:\n",
    "            fout.write(ujson.dumps(line) + \"\\n\")\n",
    "    with open(os.path.join(out_dir, urel_path), \"w\") as fout:\n",
    "        for uid, pos_pids in test_uid_to_pospids.items():\n",
    "            for pos_pid in pos_pids:\n",
    "                fout.write(f\"{uid}\\tQ0\\t{pos_pid}\\t{1}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ignore hids = 6644\n"
     ]
    }
   ],
   "source": [
    "def create_neg_value_ids(query_ids, pos_value_ids, miss_qids, sampler=None):\n",
    "    assert type(sampler) == dict\n",
    "    assert len(query_ids) == len(pos_value_ids)\n",
    "    neg_value_ids = []\n",
    "    for qid, pos_vid in zip(query_ids, pos_value_ids):\n",
    "        if qid not in sampler:\n",
    "            miss_qids.add(qid)\n",
    "            neg_vid = random.sample(range(2_000_000), k=1)[0]\n",
    "            while neg_vid == pos_vid:\n",
    "                neg_vid = random.sample(range(2_000_000), k=1)[0]\n",
    "            neg_value_ids.append(neg_vid)\n",
    "        else:\n",
    "            neg_vid = random.sample(sampler[qid], k=1)[0]\n",
    "            while neg_vid == pos_vid:\n",
    "                neg_vid = random.sample(range(2_000_000), k=1)[0]\n",
    "            neg_value_ids.append(neg_vid)\n",
    "    \n",
    "    assert len(neg_value_ids) == len(pos_value_ids)\n",
    "    \n",
    "    return neg_value_ids\n",
    "\n",
    "run_path = os.path.join(in_dir, \"runs/bm25.all.run\")\n",
    "df = pd.read_csv(run_path, sep=\" \", names=[\"hid\", \"q0\", \"tid\", \"rank\", \"score\", \"model_name\"])\n",
    "bm25_hid_to_tids = {}\n",
    "ignore_hids = set()\n",
    "for hid, group in df.groupby(\"hid\"):\n",
    "    cand_tids = list(group.tid.values)\n",
    "    if len(cand_tids) < 10:\n",
    "        ignore_hids.add(int(hid))\n",
    "    else:\n",
    "        bm25_hid_to_tids[int(hid)] = [int(x) for x in cand_tids]\n",
    "        \n",
    "print(\"number of ignore hids = {}\".format(len(ignore_hids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "search_sequential.train.json: 100%|██████████| 815832/815832 [00:10<00:00, 79697.56it/s]\n",
      "search_sequential.test.json: 100%|██████████| 10000/10000 [00:00<00:00, 182879.47it/s]\n",
      "sim_rec_sequential.train.json: 100%|██████████| 81664/81664 [00:00<00:00, 83854.00it/s]\n",
      "sim_rec_sequential.test.json: 100%|██████████| 10000/10000 [00:00<00:00, 219623.51it/s]\n",
      "compl_rec_sequential.train.json: 100%|██████████| 12628/12628 [00:00<00:00, 87230.37it/s]\n",
      "compl_rec_sequential.test.json: 100%|██████████| 10000/10000 [00:00<00:00, 202789.92it/s]\n",
      "search_sequential.train.json: 100%|██████████| 815832/815832 [00:18<00:00, 43379.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bm25 suffix: search_sequential.train.json's miss_hids = 25114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "search_sequential.test.json: 100%|██████████| 10000/10000 [00:00<00:00, 211321.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bm25 suffix: search_sequential.test.json's miss_hids = 25114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sim_rec_sequential.train.json: 100%|██████████| 81664/81664 [00:01<00:00, 75563.93it/s]\n",
      "sim_rec_sequential.test.json: 100%|██████████| 10000/10000 [00:00<00:00, 152583.95it/s]\n",
      "compl_rec_sequential.train.json: 100%|██████████| 12628/12628 [00:00<00:00, 49535.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bm25 suffix: compl_rec_sequential.train.json's miss_hids = 25114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "compl_rec_sequential.test.json: 100%|██████████| 10000/10000 [00:00<00:00, 226594.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bm25 suffix: compl_rec_sequential.test.json's miss_hids = 25114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "search_sequential.train.json: 100%|██████████| 815832/815832 [00:14<00:00, 55988.87it/s]\n",
      "search_sequential.test.json: 100%|██████████| 10000/10000 [00:00<00:00, 169429.86it/s]\n",
      "sim_rec_sequential.train.json: 100%|██████████| 81664/81664 [00:01<00:00, 59523.23it/s]\n",
      "sim_rec_sequential.test.json: 100%|██████████| 10000/10000 [00:00<00:00, 175142.87it/s]\n",
      "compl_rec_sequential.train.json: 100%|██████████| 12628/12628 [00:00<00:00, 78450.46it/s]\n",
      "compl_rec_sequential.test.json: 100%|██████████| 10000/10000 [00:00<00:00, 218129.55it/s]\n",
      "search_sequential.train.json: 100%|██████████| 815832/815832 [00:32<00:00, 25448.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bm25 suffix: search_sequential.train.json's miss_hids = 33608\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/sequential_train_test/hlen_8_bm25/search_sequential.test.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7829/2179701785.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelation\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuffix_to_examples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mdest_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;34m\"train.json\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdest_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/sequential_train_test/hlen_8_bm25/search_sequential.test.json'"
     ]
    }
   ],
   "source": [
    "import ujson\n",
    "from tqdm import tqdm \n",
    "import random\n",
    "\n",
    "SIM_RELATION = \"is_similar_to\"\n",
    "COMPL_RELATION = \"is_complementary_to\"\n",
    "REL_RELATION = \"is_relevant_to\"\n",
    "\n",
    "in_dir = \"/home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/\"\n",
    "out_dir = os.path.join(in_dir, \"sequential_train_test/\")\n",
    "train_search_examples, test_search_examples, train_sim_rec_examples, test_sim_rec_examples, train_compl_rec_examples, \\\n",
    "test_compl_rec_examples = [],[],[],[],[],[]\n",
    "fn_to_example = {\n",
    "    os.path.join(out_dir, \"search_sequential.train.json\"): train_search_examples,\n",
    "    os.path.join(out_dir, \"search_sequential.test.json\"): test_search_examples,\n",
    "    os.path.join(out_dir, \"sim_rec_sequential.train.json\"): train_sim_rec_examples,\n",
    "    os.path.join(out_dir, \"sim_rec_sequential.test.json\"): test_sim_rec_examples,\n",
    "    os.path.join(out_dir, \"compl_rec_sequential.train.json\"): train_compl_rec_examples,\n",
    "    os.path.join(out_dir, \"compl_rec_sequential.test.json\"): test_compl_rec_examples,\n",
    "}\n",
    "\n",
    "for fn, data_examples in fn_to_example.items():\n",
    "    with open(fn) as fin:\n",
    "        for line in fin:\n",
    "            data_examples.append(ujson.loads(line))\n",
    "            \n",
    "miss_hids = set()\n",
    "\n",
    "suffix_to_examples = {\n",
    "    \"search_sequential.train.json\": (train_search_examples,REL_RELATION),\n",
    "    \"search_sequential.test.json\": (test_search_examples,REL_RELATION) ,\n",
    "    \"sim_rec_sequential.train.json\": (train_sim_rec_examples,SIM_RELATION),\n",
    "    \"sim_rec_sequential.test.json\": (test_sim_rec_examples,SIM_RELATION),\n",
    "    \"compl_rec_sequential.train.json\": (train_compl_rec_examples,COMPL_RELATION),\n",
    "    \"compl_rec_sequential.test.json\": (test_compl_rec_examples,COMPL_RELATION),\n",
    "}\n",
    "\n",
    "history_lengths = [4, 8]\n",
    "for hist_len in history_lengths:\n",
    "    for dest_signature in [\"hlen_{}_randneg\".format(hist_len), \"hlen_{}_bm25\".format(hist_len)]:\n",
    "        dest_dir = os.path.join(out_dir, dest_signature)\n",
    "        if not os.path.exists(dest_dir):\n",
    "            os.mkdir(dest_dir)\n",
    "        for suffix, (data_examples, relation) in suffix_to_examples.items():\n",
    "            dest_fn = os.path.join(dest_dir, suffix)\n",
    "            with open(dest_fn, \"w\") as fout:\n",
    "                for example in tqdm(data_examples, desc=suffix):\n",
    "                    if \"train.json\" in dest_fn:\n",
    "                        start_idx = max(0, len(example[\"query_ids\"])-hist_len)\n",
    "                    elif \"test.json\":\n",
    "                        start_idx = max(0, len(example[\"query_ids\"])-hist_len-1)\n",
    "                    else:\n",
    "                        raise ValueError(f\"{suffix} is not valid.\")\n",
    "\n",
    "                    query_ids = example[\"query_ids\"][start_idx:]\n",
    "                    context_key_ids = example[\"context_key_ids\"][start_idx:]\n",
    "                    context_value_ids = example[\"context_value_ids\"][start_idx:]\n",
    "                    if \"train.json\" in dest_fn:\n",
    "                        target_value_ids = example[\"target_value_ids\"][start_idx:]\n",
    "                        if \"randneg\" in dest_signature:\n",
    "                            neg_value_ids = random.sample(range(2_000_000), k=len(target_value_ids))\n",
    "                        elif \"bm25\" in dest_signature:\n",
    "                            if \"sim_rec_sequential\" in suffix:\n",
    "                                neg_value_ids = random.sample(range(2_000_000), k=len(target_value_ids))\n",
    "                            elif \"search_sequential\" in suffix or \"compl_rec_sequential\" in suffix:\n",
    "                                neg_value_ids = create_neg_value_ids(query_ids=query_ids, \n",
    "                                                                     pos_value_ids=target_value_ids, \n",
    "                                                                     miss_qids=miss_hids, \n",
    "                                                                     sampler=bm25_hid_to_tids)\n",
    "                                \n",
    "                        else:\n",
    "                            raise ValueError(f\"dest signature: {dest_signature} is not valid.\")\n",
    "                        dest_example = {\"uid\": example[\"uid\"], \"query_ids\": query_ids, \"context_key_ids\": context_key_ids,\n",
    "                                    \"context_value_ids\": context_value_ids, \n",
    "                                    \"target_value_ids\": target_value_ids, \"neg_value_ids\": neg_value_ids, \"relation\": relation}\n",
    "                    elif \"test.json\" in dest_fn:\n",
    "                        dest_example = {\"uid\": example[\"uid\"], \"query_ids\": query_ids, \"context_key_ids\": context_key_ids,\n",
    "                                    \"context_value_ids\": context_value_ids, \"relation\": relation}\n",
    "                    else:\n",
    "                        raise ValueError(f\"{suffix} is not valid.\")\n",
    "                    fout.write(ujson.dumps(dest_example) + \"\\n\")\n",
    "            if \"bm25\" in dest_signature:\n",
    "                if \"search_sequential\" in suffix or \"compl_rec_sequential\" in suffix:\n",
    "                    print(\"bm25 suffix: {}'s miss_hids = {}\".format(suffix, len(miss_hids)))\n",
    "\n",
    "\n",
    "root_dir=\"/home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/\"\n",
    "eid_to_text = {}\n",
    "with open(os.path.join(root_dir, \"all_entities.tsv\")) as fin:\n",
    "    for line in fin:\n",
    "        eid, text = line.strip().split(\"\\t\")\n",
    "        eid_to_text[int(eid)] = text\n",
    "\n",
    "dest_dir = os.path.join(out_dir, \"without_context/\")\n",
    "if not os.path.exists(dest_dir):\n",
    "    os.mkdir(dest_dir)\n",
    "fn_to_example = {\n",
    "    os.path.join(dest_dir, \"uid_anchors.test.sim.tsv\"): (test_sim_rec_examples, SIM_RELATION),\n",
    "    os.path.join(dest_dir, \"uid_anchors.test.compl.tsv\"): (test_compl_rec_examples, COMPL_RELATION),\n",
    "    os.path.join(dest_dir, \"uid_queries.test.search.tsv\"): (test_search_examples, REL_RELATION)\n",
    "}\n",
    "for fn, (test_examples, relation) in fn_to_example.items():\n",
    "    with open(fn, \"w\") as fout:\n",
    "        for example in test_examples:\n",
    "            uid, query = example[\"uid\"], eid_to_text[example[\"query_ids\"][-1]]\n",
    "            fout.write(f\"{uid}\\t{query}\\t{relation}\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12628 /work/hzeng_umass_edu/ir-research/joint_modeling_search_and_rec/datasets/unified_kgc/unified_user/sequential_train_test/without_context/uid_anchors.test.compl.tsv\n",
      "6\tGeneral Shale Providence series 50-Pack Carbon 1/2-in x 8-in Tumbled Ceramic Brick Look Wall Tile ; Tile\tis_complementary_to\n",
      "92\tLegrand Plastic RCA to F-Type Wall Jack ; Audio & Video Wall Jacks\tis_complementary_to\n",
      "====================================================================================================\n",
      "10000 /work/hzeng_umass_edu/ir-research/joint_modeling_search_and_rec/datasets/unified_kgc/unified_user/sequential_train_test/without_context/uid_anchors.test.sim.small.tsv\n",
      "376229\tLG Smart Wi-Fi Enabled 4.5-cu ft High Efficiency Stackable Steam Cycle Front-Load Washer (Graphite Steel) ENERGY STAR ; Front-Load Washers\tis_similar_to\n",
      "490831\tClosetMaid BrightWood 5-ft to 10-ft W x 6.85-ft H White Wood Closet Kit ; Wood Closet Kits\tis_similar_to\n",
      "====================================================================================================\n",
      "10000 /work/hzeng_umass_edu/ir-research/joint_modeling_search_and_rec/datasets/unified_kgc/unified_user/sequential_train_test/without_context/uid_queries.test.search.small.tsv\n",
      "35926\tsoil\tis_relevant_to\n",
      "146571\tbathroom exhaust fan motor\tis_relevant_to\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "check_dir = os.path.join(out_dir, \"hlen_4_randneg\")\n",
    "\n",
    "for fn in os.listdir(check_dir):\n",
    "    if \"search\" not in fn or \"small\" in fn:\n",
    "        continue \n",
    "    fn = os.path.join(check_dir, fn)\n",
    "    \n",
    "    ! wc -l $fn \n",
    "    ! head -n 2 $fn\n",
    "    print(100*\"=\")\n",
    "\n",
    "for fn in os.listdir(out_dir):\n",
    "    if \"search\" not in fn or \"small\" in fn:\n",
    "        continue \n",
    "    fn = os.path.join(out_dir, fn)\n",
    "    \n",
    "    ! wc -l $fn \n",
    "    ! head -n 2 $fn\n",
    "    print(100*\"=\")\n",
    "\n",
    "print(100*\"=.\")\n",
    "\"\"\"\n",
    "check_dir = os.path.join(out_dir, \"without_context\")\n",
    "\n",
    "for fn in os.listdir(check_dir):\n",
    "    fn = os.path.join(check_dir, fn)\n",
    "    \n",
    "    ! wc -l $fn \n",
    "    ! head -n 2 $fn\n",
    "    print(100*\"=\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uid': 146571, 'query_ids': [2981289, 2658084, 2619697, 2438241, 2438241, 3012079, 2494185, 2730445, 2282185, 2650790, 3012079, 3012079, 3059960, 3170758, 2809002], 'context_key_ids': [2467143, 2981289, 2658084, 2619697, 2438241, 2438241, 3012079, 2494185, 2730445, 2282185, 2650790, 3012079, 3012079, 3059960, 3170758], 'context_value_ids': [420438, 1470982, 1430343, 200653, 2251545, 1302684, 1921262, 1276207, 1572750, 656803, 804622, 1545826, 1921262, 1461566, 555962]}\n",
      "bathroom exhaust fan motor\n"
     ]
    }
   ],
   "source": [
    "uid = 146571\n",
    "for example in test_search_examples:\n",
    "    if example[\"uid\"] == uid:\n",
    "        print(example)\n",
    "print(eid_to_text[2809002])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qid: sputnik light\n",
      " ctxpid : ReliaBilt 30001 Series 31.75-in x 15.75-in x 3-in Jamb Tilting Vinyl Replacement White Basement Hopper Window ; Basement Hopper Windows \n",
      " relpid : allen + roth Grayford 9-Light Brushed Nickel Mid-century Sputnik Pendant Light ; Pendant Lighting\n",
      "===========================================================================\n",
      "qid: pantry cabinet\n",
      " ctxpid : allen + roth Grayford 9-Light Brushed Nickel Mid-century Sputnik Pendant Light ; Pendant Lighting \n",
      " relpid : Project Source 18-in W x 84-in H x 23.75-in D Natural Unfinished Oak Door Pantry Fully Assembled Stock Cabinet (Square Door Style) ; Kitchen Cabinets\n",
      "===========================================================================\n",
      "qid: linoleum sheet flooring\n",
      " ctxpid : Project Source 18-in W x 84-in H x 23.75-in D Natural Unfinished Oak Door Pantry Fully Assembled Stock Cabinet (Square Door Style) ; Kitchen Cabinets \n",
      " relpid : Armstrong Flooring Pickwick Landing I 12-ft W Cut-to-Length Bear Path Oak Dark Brown Wood Look Low-Gloss Finish Sheet Vinyl ; Sheet Vinyl (Cut-to-Length)\n",
      "===========================================================================\n",
      "qid: shop heater\n",
      " ctxpid : Armstrong Flooring Pickwick Landing I 12-ft W Cut-to-Length Bear Path Oak Dark Brown Wood Look Low-Gloss Finish Sheet Vinyl ; Sheet Vinyl (Cut-to-Length) \n",
      " relpid : Dr. Infrared Heater 5600-Watt Infrared Portable Electric Garage Heater with Thermostat ; Electric Garage Heaters\n",
      "===========================================================================\n",
      "hi\n"
     ]
    }
   ],
   "source": [
    "root_dir=\"/home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/\n",
    "with open(os.path.join(root_dir, \"all_entities.tsv\")) as fin:\n",
    "    for line in fin:\n",
    "        eid, text = line.strip().split(\"\\t\")\n",
    "        eid_to_text[int(eid)] = text\n",
    "\n",
    "query_ids = [2417516,2374200, 2576318, 2836633 ]\n",
    "context_value_ids = [202549,1582436,1400531,520284]\n",
    "target_value_ids = [1582436,1400531,520284,27714]\n",
    "\n",
    "for qid, context_pid, target_pid in zip(query_ids, context_value_ids, target_value_ids):\n",
    "    print(\"qid: {}\\n ctxpid : {} \\n relpid : {}\".format(eid_to_text[qid],eid_to_text[context_pid], eid_to_text[target_pid]))\n",
    "    print(\"=\"*75)\n",
    "print(\"hi\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-9.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9c9569641937ce4addf5496cbe81a89e9d276a9857e7a9967fd6589fdce30733"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
