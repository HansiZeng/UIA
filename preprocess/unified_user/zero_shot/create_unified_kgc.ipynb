{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79a028cc-6b92-4b19-8346-71919c72cb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================ For anchor_to_compl_ivms: ===================================\n",
      "number of unique product = 2,260,878, anchors = 86,870, complementary_compl_ivms = 65,561\n",
      "================================ After updating anchor_to_similar_ivms: ===================================\n",
      "all_compl_ivms = 109,758, all_sim_ivms = 256,765\n",
      "sim_compl_intersect = 87,425 (0.797)\n",
      "all_ivms = 279,098\n",
      "all queries = 953,773\n",
      "total ivms (queries) = 360,744, length >=3 = 196,481, length >= 5 = 142,527\n",
      "================================ For anchor_to_compl_ivms: ===================================\n",
      "total_compl_ivms = 86,870, length >=3 = 35,837, length >= 5 = 22,121\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "bq_in_dir=\"/home/jupyter/data_transfer/data/\"\n",
    "sim_rec_df = pd.read_csv(os.path.join(bq_in_dir, \"hansi_rec_ClicksData_5core.csv\"), index_col=0)\n",
    "compl_rec_df = pd.read_csv(os.path.join(bq_in_dir, \"comp_rec_ClicksData_2core.csv\"), index_col=0)\n",
    "search_df = pd.read_csv(os.path.join(bq_in_dir, \"search_ClicksData_1year_5core.csv\"), index_col=0)\n",
    "product_df = pd.read_csv(os.path.join(bq_in_dir, \"all_products_info.csv\"), index_col=0)\n",
    "\n",
    "all_products = set(product_df.product_id)\n",
    "anchors = set(compl_rec_df.anchor)\n",
    "compl_ivms = set(compl_rec_df.ivm)\n",
    "all_compl_ivms = anchors.union(compl_ivms)\n",
    "\n",
    "print(\"================================ For anchor_to_compl_ivms: ===================================\")\n",
    "print(\"number of unique product = {:,}, anchors = {:,}, complementary_compl_ivms = {:,}\".format(len(all_products), len(anchors), len(compl_ivms)))\n",
    "assert len(all_products & anchors) == len(anchors) and len(all_products & compl_ivms) == len(compl_ivms),(\n",
    "    len(all_products & anchors), len(anchors), len(all_products & compl_ivms), len(compl_ivms)\n",
    ")\n",
    "\n",
    "all_sim_ivms = set(sim_rec_df.anchor).union(set(sim_rec_df.ivm))\n",
    "print(\"================================ After updating anchor_to_similar_ivms: ===================================\")\n",
    "print(\"all_compl_ivms = {:,}, all_sim_ivms = {:,}\".format(len(all_compl_ivms), len(all_sim_ivms)))\n",
    "print(\"sim_compl_intersect = {:,} ({:.3f})\".format(len(all_compl_ivms & all_sim_ivms), len(all_compl_ivms & all_sim_ivms) / len(all_compl_ivms)))\n",
    "print(\"all_ivms = {:,}\".format(len(all_compl_ivms | all_sim_ivms)))\n",
    "all_ivms = all_compl_ivms | all_sim_ivms\n",
    "\n",
    "assert len(all_products & all_ivms) == len(all_ivms), (len(all_products & all_ivms), len(all_ivms))\n",
    "\n",
    "query_to_ivms = search_df.groupby(\"query\")[\"ivm\"].apply(list)\n",
    "ivm_to_queries = search_df.groupby(\"ivm\")[\"query\"].apply(list)\n",
    "query_lengths = np.array([len(x) for x in ivm_to_queries.values])\n",
    "all_queries = set(search_df[\"query\"])\n",
    "print(\"all queries = {:,}\".format(len(all_queries)))\n",
    "assert len(all_queries) == len(query_to_ivms), len(query_to_ivms)\n",
    "print(\"total ivms (queries) = {:,}, length >=3 = {:,}, length >= 5 = {:,}\".format(\n",
    "    len(query_lengths), np.sum(query_lengths >=3), np.sum(query_lengths >= 5) ))\n",
    "\n",
    "anchor_to_compl_ivms = compl_rec_df.groupby(\"anchor\")[\"ivm\"].apply(list)\n",
    "compl_ivms_length = np.array([len(x) for x in anchor_to_compl_ivms.values])\n",
    "print(\"================================ For anchor_to_compl_ivms: ===================================\")\n",
    "print(\"total_compl_ivms = {:,}, length >=3 = {:,}, length >= 5 = {:,}\".format(len(compl_ivms_length), np.sum(compl_ivms_length >=3), np.sum(compl_ivms_length >= 5) ))\n",
    "\n",
    "anchor_to_sim_ivms = sim_rec_df.groupby(\"anchor\")[\"ivm\"].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7047b8cd-f2e4-4e62-bcc2-453cf26eb1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "user_dir = \"/home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/\"\n",
    "\n",
    "with open(os.path.join(user_dir, \"ivm_to_pid.pkl\"), \"rb\") as fin:\n",
    "    ivm_to_pid = pickle.load(fin)\n",
    "with open(os.path.join(user_dir, \"query_to_qid.pkl\"), \"rb\") as fin:\n",
    "    query_to_qid = pickle.load(fin)\n",
    "\n",
    "\n",
    "pid_to_qids = {ivm_to_pid[ivm]: [query_to_qid[query] for query in queries] for ivm, queries in ivm_to_queries.items()}\n",
    "qid_to_pids = {query_to_qid[query]: [ivm_to_pid[ivm] for ivm in ivms] for query, ivms in query_to_ivms.items()}\n",
    "aid_to_complpids = {ivm_to_pid[anchor]: [ivm_to_pid[prod] for prod in products] for anchor, products in anchor_to_compl_ivms.items()}\n",
    "aid_to_simpids = {ivm_to_pid[anchor]: [ivm_to_pid[prod] for prod in products] for anchor, products in anchor_to_sim_ivms.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55c65b2a-3b2a-487c-b214-526b7301c06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216238/216238 [00:00<00:00, 772479.93it/s]\n",
      "100%|██████████| 86870/86870 [00:00<00:00, 1004893.17it/s]\n",
      "100%|██████████| 953773/953773 [00:01<00:00, 894635.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of arels for sim_rec before exclusion and after exclusion = 968,778, 944,349\n",
      "number of arels for compl_rec before exclusion and after exclusion = 329,992, 324,746\n",
      "number of qrels for search before exclusion and after exclusion = 4,075,996, 4,056,138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1375625/1375625 [00:09<00:00, 142695.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of edges = 5,325,233, number of multi-attr edges = 17,830, (0.003)\n"
     ]
    }
   ],
   "source": [
    "# start create graph\n",
    "\n",
    "import random \n",
    "import pickle as pkl\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "random.seed(4680)\n",
    "\n",
    "test_dir = os.path.join(user_dir, \"zero_shot\")\n",
    "\n",
    "with open(os.path.join(test_dir, \"exclude_aid_to_simpids.pkl\"), \"rb\") as fin:\n",
    "    exclude_aid_to_simpids = pkl.load(fin)\n",
    "with open(os.path.join(test_dir, \"exclude_aid_to_complpids.pkl\"), \"rb\") as fin:\n",
    "    exclude_aid_to_complpids = pkl.load(fin)\n",
    "with open(os.path.join(test_dir, \"exclude_qid_to_relpids.pkl\"), \"rb\") as fin:\n",
    "    exclude_qid_to_relpids = pkl.load(fin)\n",
    "\n",
    "train_aid_to_simpids = {}\n",
    "for aid, simpids in tqdm(aid_to_simpids.items(), total=len(aid_to_simpids)):\n",
    "    if aid in exclude_aid_to_simpids:\n",
    "        keep_simpids = list(set(simpids).difference(set(exclude_aid_to_simpids[aid])))\n",
    "    else:\n",
    "        keep_simpids = simpids\n",
    "    train_aid_to_simpids[aid] = keep_simpids\n",
    "\n",
    "train_aid_to_complpids = {}\n",
    "for aid, complpids in tqdm(aid_to_complpids.items(), total=len(aid_to_complpids)):\n",
    "    if aid in exclude_aid_to_complpids:\n",
    "        keep_complpids = list(set(complpids).difference(set(exclude_aid_to_complpids[aid])))\n",
    "    else:\n",
    "        keep_complpids = complpids\n",
    "    train_aid_to_complpids[aid] = keep_complpids\n",
    "        \n",
    "train_qid_to_pids = {}\n",
    "for qid, pids in tqdm(qid_to_pids.items(), total=len(qid_to_pids)):\n",
    "    if qid in exclude_qid_to_relpids:\n",
    "        keep_relpids = list(set(pids).difference(exclude_qid_to_relpids[qid]))\n",
    "    else:\n",
    "        keep_relpids = pids \n",
    "    train_qid_to_pids[qid] = keep_relpids\n",
    "        \n",
    "\n",
    "print(\"number of arels for sim_rec before exclusion and after exclusion = {:,}, {:,}\".format(\n",
    "    sum([len(xs) for xs in aid_to_simpids.values()]), sum([len(xs) for xs in train_aid_to_simpids.values()])\n",
    "))\n",
    "print(\"number of arels for compl_rec before exclusion and after exclusion = {:,}, {:,}\".format(\n",
    "    sum([len(xs) for xs in aid_to_complpids.values()]), sum([len(xs) for xs in train_aid_to_complpids.values()])\n",
    "))\n",
    "print(\"number of qrels for search before exclusion and after exclusion = {:,}, {:,}\".format(\n",
    "    sum([len(xs) for xs in qid_to_pids.values()]), sum([len(xs) for xs in train_qid_to_pids.values()])\n",
    "))\n",
    "\n",
    "G = nx.MultiDiGraph()\n",
    "SIM_RELATION = \"is_similar_to\"\n",
    "COMPL_RELATION = \"is_complementary_to\"\n",
    "REL_RELATION = \"is_relevant_to\"\n",
    "\n",
    "for aid, sim_pids in train_aid_to_simpids.items():\n",
    "    triples = [(aid, sim_pid, {\"type\":SIM_RELATION}) for sim_pid in sim_pids]\n",
    "    G.add_edges_from(triples)\n",
    "    \n",
    "for aid, compl_pids in train_aid_to_complpids.items():\n",
    "    triples = [(aid, compl_pid, {\"type\":COMPL_RELATION}) for compl_pid in compl_pids]\n",
    "    G.add_edges_from(triples)\n",
    "    \n",
    "for qid, pids in train_qid_to_pids.items():\n",
    "    triples = [(pid, qid, {\"type\": REL_RELATION}) for pid in pids]\n",
    "    G.add_edges_from(triples)\n",
    "    \n",
    "multi_edge_pairs = []\n",
    "for n, nbrs_dict in tqdm(G.adj.items(), total=G.number_of_nodes()):\n",
    "    for nbr_node, edge_attrs in nbrs_dict.items():\n",
    "        assert len(edge_attrs) == 1 or len(edge_attrs) == 2\n",
    "        if len(edge_attrs) == 2:\n",
    "            multi_edge_pairs.append((n, nbr_node))\n",
    "            \n",
    "print(\"number of edges = {:,}, number of multi-attr edges = {:,}, ({:.3f})\".format(G.number_of_edges(), len(multi_edge_pairs), \n",
    "                                                                                   len(multi_edge_pairs)/G.number_of_edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b06863b-95e1-400a-a28d-8e073ec6bd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ignore hids = 6644\n"
     ]
    }
   ],
   "source": [
    "def create_triples(hid, pos_tid, miss_hids, duplicate_pairs, eid_to_text, sampler=None):\n",
    "    if sampler != None:\n",
    "        assert type(sampler) == dict, type(sampler)\n",
    "        if hid not in sampler:\n",
    "            miss_hids.append(hid)\n",
    "            return 0\n",
    "    if eid_to_text[hid] == eid_to_text[pos_tid]:\n",
    "        duplicate_pairs.append((hid, pos_tid))\n",
    "        return 0\n",
    "    \n",
    "    if sampler != None:\n",
    "        neg_tid = random.sample(sampler[hid], k=1)[0]\n",
    "        while neg_tid == pos_tid:\n",
    "            neg_tid = random.sample(sampler[hid], k=1)[0]\n",
    "    else:\n",
    "        neg_tid = random.sample(range(2_000_000), k=1)[0]\n",
    "        while neg_tid == pos_tid:\n",
    "            neg_tid = random.sample(range(2_000_000), k=1)[0]\n",
    "            \n",
    "    return (hid, pos_tid, neg_tid)\n",
    "\n",
    "\n",
    "eid_to_text = {}\n",
    "with open(os.path.join(user_dir, \"all_entities.tsv\")) as fin:\n",
    "    for line in fin:\n",
    "        eid, text = line.strip().split(\"\\t\")\n",
    "        eid_to_text[int(eid)] = text\n",
    "        \n",
    "run_path = os.path.join(user_dir, \"runs/bm25.all.run\")\n",
    "df = pd.read_csv(run_path, sep=\" \", names=[\"hid\", \"q0\", \"tid\", \"rank\", \"score\", \"model_name\"])\n",
    "bm25_hid_to_tids = {}\n",
    "ignore_hids = set()\n",
    "for hid, group in df.groupby(\"hid\"):\n",
    "    cand_tids = list(group.tid.values)\n",
    "    if len(cand_tids) < 10:\n",
    "        ignore_hids.add(int(hid))\n",
    "    else:\n",
    "        bm25_hid_to_tids[int(hid)] = [int(x) for x in cand_tids]\n",
    "        \n",
    "print(\"number of ignore hids = {}\".format(len(ignore_hids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a3508b0-a771-485c-95b6-d4b3d2f7f02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1375625/1375625 [00:26<00:00, 51996.84it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miss_hids = 0, duplicate_pairs = 33,036\n",
      "===========================================================================\n",
      "miss_hids = 0, duplicate_pairs = 33,423\n",
      "===========================================================================\n",
      "miss_hids = 38,299, duplicate_pairs = 33,423\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "max5_h2sp = {}\n",
    "max5_h2cp = {}\n",
    "max5_h2q = {}\n",
    "\n",
    "for head_node, nbrs_dict in tqdm(G.adj.items(), total=G.number_of_nodes()):\n",
    "    sim_pids = []\n",
    "    compl_pids = []\n",
    "    rel_qids = []\n",
    "    for tail_node, edge_attrs in nbrs_dict.items():\n",
    "        assert len(edge_attrs) == 1 or len(edge_attrs) == 2\n",
    "        relations = []\n",
    "        for no, edge_attr in edge_attrs.items():\n",
    "            relations.append(edge_attr[\"type\"])\n",
    "        for rel in relations:\n",
    "            assert rel in [SIM_RELATION, COMPL_RELATION, REL_RELATION]\n",
    "            if rel in SIM_RELATION:\n",
    "                sim_pids.append(tail_node)\n",
    "            if rel in COMPL_RELATION:\n",
    "                compl_pids.append(tail_node)\n",
    "            if rel in REL_RELATION:\n",
    "                rel_qids.append(tail_node)\n",
    "    if len(sim_pids) != 0:\n",
    "        max5_h2sp[head_node] = random.sample(sim_pids, k=len(sim_pids))[:5]\n",
    "    if len(compl_pids) != 0:\n",
    "        max5_h2cp[head_node] = random.sample(compl_pids, k=len(compl_pids))[:5]\n",
    "    if len(rel_qids) != 0:\n",
    "        max5_h2q[head_node] = random.sample(rel_qids, k=len(rel_qids))[:5]\n",
    "        \n",
    "miss_hids = []\n",
    "duplicate_pairs = []\n",
    "\n",
    "h2sp_triples = []\n",
    "h2cp_triples = []\n",
    "q2h_triples = []\n",
    "for hid, tail_ids in max5_h2sp.items():\n",
    "    for pos_tid in tail_ids:\n",
    "        triple = create_triples(hid, pos_tid, miss_hids, duplicate_pairs, eid_to_text)\n",
    "        if triple != 0:\n",
    "            h2sp_triples.append(triple)\n",
    "print(\"miss_hids = {:,}, duplicate_pairs = {:,}\".format(len(miss_hids), len(duplicate_pairs)))\n",
    "print(\"=\"*75)\n",
    "for hid, tail_ids in max5_h2cp.items():\n",
    "    for pos_tid in tail_ids:\n",
    "        triple = create_triples(hid, pos_tid, miss_hids, duplicate_pairs, eid_to_text, sampler=bm25_hid_to_tids)\n",
    "        if triple != 0:\n",
    "            h2cp_triples.append(triple)\n",
    "print(\"miss_hids = {:,}, duplicate_pairs = {:,}\".format(len(miss_hids), len(duplicate_pairs)))\n",
    "print(\"=\"*75)\n",
    "for pos_tid, head_ids in max5_h2q.items():\n",
    "    for hid in head_ids:\n",
    "        triple = create_triples(hid, pos_tid, miss_hids, duplicate_pairs, eid_to_text, sampler=bm25_hid_to_tids)\n",
    "        if triple != 0:\n",
    "            q2h_triples.append(triple)\n",
    "print(\"miss_hids = {:,}, duplicate_pairs = {:,}\".format(len(miss_hids), len(duplicate_pairs)))\n",
    "print(\"=\"*75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1db1ddad-f710-476d-8017-ee2abd7a1db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miss_hids = 132,829, duplicate_pairs = 33,423\n",
      "===========================================================================\n",
      "miss_hids = 210,529, duplicate_pairs = 33,423\n",
      "===========================================================================\n",
      "593676 220656 1072097 1373732\n"
     ]
    }
   ],
   "source": [
    "max5_q2p = {}\n",
    "max2_q2p = {}\n",
    "max5_q2p_triples = []\n",
    "max2_q2p_triples = []\n",
    "for qid, pids in train_qid_to_pids.items():\n",
    "    max5_q2p[qid] = random.sample(pids, k=len(pids))[:5]\n",
    "    max2_q2p[qid] = random.sample(pids, k=len(pids))[:2]\n",
    "    \n",
    "for qid, pos_pids in max5_q2p.items():\n",
    "    for pos_pid in pos_pids:\n",
    "        triple = create_triples(qid, pos_pid, miss_hids, duplicate_pairs, eid_to_text, sampler=bm25_hid_to_tids)\n",
    "        if triple != 0:\n",
    "            max5_q2p_triples.append(triple)\n",
    "print(\"miss_hids = {:,}, duplicate_pairs = {:,}\".format(len(miss_hids), len(duplicate_pairs)))\n",
    "print(\"=\"*75)\n",
    "\n",
    "for qid, pos_pids in max2_q2p.items():\n",
    "    for pos_pid in pos_pids:\n",
    "        triple = create_triples(qid, pos_pid, miss_hids, duplicate_pairs, eid_to_text, sampler=bm25_hid_to_tids)\n",
    "        if triple != 0:\n",
    "            max2_q2p_triples.append(triple)\n",
    "print(\"miss_hids = {:,}, duplicate_pairs = {:,}\".format(len(miss_hids), len(duplicate_pairs)))\n",
    "print(\"=\"*75)\n",
    "print(len(h2sp_triples), len(h2cp_triples), len(q2h_triples), len(max2_q2p_triples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74fc77da-febb-411f-a393-edaa994bb2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import ujson\n",
    "\n",
    "# downsample max2_q2p_triples\n",
    "keep_q2p_num = 1_200_000\n",
    "all_max2_q2p_triples = max2_q2p_triples\n",
    "max2_q2p_triples = random.sample(all_max2_q2p_triples, k=keep_q2p_num)\n",
    "\n",
    "kgc_train_dir = os.path.join(test_dir, \"unified_kgc_train\")\n",
    "if not os.path.exists(kgc_train_dir):\n",
    "    os.mkdir(kgc_train_dir)\n",
    "\n",
    "fn_to_tripleNrel = {\n",
    "    \"a2sp.train.tsv\": (h2sp_triples, SIM_RELATION),\n",
    "    \"a2cp.train.tsv\": (h2cp_triples, COMPL_RELATION),\n",
    "    \"max2_qorient_q2p.train.tsv\": (max2_q2p_triples, REL_RELATION),\n",
    "}\n",
    "\n",
    "for fn, (triples, relation) in fn_to_tripleNrel.items():\n",
    "    with open(os.path.join(kgc_train_dir, fn), \"w\") as fout:\n",
    "        for (hid, pos_tid, neg_tid) in triples:\n",
    "            fout.write(f\"{hid}\\t{pos_tid}\\t{neg_tid}\\t{relation}\\n\")\n",
    "            \n",
    "\n",
    "# create test dir \n",
    "kgc_test_dir = os.path.join(test_dir, \"unified_kgc_test_without_ctx\")\n",
    "if not os.path.exists(kgc_test_dir):\n",
    "    os.mkdir(kgc_test_dir)\n",
    "\n",
    "fin_to_fout = {\n",
    "    os.path.join(test_dir, \"sequential_train_test_hlen_4_bm25/sim_rec_sequential.test.json\"): os.path.join(kgc_test_dir, \"tid_anchors.sim.test.tsv\"),\n",
    "    os.path.join(test_dir, \"sequential_train_test_hlen_4_bm25/compl_rec_sequential.test.json\"): os.path.join(kgc_test_dir, \"tid_anchors.compl.test.tsv\"),\n",
    "    os.path.join(test_dir, \"sequential_train_test_hlen_4_bm25/search_sequential.test.json\"): os.path.join(kgc_test_dir, \"tid_queries.search.test.tsv\")\n",
    "}\n",
    "for in_fn, out_fn in fin_to_fout.items():\n",
    "    with open(in_fn) as fin:\n",
    "        with open(out_fn, \"w\") as fout:\n",
    "            for line in fin:\n",
    "                example = ujson.loads(line)\n",
    "                tid, qid, relation = example[\"test_id\"], example[\"query_ids\"][-1], example[\"relation\"]\n",
    "                fout.write(f\"{tid}\\t{eid_to_text[qid]}\\t{relation}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fd2fc18-0fc2-4905-9f34-a63be5c5858e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "593676 /home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/zero_shot/unified_kgc_train/a2sp.train.tsv\n",
      "1048567\t331101\t1389878\tis_similar_to\n",
      "1048567\t1144496\t1386487\tis_similar_to\n",
      "1144496\t789117\t1172136\tis_similar_to\n",
      "1891760\t2233677\t1194885\tis_similar_to\n",
      "352886\t1068443\t1463456\tis_similar_to\n",
      "615570\t1017607\t433181\tis_similar_to\n",
      "====================================================================================================\n",
      "220656 /home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/zero_shot/unified_kgc_train/a2cp.train.tsv\n",
      "331101\t1221769\t813160\tis_complementary_to\n",
      "331101\t1937825\t1861909\tis_complementary_to\n",
      "331101\t1503775\t1144496\tis_complementary_to\n",
      "364149\t41477\t559183\tis_complementary_to\n",
      "187314\t1236902\t2085856\tis_complementary_to\n",
      "1864309\t2154277\t396790\tis_complementary_to\n",
      "====================================================================================================\n",
      "1200000 /home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/zero_shot/unified_kgc_train/max2_qorient_q2p.train.tsv\n",
      "2435046\t1531376\t2142901\tis_relevant_to\n",
      "2773273\t1314329\t765662\tis_relevant_to\n",
      "2660965\t1413207\t417494\tis_relevant_to\n",
      "2470627\t8625\t1022409\tis_relevant_to\n",
      "2283652\t1587086\t1567213\tis_relevant_to\n",
      "2916540\t661987\t1563919\tis_relevant_to\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "for path in os.listdir(kgc_train_dir):\n",
    "    path = os.path.join(kgc_train_dir, path)\n",
    "    if \"train_graph.pkl\" in path:\n",
    "        continue\n",
    "    ! wc -l $path\n",
    "    ! head -n 3 $path\n",
    "    ! tail -n 3 $path\n",
    "    print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9148d9c4-1c3c-4c51-9209-7b310e6f0025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1048567 ReliaBilt 11/16-in x 12-in x 6-ft Western Red Cedar Dog Ear Fence Picket ; Wood Fence Pickets\n",
      "331101  5/8-in x 5-1/2-in x 6-ft Cedar Dog Ear Fence Picket ; Wood Fence Pickets\n",
      "1389878 Caroline's Treasures 14-in W x 21-in L x 0.2-in H Fabric Drying Mat ; Dish Racks & Trays\n"
     ]
    }
   ],
   "source": [
    "hid, pos_tid, neg_tid = (1048567,331101,1389878)\n",
    "print(hid, eid_to_text[hid])\n",
    "print(pos_tid, eid_to_text[pos_tid])\n",
    "print(neg_tid, eid_to_text[neg_tid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd978e6b-a3c2-42b5-95a9-3c0a7cf684ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3612 /home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/zero_shot/unified_kgc_test_without_ctx/tid_anchors.compl.test.tsv\n",
      "0\tMr. Heater Buddy heaters 9000-BTU Outdoor Portable Radiant Propane Heater ; Propane Heaters\tis_complementary_to\n",
      "1\tPit Boss Meat Probe 2-Pack Stainless Steel Accessory Kit ; Grilling Tools & Utensils\tis_complementary_to\n",
      "2\tPit Boss Pit Boss Pro Series 4 Series Vertical Smoker ; Pellet Smokers\tis_complementary_to\n",
      "3\tReflectix 2-in x 30-ft Reflective Insulation Tape ; Reflective Insulation Tape\tis_complementary_to\n",
      "4\t R-, 0.75-in x 4-ft x 8-ft Expanded Polystyrene Board Insulation ; Board Insulation\tis_complementary_to\n",
      "3607\tQuickie BULLDOZER 9-in Poly Fiber Stiff Deck Brush ; Deck Brushes\tis_complementary_to\n",
      "3608\tSeal-Krete Interior/Exterior Concentrated Cleaner and Degreaser (1-Gallon) ; Concrete Preparation\tis_complementary_to\n",
      "3609\tAmerimax Contemporary 4-in x 6-in White Half Round Gutter Drop Outlet ; Gutters\tis_complementary_to\n",
      "3610\tHillman 10 x 2-1/2-in Wood To Wood Deck Screws (1000) ; Deck Screws\tis_complementary_to\n",
      "3611\tAmerimax Traditional 5-in x 9-in White K Style Gutter End with Drop ; Gutters\tis_complementary_to\n",
      "====================================================================================================\n",
      "22884 /home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/zero_shot/unified_kgc_test_without_ctx/tid_anchors.sim.test.tsv\n",
      "0\tWeber 18 in. Bamboo 18-in Grill Brush ; Grill Brushes & Cleaning Blocks\tis_similar_to\n",
      "1\tB&B Charcoal Natural lump charcoal 15-lb Lump Charcoal ; Charcoal & Accessories\tis_similar_to\n",
      "2\tCowboy Charcoal Cowboy lump charcoal 15-lb Lump Charcoal ; Charcoal & Accessories\tis_similar_to\n",
      "3\tBosch 100 48-Decibel Top Control 24-in Built-In Dishwasher (Stainless Steel) ENERGY STAR ; Built-In Dishwashers\tis_similar_to\n",
      "4\tBosch 100 48-Decibel Top Control 24-in Built-In Dishwasher (Stainless Steel) ENERGY STAR ; Built-In Dishwashers\tis_similar_to\n",
      "22879\tShaw Floorigami Dynamic Vision Shoreline DIY Carpet 12-Pack 9-in Shoreline Pattern Peel-and-Stick Carpet Tile ; Carpet Tile\tis_similar_to\n",
      "22880\tShaw Floorigami Dynamic Vision Shoreline DIY Carpet 12-Pack 9-in Shoreline Pattern Peel-and-Stick Carpet Tile ; Carpet Tile\tis_similar_to\n",
      "22881\tFreedom 6-ft H x 3-in W White Vinyl Fence Gate Kit ; Vinyl Fencing\tis_similar_to\n",
      "22882\tFreedom 6-ft H x 6-ft W White Vinyl Flat-top ; Vinyl Fencing\tis_similar_to\n",
      "22883\tFreedom Everton 6-ft H x 6-ft W White Vinyl Flat-top Fence Panel ; Vinyl Fencing\tis_similar_to\n",
      "====================================================================================================\n",
      "29658 /home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/zero_shot/unified_kgc_test_without_ctx/tid_queries.search.test.tsv\n",
      "0\tdimmer switch\tis_relevant_to\n",
      "1\tdishwasher hose kit\tis_relevant_to\n",
      "2\tkitchen faucet with sprayer\tis_relevant_to\n",
      "3\toak boards\tis_relevant_to\n",
      "4\tgarden netting\tis_relevant_to\n",
      "29653\ttoilet bowl cleaner\tis_relevant_to\n",
      "29654\tbucket and mop\tis_relevant_to\n",
      "29655\tshed window\tis_relevant_to\n",
      "29656\t4x4\tis_relevant_to\n",
      "29657\ttractor\tis_relevant_to\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "for path in os.listdir(kgc_test_dir):\n",
    "    path = os.path.join(kgc_test_dir, path)\n",
    "    ! wc -l $path\n",
    "    ! head -n 5 $path\n",
    "    ! tail -n 5 $path\n",
    "    print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "375f9c7c-cfa6-4ba0-b068-c205eb6fd3c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reflectix 2-in x 30-ft Reflective Insulation Tape ; Reflective Insulation Tape'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eid_to_text[2216303]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15d00ac-9863-4be6-b19f-a2f54b02779d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-9.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
