{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6593c73-63a9-41e3-8018-53a058770faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "bq_in_dir=\"/home/jupyter/data_transfer/data/\"\n",
    "sim_rec_df = pd.read_csv(os.path.join(bq_in_dir, \"hansi_rec_ClicksData_5core.csv\"), index_col=0)\n",
    "compl_rec_df = pd.read_csv(os.path.join(bq_in_dir, \"comp_rec_ClicksData_2core.csv\"), index_col=0)\n",
    "search_df = pd.read_csv(os.path.join(bq_in_dir, \"search_ClicksData_1year_5core.csv\"), index_col=0)\n",
    "product_df = pd.read_csv(os.path.join(bq_in_dir, \"all_products_info.csv\"), index_col=0)\n",
    "\n",
    "all_products = set(product_df.product_id)\n",
    "anchors = set(compl_rec_df.anchor)\n",
    "compl_ivms = set(compl_rec_df.ivm)\n",
    "all_compl_ivms = anchors.union(compl_ivms)\n",
    "\n",
    "print(\"================================ For anchor_to_compl_ivms: ===================================\")\n",
    "print(\"number of unique product = {:,}, anchors = {:,}, complementary_compl_ivms = {:,}\".format(len(all_products), len(anchors), len(compl_ivms)))\n",
    "assert len(all_products & anchors) == len(anchors) and len(all_products & compl_ivms) == len(compl_ivms),(\n",
    "    len(all_products & anchors), len(anchors), len(all_products & compl_ivms), len(compl_ivms)\n",
    ")\n",
    "\n",
    "all_sim_ivms = set(sim_rec_df.anchor).union(set(sim_rec_df.ivm))\n",
    "print(\"================================ After updating anchor_to_similar_ivms: ===================================\")\n",
    "print(\"all_compl_ivms = {:,}, all_sim_ivms = {:,}\".format(len(all_compl_ivms), len(all_sim_ivms)))\n",
    "print(\"sim_compl_intersect = {:,} ({:.3f})\".format(len(all_compl_ivms & all_sim_ivms), len(all_compl_ivms & all_sim_ivms) / len(all_compl_ivms)))\n",
    "print(\"all_ivms = {:,}\".format(len(all_compl_ivms | all_sim_ivms)))\n",
    "all_ivms = all_compl_ivms | all_sim_ivms\n",
    "\n",
    "assert len(all_products & all_ivms) == len(all_ivms), (len(all_products & all_ivms), len(all_ivms))\n",
    "\n",
    "query_to_ivms = search_df.groupby(\"query\")[\"ivm\"].apply(list)\n",
    "ivm_to_queries = search_df.groupby(\"ivm\")[\"query\"].apply(list)\n",
    "query_lengths = np.array([len(x) for x in ivm_to_queries.values])\n",
    "all_queries = set(search_df[\"query\"])\n",
    "print(\"all queries = {:,}\".format(len(all_queries)))\n",
    "assert len(all_queries) == len(query_to_ivms), len(query_to_ivms)\n",
    "print(\"total ivms (queries) = {:,}, length >=3 = {:,}, length >= 5 = {:,}\".format(\n",
    "    len(query_lengths), np.sum(query_lengths >=3), np.sum(query_lengths >= 5) ))\n",
    "\n",
    "anchor_to_compl_ivms = compl_rec_df.groupby(\"anchor\")[\"ivm\"].apply(list)\n",
    "compl_ivms_length = np.array([len(x) for x in anchor_to_compl_ivms.values])\n",
    "print(\"================================ For anchor_to_compl_ivms: ===================================\")\n",
    "print(\"total_compl_ivms = {:,}, length >=3 = {:,}, length >= 5 = {:,}\".format(len(compl_ivms_length), np.sum(compl_ivms_length >=3), np.sum(compl_ivms_length >= 5) ))\n",
    "\n",
    "anchor_to_sim_ivms = sim_rec_df.groupby(\"anchor\")[\"ivm\"].apply(list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc682d49-ac84-4bca-8f51-1f623a8231df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anchor</th>\n",
       "      <th>ivm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>267048-70-45619</td>\n",
       "      <td>43344-70-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>149825-1147-EU5</td>\n",
       "      <td>45575-0-EG5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>773301-215-003168</td>\n",
       "      <td>4934-215-NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1944512-72913-EPB44PHDG</td>\n",
       "      <td>6483-215-NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>191956-37672-260272</td>\n",
       "      <td>6483-215-NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329987</th>\n",
       "      <td>4767637-350-300BRNL SQT 514 RCS</td>\n",
       "      <td>3672733-350-816 SQT 514 SMT RCAL RCS RBP KC IN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329988</th>\n",
       "      <td>1798630-350-730HFL SQT 514 CP</td>\n",
       "      <td>3672733-350-816 SQT 514 SMT RCAL RCS RBP KC IN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329989</th>\n",
       "      <td>804702-350-730HFL SQT 514 6AL</td>\n",
       "      <td>3672733-350-816 SQT 514 SMT RCAL RCS RBP KC IN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329990</th>\n",
       "      <td>4767626-350-818PGHXPSK RDT 514</td>\n",
       "      <td>3672733-350-816 SQT 514 SMT RCAL RCS RBP KC IN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329991</th>\n",
       "      <td>65249-50881-VK1104</td>\n",
       "      <td>408051-50881-VK1132BL 6CS KYD FLUSH MNT W/WD H...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>329992 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 anchor  \\\n",
       "0                       267048-70-45619   \n",
       "1                       149825-1147-EU5   \n",
       "2                     773301-215-003168   \n",
       "3               1944512-72913-EPB44PHDG   \n",
       "4                   191956-37672-260272   \n",
       "...                                 ...   \n",
       "329987  4767637-350-300BRNL SQT 514 RCS   \n",
       "329988    1798630-350-730HFL SQT 514 CP   \n",
       "329989    804702-350-730HFL SQT 514 6AL   \n",
       "329990   4767626-350-818PGHXPSK RDT 514   \n",
       "329991               65249-50881-VK1104   \n",
       "\n",
       "                                                      ivm  \n",
       "0                                             43344-70-17  \n",
       "1                                             45575-0-EG5  \n",
       "2                                             4934-215-NA  \n",
       "3                                             6483-215-NA  \n",
       "4                                             6483-215-NA  \n",
       "...                                                   ...  \n",
       "329987  3672733-350-816 SQT 514 SMT RCAL RCS RBP KC IN...  \n",
       "329988  3672733-350-816 SQT 514 SMT RCAL RCS RBP KC IN...  \n",
       "329989  3672733-350-816 SQT 514 SMT RCAL RCS RBP KC IN...  \n",
       "329990  3672733-350-816 SQT 514 SMT RCAL RCS RBP KC IN...  \n",
       "329991  408051-50881-VK1132BL 6CS KYD FLUSH MNT W/WD H...  \n",
       "\n",
       "[329992 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compl_rec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44ca3995-6c37-409a-9ade-8298335f0492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "user_dir = \"/home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/\"\n",
    "\n",
    "with open(os.path.join(user_dir, \"ivm_to_pid.pkl\"), \"rb\") as fin:\n",
    "    ivm_to_pid = pickle.load(fin)\n",
    "with open(os.path.join(user_dir, \"query_to_qid.pkl\"), \"rb\") as fin:\n",
    "    query_to_qid = pickle.load(fin)\n",
    "\n",
    "\n",
    "pid_to_qids = {ivm_to_pid[ivm]: [query_to_qid[query] for query in queries] for ivm, queries in ivm_to_queries.items()}\n",
    "qid_to_pids = {query_to_qid[query]: [ivm_to_pid[ivm] for ivm in ivms] for query, ivms in query_to_ivms.items()}\n",
    "aid_to_complpids = {ivm_to_pid[anchor]: [ivm_to_pid[prod] for prod in products] for anchor, products in anchor_to_compl_ivms.items()}\n",
    "aid_to_simpids = {ivm_to_pid[anchor]: [ivm_to_pid[prod] for prod in products] for anchor, products in anchor_to_sim_ivms.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a110cf69-c8f4-4386-995b-bc572a72d1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start create graph\n",
    "\n",
    "import random \n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "random.seed(4680)\n",
    "\n",
    "test_dir = os.path.join(user_dir, \"selected_test_user\")\n",
    "kgc_dir = \"/home/jupyter/unity_jointly_rec_and_search/datasets/unified_kgc/\"\n",
    "\n",
    "test_sim_aids, test_compl_aids, test_search_qids = set(), set(), set()\n",
    "fn_to_qids = {\n",
    "    os.path.join(test_dir, \"anchors.sim.test.tsv\"): test_sim_aids,\n",
    "    os.path.join(test_dir, \"anchors.compl.test.tsv\"): test_compl_aids,\n",
    "    os.path.join(test_dir, \"queries.search.test.tsv\"): test_search_qids\n",
    "}\n",
    "for fn, qids in fn_to_qids.items():\n",
    "    with open(fn) as fin:\n",
    "        for line in fin:\n",
    "            qid, _ = line.strip().split(\"\\t\")\n",
    "            qids.add(int(qid))\n",
    "\n",
    "train_aid_to_simpids, test_aid_to_simpids = {}, {}\n",
    "for aid, simpids in tqdm(aid_to_simpids.items(), total=len(aid_to_simpids)):\n",
    "    if aid in test_sim_aids:\n",
    "        test_aid_to_simpids[aid] = simpids\n",
    "    else:\n",
    "        train_aid_to_simpids[aid] = simpids       \n",
    "\n",
    "train_aid_to_complpids, test_aid_to_complpids = {}, {}\n",
    "for aid, complpids in tqdm(aid_to_complpids.items(), total=len(aid_to_complpids)):\n",
    "    if aid in test_compl_aids:\n",
    "        test_aid_to_complpids[aid] = complpids\n",
    "    else:\n",
    "        train_aid_to_complpids[aid] = complpids\n",
    "        \n",
    "train_qid_to_pids, test_qid_to_pids = {}, {}\n",
    "for qid, pids in tqdm(qid_to_pids.items(), total=len(qid_to_pids)):\n",
    "    if qid in test_search_qids:\n",
    "        test_qid_to_pids[qid] = pids\n",
    "    else:\n",
    "        train_qid_to_pids[qid] = pids\n",
    "        \n",
    "print(\"number of aid_to_simpids  train = {:,}, test = {:,}\".format(len(train_aid_to_simpids), \n",
    "                                                                     len(test_aid_to_simpids)))\n",
    "print(\"number of aid_to_complpids train = {:,}, test = {:,}\".format(len(train_aid_to_complpids), \n",
    "                                                                     len(test_aid_to_complpids)))\n",
    "print(\"number of qid_to_pids train = {:,}, test = {:,}\".format(len(train_qid_to_pids), \n",
    "                                                                      len(test_qid_to_pids)))\n",
    "\n",
    "assert len( set(train_aid_to_simpids.keys()) &  set(test_aid_to_simpids.keys()) ) == 0\n",
    "assert len( set(train_aid_to_complpids.keys())  & set(test_aid_to_complpids.keys())) == 0\n",
    "assert len( set(train_qid_to_pids.keys())  & set(test_qid_to_pids.keys())) == 0\n",
    "\n",
    "G = nx.MultiDiGraph()\n",
    "SIM_RELATION = \"is_similar_to\"\n",
    "COMPL_RELATION = \"is_complementary_to\"\n",
    "REL_RELATION = \"is_relevant_to\"\n",
    "\n",
    "for aid, sim_pids in train_aid_to_simpids.items():\n",
    "    triples = [(aid, sim_pid, {\"type\":SIM_RELATION}) for sim_pid in sim_pids]\n",
    "    G.add_edges_from(triples)\n",
    "    \n",
    "for aid, compl_pids in train_aid_to_complpids.items():\n",
    "    triples = [(aid, compl_pid, {\"type\":COMPL_RELATION}) for compl_pid in compl_pids]\n",
    "    G.add_edges_from(triples)\n",
    "    \n",
    "for qid, pids in train_qid_to_pids.items():\n",
    "    triples = [(pid, qid, {\"type\": REL_RELATION}) for pid in pids]\n",
    "    G.add_edges_from(triples)\n",
    "    \n",
    "multi_edge_pairs = []\n",
    "for n, nbrs_dict in tqdm(G.adj.items(), total=G.number_of_nodes()):\n",
    "    for nbr_node, edge_attrs in nbrs_dict.items():\n",
    "        assert len(edge_attrs) == 1 or len(edge_attrs) == 2\n",
    "        if len(edge_attrs) == 2:\n",
    "            multi_edge_pairs.append((n, nbr_node))\n",
    "            \n",
    "print(\"number of edges = {:,}, number of multi-attr edges = {:,}, ({:.3f})\".format(G.number_of_edges(), len(multi_edge_pairs), \n",
    "                                                                                   len(multi_edge_pairs)/G.number_of_edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7641ddd-5a3f-4423-b0b5-90cf59e36b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ignore hids = 6644\n"
     ]
    }
   ],
   "source": [
    "def create_triples(hid, pos_tid, miss_hids, duplicate_pairs, eid_to_text, sampler=None):\n",
    "    if sampler != None:\n",
    "        assert type(sampler) == dict, type(sampler)\n",
    "        if hid not in sampler:\n",
    "            miss_hids.append(hid)\n",
    "            return 0\n",
    "    if eid_to_text[hid] == eid_to_text[pos_tid]:\n",
    "        duplicate_pairs.append((hid, pos_tid))\n",
    "        return 0\n",
    "    \n",
    "    if sampler != None:\n",
    "        neg_tid = random.sample(sampler[hid], k=1)[0]\n",
    "        while neg_tid == pos_tid:\n",
    "            neg_tid = random.sample(sampler[hid], k=1)[0]\n",
    "    else:\n",
    "        neg_tid = random.sample(range(2_000_000), k=1)[0]\n",
    "        while neg_tid == pos_tid:\n",
    "            neg_tid = random.sample(range(2_000_000), k=1)[0]\n",
    "            \n",
    "    return (hid, pos_tid, neg_tid)\n",
    "\n",
    "\n",
    "eid_to_text = {}\n",
    "with open(os.path.join(user_dir, \"all_entities.tsv\")) as fin:\n",
    "    for line in fin:\n",
    "        eid, text = line.strip().split(\"\\t\")\n",
    "        eid_to_text[int(eid)] = text\n",
    "        \n",
    "run_path = os.path.join(user_dir, \"runs/bm25.all.run\")\n",
    "df = pd.read_csv(run_path, sep=\" \", names=[\"hid\", \"q0\", \"tid\", \"rank\", \"score\", \"model_name\"])\n",
    "bm25_hid_to_tids = {}\n",
    "ignore_hids = set()\n",
    "for hid, group in df.groupby(\"hid\"):\n",
    "    cand_tids = list(group.tid.values)\n",
    "    if len(cand_tids) < 10:\n",
    "        ignore_hids.add(int(hid))\n",
    "    else:\n",
    "        bm25_hid_to_tids[int(hid)] = [int(x) for x in cand_tids]\n",
    "        \n",
    "print(\"number of ignore hids = {}\".format(len(ignore_hids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6e25e1c-27b1-408c-82a7-7226b3997c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1361782/1361782 [00:26<00:00, 51086.91it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miss_hids = 0, duplicate_pairs = 32,389\n",
      "===========================================================================\n",
      "miss_hids = 0, duplicate_pairs = 32,755\n",
      "===========================================================================\n",
      "miss_hids = 39,026, duplicate_pairs = 32,755\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "max5_h2sp = {}\n",
    "max5_h2cp = {}\n",
    "max5_h2q = {}\n",
    "\n",
    "for head_node, nbrs_dict in tqdm(G.adj.items(), total=G.number_of_nodes()):\n",
    "    sim_pids = []\n",
    "    compl_pids = []\n",
    "    rel_qids = []\n",
    "    for tail_node, edge_attrs in nbrs_dict.items():\n",
    "        assert len(edge_attrs) == 1 or len(edge_attrs) == 2\n",
    "        relations = []\n",
    "        for no, edge_attr in edge_attrs.items():\n",
    "            relations.append(edge_attr[\"type\"])\n",
    "        for rel in relations:\n",
    "            assert rel in [SIM_RELATION, COMPL_RELATION, REL_RELATION]\n",
    "            if rel in SIM_RELATION:\n",
    "                sim_pids.append(tail_node)\n",
    "            if rel in COMPL_RELATION:\n",
    "                compl_pids.append(tail_node)\n",
    "            if rel in REL_RELATION:\n",
    "                rel_qids.append(tail_node)\n",
    "    if len(sim_pids) != 0:\n",
    "        max5_h2sp[head_node] = random.sample(sim_pids, k=len(sim_pids))[:5]\n",
    "    if len(compl_pids) != 0:\n",
    "        max5_h2cp[head_node] = random.sample(compl_pids, k=len(compl_pids))[:5]\n",
    "    if len(rel_qids) != 0:\n",
    "        max5_h2q[head_node] = random.sample(rel_qids, k=len(rel_qids))[:5]\n",
    "        \n",
    "miss_hids = []\n",
    "duplicate_pairs = []\n",
    "\n",
    "h2sp_triples = []\n",
    "h2cp_triples = []\n",
    "q2h_triples = []\n",
    "for hid, tail_ids in max5_h2sp.items():\n",
    "    for pos_tid in tail_ids:\n",
    "        triple = create_triples(hid, pos_tid, miss_hids, duplicate_pairs, eid_to_text)\n",
    "        if triple != 0:\n",
    "            h2sp_triples.append(triple)\n",
    "print(\"miss_hids = {:,}, duplicate_pairs = {:,}\".format(len(miss_hids), len(duplicate_pairs)))\n",
    "print(\"=\"*75)\n",
    "for hid, tail_ids in max5_h2cp.items():\n",
    "    for pos_tid in tail_ids:\n",
    "        triple = create_triples(hid, pos_tid, miss_hids, duplicate_pairs, eid_to_text, sampler=bm25_hid_to_tids)\n",
    "        if triple != 0:\n",
    "            h2cp_triples.append(triple)\n",
    "print(\"miss_hids = {:,}, duplicate_pairs = {:,}\".format(len(miss_hids), len(duplicate_pairs)))\n",
    "print(\"=\"*75)\n",
    "for pos_tid, head_ids in max5_h2q.items():\n",
    "    for hid in head_ids:\n",
    "        triple = create_triples(hid, pos_tid, miss_hids, duplicate_pairs, eid_to_text, sampler=bm25_hid_to_tids)\n",
    "        if triple != 0:\n",
    "            q2h_triples.append(triple)\n",
    "print(\"miss_hids = {:,}, duplicate_pairs = {:,}\".format(len(miss_hids), len(duplicate_pairs)))\n",
    "print(\"=\"*75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "383b3f82-48c6-4a89-853b-775c1420fafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miss_hids = 133,224, duplicate_pairs = 32,755\n",
      "===========================================================================\n",
      "miss_hids = 210,819, duplicate_pairs = 32,755\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "max5_q2p = {}\n",
    "max2_q2p = {}\n",
    "max5_q2p_triples = []\n",
    "max2_q2p_triples = []\n",
    "for qid, pids in train_qid_to_pids.items():\n",
    "    max5_q2p[qid] = random.sample(pids, k=len(pids))[:5]\n",
    "    max2_q2p[qid] = random.sample(pids, k=len(pids))[:2]\n",
    "    \n",
    "for qid, pos_pids in max5_q2p.items():\n",
    "    for pos_pid in pos_pids:\n",
    "        triple = create_triples(qid, pos_pid, miss_hids, duplicate_pairs, eid_to_text, sampler=bm25_hid_to_tids)\n",
    "        if triple != 0:\n",
    "            max5_q2p_triples.append(triple)\n",
    "print(\"miss_hids = {:,}, duplicate_pairs = {:,}\".format(len(miss_hids), len(duplicate_pairs)))\n",
    "print(\"=\"*75)\n",
    "\n",
    "for qid, pos_pids in max2_q2p.items():\n",
    "    for pos_pid in pos_pids:\n",
    "        triple = create_triples(qid, pos_pid, miss_hids, duplicate_pairs, eid_to_text, sampler=bm25_hid_to_tids)\n",
    "        if triple != 0:\n",
    "            max2_q2p_triples.append(triple)\n",
    "print(\"miss_hids = {:,}, duplicate_pairs = {:,}\".format(len(miss_hids), len(duplicate_pairs)))\n",
    "print(\"=\"*75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9571691-3da0-4fdd-9ad2-394d3b342217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563262 192324 1017083 1364315\n"
     ]
    }
   ],
   "source": [
    "print(len(h2sp_triples), len(h2cp_triples), len(q2h_triples), len(max2_q2p_triples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2951ab78-320d-474e-9b61-ab7d7dc7b58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# downsample max2_q2p_triples\n",
    "keep_q2p_num = 1_200_000\n",
    "all_max2_q2p_triples = max2_q2p_triples\n",
    "max2_q2p_triples = random.sample(all_max2_q2p_triples, k=keep_q2p_num)\n",
    "\n",
    "\n",
    "if not os.path.exists(kgc_dir):\n",
    "    os.mkdir(kgc_dir)\n",
    "out_dir = os.path.join(kgc_dir, \"unified_train/\")\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "\n",
    "with open(os.path.join(out_dir, \"train_graph.pkl\"), \"wb\") as fout:\n",
    "    pickle.dump(G, fout)\n",
    "\n",
    "fn_to_tripleNrel = {\n",
    "    \"a2sp.train.tsv\": (h2sp_triples, SIM_RELATION),\n",
    "    \"a2cp.train.tsv\": (h2cp_triples, COMPL_RELATION),\n",
    "    \"q2a.train.tsv\": (q2h_triples, REL_RELATION),   \n",
    "    \"max2_qorient_q2p.train.tsv\": (max2_q2p_triples, REL_RELATION),\n",
    "}\n",
    "\n",
    "for fn, (triples, relation) in fn_to_tripleNrel.items():\n",
    "    with open(os.path.join(out_dir, fn), \"w\") as fout:\n",
    "        for (hid, pos_tid, neg_tid) in triples:\n",
    "            fout.write(f\"{hid}\\t{pos_tid}\\t{neg_tid}\\t{relation}\\n\")\n",
    "            \n",
    "out_dir = os.path.join(kgc_dir, \"unified_test/\")\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "\n",
    "# for similar items\n",
    "fn_to_aids = {\n",
    "    \"anchors.train.sim.tsv\": list(train_aid_to_simpids.keys()),\n",
    "    \"anchors.test.sim.tsv\": list(test_aid_to_simpids.keys()),\n",
    "}\n",
    "for fn, aids in fn_to_aids.items():\n",
    "    with open(os.path.join(out_dir, fn), \"w\") as fout:\n",
    "        for aid in aids:\n",
    "            text = eid_to_text[aid]\n",
    "            fout.write(f\"{aid}\\t{text}\\t{SIM_RELATION}\\n\")\n",
    "            \n",
    "fn_to_arels = {\n",
    "    \"arels.train.sim.tsv\": [(aid, pid) for aid, simpids in train_aid_to_simpids.items() for pid in simpids],\n",
    "    \"arels.test.sim.tsv\": [(aid, pid) for aid, simpids in test_aid_to_simpids.items() for pid in simpids],\n",
    "}\n",
    "for fn, arels in fn_to_arels.items():\n",
    "    with open(os.path.join(out_dir, fn), \"w\") as fout:\n",
    "        for (aid, pid) in arels:\n",
    "            fout.write(f\"{aid}\\tQ0\\t{pid}\\t{1}\\n\")\n",
    "            \n",
    "# for complementary items\n",
    "fn_to_aids = {\n",
    "    \"anchors.train.compl.tsv\": list(train_aid_to_complpids.keys()),\n",
    "    \"anchors.test.compl.tsv\": list(test_aid_to_complpids.keys()),\n",
    "}\n",
    "for fn, aids in fn_to_aids.items():\n",
    "    with open(os.path.join(out_dir, fn), \"w\") as fout:\n",
    "        for aid in aids:\n",
    "            text = eid_to_text[aid]\n",
    "            fout.write(f\"{aid}\\t{text}\\t{COMPL_RELATION}\\n\")\n",
    "fn_to_arels = {\n",
    "    \"arels.train.compl.tsv\": [(aid, pid) for aid, complpids in train_aid_to_complpids.items() for pid in complpids],\n",
    "    \"arels.test.compl.tsv\": [(aid, pid) for aid, complpids in test_aid_to_complpids.items() for pid in complpids]\n",
    "}\n",
    "for fn, arels in fn_to_arels.items():\n",
    "    with open(os.path.join(out_dir, fn), \"w\") as fout:\n",
    "        for (aid, pid) in arels:\n",
    "            fout.write(f\"{aid}\\tQ0\\t{pid}\\t{1}\\n\")\n",
    "            \n",
    "# for queries\n",
    "fn_to_qids = {\n",
    "    \"queries.train.tsv\": list(train_qid_to_pids.keys()),\n",
    "    \"queries.test.tsv\": list(test_qid_to_pids.keys()),\n",
    "}\n",
    "for fn, qids in fn_to_qids.items():\n",
    "    with open(os.path.join(out_dir, fn), \"w\") as fout:\n",
    "        for qid in qids:\n",
    "            text = eid_to_text[qid]\n",
    "            fout.write(f\"{qid}\\t{text}\\t{REL_RELATION}\\n\")\n",
    "fn_to_qrels = {\n",
    "    \"qrels.train.tsv\": [(qid, pid) for qid, pids in train_qid_to_pids.items() for pid in pids],\n",
    "    \"qrels.test.tsv\": [(qid, pid) for (qid, pids) in test_qid_to_pids.items() for pid in pids],\n",
    "}\n",
    "for fn, qrels in fn_to_qrels.items():\n",
    "    with open(os.path.join(out_dir, fn), \"w\") as fout:\n",
    "        for (qid, pid) in qrels:\n",
    "            fout.write(f\"{qid}\\tQ0\\t{pid}\\t{1}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66254c47-1631-4699-973e-ca2f793f026a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "out_dir = os.path.join(kgc_dir, \"unified_train/\")\n",
    "for path in os.listdir(out_dir):\n",
    "    path = os.path.join(out_dir, path)\n",
    "    if \"train_graph.pkl\" in path:\n",
    "        continue\n",
    "    ! wc -l $path\n",
    "    ! head -n 3 $path\n",
    "    ! tail -n 3 $path\n",
    "    print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c9ddcb9-fe82-452f-bf1d-6c53b3f27b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "745636 Broan 30-in Ducted Black Undercabinet Range Hood ; Undercabinet Range Hoods\n",
      "1173319 Broan Wall Cap (Black) ; Range Hood Parts\n",
      "629502 Broan 30-in Convertible Stainless Steel Undercabinet Range Hood ; Undercabinet Range Hoods\n"
     ]
    }
   ],
   "source": [
    "hid, pos_tid, neg_tid = (745636,1173319,629502)\n",
    "print(hid, eid_to_text[hid])\n",
    "print(pos_tid, eid_to_text[pos_tid])\n",
    "print(neg_tid, eid_to_text[neg_tid])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "31517e15-b4be-4a48-817c-82d2768875ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_fn: /home/jupyter/unity_jointly_rec_and_search/datasets/unified_kgc/unified_test/arels.test.sim.tsv, size: 8,273\n",
      "train_fn: /home/jupyter/unity_jointly_rec_and_search/datasets/unified_kgc/unified_train/a2sp.train.tsv, size: 199,507\n",
      "===========================================================================\n",
      "test_fn: /home/jupyter/unity_jointly_rec_and_search/datasets/unified_kgc/unified_test/arels.test.compl.tsv, size: 6,720\n",
      "train_fn: /home/jupyter/unity_jointly_rec_and_search/datasets/unified_kgc/unified_train/a2cp.train.tsv, size: 80,042\n",
      "===========================================================================\n",
      "test_fn: /home/jupyter/unity_jointly_rec_and_search/datasets/unified_kgc/unified_test/qrels.test.tsv, size: 7,274\n",
      "train_fn: /home/jupyter/unity_jointly_rec_and_search/datasets/unified_kgc/unified_train/max2_qorient_q2p.train.tsv, size: 832,482\n",
      "===========================================================================\n",
      "SUCCESS: test and train qids not overlap.\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "import os\n",
    "\n",
    "#test_dir = os.path.join(user_dir, \"selected_test_user\")\n",
    "kgc_dir = \"/home/jupyter/unity_jointly_rec_and_search/datasets/unified_kgc/\"\n",
    "\n",
    "test_fns = [\n",
    "    os.path.join(kgc_dir, \"unified_test/arels.test.sim.tsv\"),\n",
    "    os.path.join(kgc_dir, \"unified_test/arels.test.compl.tsv\"),\n",
    "    os.path.join(kgc_dir, \"unified_test/qrels.test.tsv\")\n",
    "]\n",
    "train_fns = [\n",
    "    os.path.join(kgc_dir, \"unified_train/a2sp.train.tsv\"),\n",
    "    os.path.join(kgc_dir, \"unified_train/a2cp.train.tsv\"),\n",
    "    os.path.join(kgc_dir, \"unified_train/max2_qorient_q2p.train.tsv\")\n",
    "]\n",
    "\n",
    "for test_fn, train_fn in zip(test_fns, train_fns):\n",
    "    test_qids, train_qids = set(), set()\n",
    "    with open(test_fn) as fin:\n",
    "        for line in fin:\n",
    "            array = line.rstrip().split(\"\\t\")\n",
    "            assert len(array) == 4\n",
    "            test_qids.add(array[0])\n",
    "    with open(train_fn) as fin:\n",
    "        for line in fin:\n",
    "            array = line.rstrip().split(\"\\t\")\n",
    "            assert len(array) == 4\n",
    "            train_qids.add(array[0])\n",
    "    assert len(test_qids & train_qids) == 0\n",
    "    print(f\"test_fn: {test_fn}, size: {len(test_qids):,}\")\n",
    "    print(f\"train_fn: {train_fn}, size: {len(train_qids):,}\")\n",
    "    print(75*\"=\")\n",
    "print(\"SUCCESS: test and train qids not overlap.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c58cf1-6f96-470f-a4fd-8ee27a2e498b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-9.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
