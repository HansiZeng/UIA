{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of sim_rec train and test = 1,017,800, 10,000\n",
      "length of compl_rec train and test = 67,310, 10,000\n",
      "length of search train and test = 13,726,249, 10,000\n",
      "length of train_merge_data = 14,811,359\n",
      "number of entites = 3,214,651\n",
      "test users for each data are subset of their corresponding train users.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pickle \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "SIM_RELATION = \"is_similar_to\"\n",
    "COMPL_RELATION = \"is_complementary_to\"\n",
    "REL_RELATION = \"is_relevant_to\"\n",
    "\n",
    "\n",
    "in_dir = \"/home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/\"\n",
    "    \n",
    "train_sim_data, train_compl_data, train_search_data = None, None, None\n",
    "data_fns = [\n",
    "    os.path.join(in_dir, \"train_sim_recs.csv\"),\n",
    "    os.path.join(in_dir, \"train_compl_recs.csv\"),\n",
    "    os.path.join(in_dir, \"train_searchs.csv\"),\n",
    "]\n",
    "datas = []\n",
    "for fn in data_fns:\n",
    "    datas.append(pd.read_csv(fn, index_col=0))\n",
    "train_sim_data, train_compl_data, train_search_data = datas\n",
    "\n",
    "\n",
    "datas = []\n",
    "test_sim_data, test_compl_data, test_search_data = None, None, None\n",
    "selected_dir = os.path.join(in_dir, \"selected_test_user\")\n",
    "data_fns = [\n",
    "    os.path.join(selected_dir, \"selected_sim_data.test.pkl\"),\n",
    "    os.path.join(selected_dir, \"selected_compl_data.test.pkl\"),\n",
    "    os.path.join(selected_dir, \"selected_search_data.test.pkl\"),\n",
    "]\n",
    "for fn in data_fns:\n",
    "    with open(fn, \"rb\") as fin:\n",
    "        datas.append(pickle.load(fin))\n",
    "test_sim_data, test_compl_data, test_search_data = datas\n",
    "datas = None\n",
    "\n",
    "root_dir=\"/home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/\"\n",
    "eid_to_text = {}\n",
    "with open(os.path.join(root_dir, \"all_entities.tsv\")) as fin:\n",
    "    for line in fin:\n",
    "        eid, text = line.strip().split(\"\\t\")\n",
    "        eid_to_text[int(eid)] = text\n",
    "        \n",
    "train_sim_data[\"relation\"] = SIM_RELATION\n",
    "test_sim_data[\"relation\"] = SIM_RELATION\n",
    "train_compl_data[\"relation\"] = COMPL_RELATION\n",
    "test_compl_data[\"relation\"] = COMPL_RELATION\n",
    "train_search_data[\"relation\"] = REL_RELATION\n",
    "test_search_data[\"relation\"] = REL_RELATION\n",
    "\n",
    "train_sim_data.rename({\"aid\": \"hid\", \"sim_pids\": \"tids\"}, axis=1, inplace=True)\n",
    "test_sim_data.rename({\"aid\": \"hid\", \"sim_pids\": \"tids\"}, axis=1, inplace=True)\n",
    "train_compl_data.rename({\"aid\": \"hid\", \"compl_pids\": \"tids\"}, axis=1, inplace=True)\n",
    "test_compl_data.rename({\"aid\": \"hid\", \"compl_pids\": \"tids\"}, axis=1, inplace=True)\n",
    "train_search_data.rename({\"qid\": \"hid\", \"rel_pids\": \"tids\"}, axis=1, inplace=True)\n",
    "test_search_data.rename({\"qid\": \"hid\", \"rel_pids\": \"tids\"}, axis=1, inplace=True)\n",
    "\n",
    "train_merge_data = pd.concat([train_sim_data, train_compl_data, train_search_data])\n",
    "train_merge_data[\"date_time\"] = pd.to_datetime(train_merge_data[\"date_time\"])\n",
    "train_merge_data = train_merge_data.sort_values(by=[\"uid\", \"date_time\"])\n",
    "\n",
    "print(\"length of sim_rec train and test = {:,}, {:,}\".format(len(train_sim_data), len(test_sim_data)))\n",
    "print(\"length of compl_rec train and test = {:,}, {:,}\".format(len(train_compl_data), len(test_compl_data)))\n",
    "print(\"length of search train and test = {:,}, {:,}\".format(len(train_search_data), len(test_search_data)))\n",
    "print(\"length of train_merge_data = {:,}\".format(len(train_merge_data)))\n",
    "print(\"number of entites = {:,}\".format(len(eid_to_text)))\n",
    "\n",
    "assert set(test_sim_data.uid).issubset(set(train_sim_data.uid)) \\\n",
    "and set(test_compl_data.uid).issubset(set(train_compl_data.uid)) \\\n",
    "and set(test_search_data.uid).issubset(set(train_search_data.uid))\n",
    "assert len(train_merge_data) == len(train_sim_data) + len(train_compl_data) + len(train_search_data)\n",
    "print(\"test users for each data are subset of their corresponding train users.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "search_sequential:  61%|██████    | 495466/815832 [2:34:54<1:42:32, 52.07it/s]"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "import ujson \n",
    "\n",
    "out_dir = os.path.join(in_dir, \"mixture_sequential_train_test/\")\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir) \n",
    "\n",
    "seq_examples_list = []\n",
    "prefixes_to_datas= {\n",
    "    os.path.join(out_dir, \"search_sequential\"): (train_search_data, test_search_data, \"urels.search.test.tsv\"),\n",
    "    os.path.join(out_dir, \"sim_rec_sequential\"): (train_sim_data, test_sim_data, \"urels.sim.test.tsv\"),\n",
    "    os.path.join(out_dir, \"compl_rec_sequential\"): (train_compl_data, test_compl_data, \"urels.compl.test.tsv\"),\n",
    "}\n",
    "\n",
    "for prefix, (train_data, test_data, urel_path) in prefixes_to_datas.items():\n",
    "    train_seq_examples = []\n",
    "    test_seq_examples = []\n",
    "    test_uid_to_pospids = {}\n",
    "    for uid, g in tqdm(train_data.groupby(\"uid\"), desc=prefix.split(\"/\")[-1]):\n",
    "        last_time = g.iloc[-1].date_time\n",
    "        group = train_merge_data[train_merge_data.uid==uid]\n",
    "        group = group[group.date_time <= last_time]\n",
    "        \n",
    "        qids = list(group.hid)\n",
    "        group_rel_pids = list(group.tids)\n",
    "        relations = list(group.relation)\n",
    "        \n",
    "        rel_pids = []\n",
    "        for xs in group_rel_pids:\n",
    "            rel_pids.append(random.sample(eval(xs), k=1)[0]) # only sample 1 relpid \n",
    "        assert len(qids) == len(rel_pids) == len(group)\n",
    "\n",
    "        uid = int(uid)\n",
    "        qids = [int(x) for x in qids]\n",
    "        rel_pids = [int(x) for x in rel_pids]\n",
    "\n",
    "        query_ids = qids[1:]\n",
    "        context_key_ids = qids[:-1]\n",
    "        context_value_ids = rel_pids[:-1]\n",
    "        target_value_ids = rel_pids[1:]\n",
    "        relations = relations[1:]\n",
    "        assert len(query_ids) == len(context_key_ids) == len(context_value_ids) == len(target_value_ids) == len(relations)\n",
    "\n",
    "        example = {\"uid\": uid, \"query_ids\": query_ids, \"context_key_ids\": context_key_ids, \"context_value_ids\": context_value_ids,\n",
    "                    \"target_value_ids\": target_value_ids, \"relations\": relations}\n",
    "        train_seq_examples.append(example)\n",
    "\n",
    "        # for test\n",
    "        test_row = test_data[test_data.uid == uid]\n",
    "        if len(test_row) == 0:\n",
    "            continue\n",
    "        assert len(test_row) == 1, test_row\n",
    "        \n",
    "        test_qid = int(test_row.iloc[0].hid)\n",
    "        test_relation = str(test_row.iloc[0].relation)\n",
    "\n",
    "        test_query_ids = qids[1:] + [test_qid]\n",
    "        test_context_key_ids = qids \n",
    "        test_context_value_ids = rel_pids\n",
    "        relations = relations + [test_relation]\n",
    "        assert len(test_query_ids) == len(test_context_key_ids) == len(test_context_value_ids), (len(test_query_ids), \n",
    "                                                                                len(test_context_key_ids), len(test_context_value_ids))\n",
    "        assert len(test_query_ids) == len(relations), (len(test_query_ids), len(relations))\n",
    "\n",
    "        example = {\"uid\": uid, \"query_ids\": test_query_ids, \"context_key_ids\": test_context_key_ids, \n",
    "                   \"context_value_ids\": test_context_value_ids, \"relations\": relations}\n",
    "        test_seq_examples.append(example)\n",
    "\n",
    "        if \"search_sequential\" in prefix:\n",
    "            test_uid_to_pospids[uid] = test_row.iloc[0].tids\n",
    "        elif \"sim_rec_sequential\" in prefix:\n",
    "            test_uid_to_pospids[uid] = test_row.iloc[0].tids\n",
    "        elif \"compl_rec_sequential\" in prefix:\n",
    "            test_uid_to_pospids[uid] = test_row.iloc[0].tids\n",
    "        else:\n",
    "            raise ValueError(f\"{prefix} not valid.\")\n",
    "        \n",
    "\n",
    "    #with open(prefix + \".train.json\", \"w\") as fout:\n",
    "    #    for line in train_seq_examples:\n",
    "    #        fout.write(ujson.dumps(line) + \"\\n\")\n",
    "    with open(prefix + \".test.json\", \"w\") as fout:\n",
    "        for line in test_seq_examples:\n",
    "            fout.write(ujson.dumps(line) + \"\\n\")\n",
    "    with open(os.path.join(out_dir, urel_path), \"w\") as fout:\n",
    "        for uid, pos_pids in test_uid_to_pospids.items():\n",
    "            for pos_pid in pos_pids:\n",
    "                fout.write(f\"{uid}\\tQ0\\t{pos_pid}\\t{1}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max pids = 2260877\n",
      "number of ignore hids = 6644\n"
     ]
    }
   ],
   "source": [
    "in_dir = \"/home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/\"\n",
    "PIDS = []\n",
    "with open(os.path.join(in_dir, \"collection_title.tsv\")) as fin:\n",
    "    for line in fin:\n",
    "        PIDS.append(int(line.strip().split(\"\\t\")[0]))\n",
    "print(f\"max pids = {max(PIDS)}\")\n",
    "\n",
    "def create_neg_value_ids(query_ids, pos_value_ids, relations, miss_qids, sampler):\n",
    "    assert type(sampler) == dict\n",
    "    assert len(query_ids) == len(pos_value_ids) == len(relations), (len(query_ids), len(pos_value_ids), len(relations))\n",
    "    neg_value_ids = []\n",
    "    for qid, pos_vid, relation in zip(query_ids, pos_value_ids, relations):\n",
    "        if relation == SIM_RELATION:\n",
    "            neg_vid = random.sample(range(2_000_000), k=1)[0]\n",
    "            while neg_vid == pos_vid:\n",
    "                neg_vid = random.sample(range(2_000_000), k=1)[0]\n",
    "            neg_value_ids.append(neg_vid)\n",
    "        elif relation in [COMPL_RELATION, REL_RELATION]:\n",
    "            if qid not in sampler or len(sampler[qid]) <= 10:\n",
    "                miss_qids.add(qid)\n",
    "                neg_vid = random.sample(range(2_000_000), k=1)[0]\n",
    "                while neg_vid == pos_vid:\n",
    "                    neg_vid = random.sample(range(2_000_000), k=1)[0]\n",
    "                neg_value_ids.append(neg_vid)\n",
    "            else:\n",
    "                neg_vid = random.sample(sampler[qid], k=1)[0]\n",
    "                while neg_vid == pos_vid:\n",
    "                    neg_vid = random.sample(sampler[qid], k=1)[0]\n",
    "                neg_value_ids.append(neg_vid)\n",
    "        else:\n",
    "            raise ValueError(\"relation = {} is not valid\".format(relation))\n",
    "    \n",
    "    assert len(neg_value_ids) == len(pos_value_ids), (len(pos_value_ids), len(neg_value_ids))\n",
    "    \n",
    "    return neg_value_ids\n",
    "\n",
    "\n",
    "run_path = os.path.join(in_dir, \"runs/bm25.all.run\")\n",
    "df = pd.read_csv(run_path, sep=\" \", names=[\"hid\", \"q0\", \"tid\", \"rank\", \"score\", \"model_name\"])\n",
    "bm25_hid_to_tids = {}\n",
    "ignore_hids = set()\n",
    "for hid, group in df.groupby(\"hid\"):\n",
    "    cand_tids = list(group.tid.values)\n",
    "    if len(cand_tids) < 10:\n",
    "        ignore_hids.add(int(hid))\n",
    "    else:\n",
    "        bm25_hid_to_tids[int(hid)] = [int(x) for x in cand_tids]\n",
    "        \n",
    "print(\"number of ignore hids = {}\".format(len(ignore_hids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "search_sequential.test.json: 100%|██████████| 10000/10000 [00:00<00:00, 179453.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bm25 suffix: search_sequential.test.json's miss_hids = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sim_rec_sequential.test.json: 100%|██████████| 10000/10000 [00:00<00:00, 174951.47it/s]\n",
      "compl_rec_sequential.test.json: 100%|██████████| 10000/10000 [00:00<00:00, 176431.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bm25 suffix: compl_rec_sequential.test.json's miss_hids = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "search_sequential.test.json: 100%|██████████| 10000/10000 [00:00<00:00, 151453.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bm25 suffix: search_sequential.test.json's miss_hids = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sim_rec_sequential.test.json: 100%|██████████| 10000/10000 [00:00<00:00, 150425.67it/s]\n",
      "compl_rec_sequential.test.json: 100%|██████████| 10000/10000 [00:00<00:00, 164535.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bm25 suffix: compl_rec_sequential.test.json's miss_hids = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import ujson\n",
    "from tqdm import tqdm \n",
    "import random\n",
    "\n",
    "train_search_examples, test_search_examples, train_sim_rec_examples, test_sim_rec_examples, train_compl_rec_examples, \\\n",
    "test_compl_rec_examples = [],[],[],[],[],[]\n",
    "fn_to_example = {\n",
    "    os.path.join(out_dir, \"search_sequential.train.json\"): train_search_examples,\n",
    "    os.path.join(out_dir, \"search_sequential.test.json\"): test_search_examples,\n",
    "    os.path.join(out_dir, \"sim_rec_sequential.train.json\"): train_sim_rec_examples,\n",
    "    os.path.join(out_dir, \"sim_rec_sequential.test.json\"): test_sim_rec_examples,\n",
    "    os.path.join(out_dir, \"compl_rec_sequential.train.json\"): train_compl_rec_examples,\n",
    "    os.path.join(out_dir, \"compl_rec_sequential.test.json\"): test_compl_rec_examples,\n",
    "}\n",
    "\n",
    "for fn, data_examples in fn_to_example.items():\n",
    "    with open(fn) as fin:\n",
    "        for line in fin:\n",
    "            data_examples.append(ujson.loads(line))\n",
    "\n",
    "miss_hids = set()\n",
    "\n",
    "suffix_to_examples = {\n",
    "    #\"search_sequential.train.json\": (train_search_examples),\n",
    "    \"search_sequential.test.json\": (test_search_examples),\n",
    "    #\"sim_rec_sequential.train.json\": (train_sim_rec_examples),\n",
    "    \"sim_rec_sequential.test.json\": (test_sim_rec_examples),\n",
    "    #\"compl_rec_sequential.train.json\": (train_compl_rec_examples),\n",
    "    \"compl_rec_sequential.test.json\": (test_compl_rec_examples),\n",
    "}\n",
    "\n",
    "history_lengths = [4, 8]\n",
    "for hist_len in history_lengths:\n",
    "    for dest_signature in [\"hlen_{}_bm25\".format(hist_len)]:\n",
    "        dest_dir = os.path.join(out_dir, dest_signature)\n",
    "        if not os.path.exists(dest_dir):\n",
    "            os.mkdir(dest_dir)\n",
    "        for suffix, data_examples in suffix_to_examples.items():\n",
    "            dest_fn = os.path.join(dest_dir, suffix)\n",
    "            with open(dest_fn, \"w\") as fout:\n",
    "                for example in tqdm(data_examples, desc=suffix):\n",
    "                    if \"train.json\" in dest_fn:\n",
    "                        start_idx = max(0, len(example[\"query_ids\"])-hist_len)\n",
    "                    elif \"test.json\":\n",
    "                        start_idx = max(0, len(example[\"query_ids\"])-hist_len-1)\n",
    "                    else:\n",
    "                        raise ValueError(f\"{suffix} is not valid.\")\n",
    "\n",
    "                    query_ids = example[\"query_ids\"][start_idx:]\n",
    "                    context_key_ids = example[\"context_key_ids\"][start_idx:]\n",
    "                    context_value_ids = example[\"context_value_ids\"][start_idx:]\n",
    "                    relations = example[\"relations\"][start_idx:]\n",
    "                    assert len(relations) == len(query_ids), (len(relations), len(query_ids))\n",
    "                    if \"train.json\" in dest_fn:\n",
    "                        raise ValueError(\"not train.json\")\n",
    "                        target_value_ids = example[\"target_value_ids\"][start_idx:]\n",
    "                        assert \"bm25\" in dest_signature\n",
    "                        neg_value_ids = create_neg_value_ids(query_ids=query_ids, \n",
    "                                                             pos_value_ids=target_value_ids,\n",
    "                                                             relations=relations,\n",
    "                                                             miss_qids=miss_hids, \n",
    "                                                             sampler=bm25_hid_to_tids)\n",
    "                        dest_example = {\"uid\": example[\"uid\"], \"query_ids\": query_ids, \"context_key_ids\": context_key_ids,\n",
    "                                    \"context_value_ids\": context_value_ids, \n",
    "                                    \"target_value_ids\": target_value_ids, \"neg_value_ids\": neg_value_ids, \"relations\": relations}\n",
    "                    elif \"test.json\" in dest_fn:\n",
    "                        dest_example = {\"uid\": example[\"uid\"], \"query_ids\": query_ids, \"context_key_ids\": context_key_ids,\n",
    "                                    \"context_value_ids\": context_value_ids, \"relations\": relations}\n",
    "                    else:\n",
    "                        raise ValueError(f\"{suffix} is not valid.\")\n",
    "                    fout.write(ujson.dumps(dest_example) + \"\\n\")\n",
    "            if \"bm25\" in dest_signature:\n",
    "                if \"search_sequential\" in suffix or \"compl_rec_sequential\" in suffix:\n",
    "                    print(\"bm25 suffix: {}'s miss_hids = {}\".format(suffix, len(miss_hids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out_dir=\"/home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/sequential_train_test/\"\n",
    "check_dir = os.path.join(out_dir, \"hlen_4_bm25\")\n",
    "\n",
    "for fn in os.listdir(check_dir):\n",
    "    fn = os.path.join(check_dir, fn)\n",
    "    if \".test.json\" in fn:\n",
    "        continue\n",
    "    if \"compl_rec\" not in fn:\n",
    "        continue\n",
    "    ! wc -l $fn \n",
    "    ! head -n 2 $fn\n",
    "    print(100*\"=\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "for fn in os.listdir(out_dir):\n",
    "    if \"search\" not in fn or \"small\" in fn:\n",
    "        continue \n",
    "    fn = os.path.join(out_dir, fn)\n",
    "    \n",
    "    ! wc -l $fn \n",
    "    ! head -n 2 $fn\n",
    "    print(100*\"=\")\n",
    "\n",
    "print(100*\"=.\")\n",
    "\n",
    "check_dir = os.path.join(out_dir, \"without_context\")\n",
    "\n",
    "for fn in os.listdir(check_dir):\n",
    "    fn = os.path.join(check_dir, fn)\n",
    "    \n",
    "    ! wc -l $fn \n",
    "    ! head -n 2 $fn\n",
    "    print(100*\"=\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid = 148\n",
    "for example in test_search_examples:\n",
    "    if example[\"uid\"] == uid:\n",
    "        print(example)\n",
    "query_ids = [3051305,2692974,2773384,1301267]\n",
    "target_vids = [1582438,609312,257280,257280]\n",
    "neg_vids = [758275,398263,164388,144947]\n",
    "\n",
    "for qid, tvid, nvid in zip(query_ids, target_vids, neg_vids):\n",
    "    print(\"query: {}\\ntarget_item: {}\\nneg_item: {}\\n\".format(eid_to_text[qid], eid_to_text[tvid], eid_to_text[nvid]))\n",
    "    print(75*\"=\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir=\"/home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/\n",
    "with open(os.path.join(root_dir, \"all_entities.tsv\")) as fin:\n",
    "    for line in fin:\n",
    "        eid, text = line.strip().split(\"\\t\")\n",
    "        eid_to_text[int(eid)] = text\n",
    "\n",
    "query_ids = [2417516,2374200, 2576318, 2836633 ]\n",
    "context_value_ids = [202549,1582436,1400531,520284]\n",
    "target_value_ids = [1582436,1400531,520284,27714]\n",
    "\n",
    "for qid, context_pid, target_pid in zip(query_ids, context_value_ids, target_value_ids):\n",
    "    print(\"qid: {}\\n ctxpid : {} \\n relpid : {}\".format(eid_to_text[qid],eid_to_text[context_pid], eid_to_text[target_pid]))\n",
    "    print(\"=\"*75)\n",
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls /home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/sequential_train_test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls /home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/sequential_train_test/hlen_4_bm25/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"relation\" in example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-9.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9c9569641937ce4addf5496cbe81a89e9d276a9857e7a9967fd6589fdce30733"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
