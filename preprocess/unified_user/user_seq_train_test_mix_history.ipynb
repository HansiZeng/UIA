{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of sim_rec train and test = 1,017,800, 10,000\n",
      "length of compl_rec train and test = 67,310, 10,000\n",
      "length of search train and test = 13,726,249, 10,000\n",
      "length of train_merge_data = 14,811,359\n",
      "number of entites = 3,214,651\n",
      "test users for each data are subset of their corresponding train users.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pickle \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "SIM_RELATION = \"is_similar_to\"\n",
    "COMPL_RELATION = \"is_complementary_to\"\n",
    "REL_RELATION = \"is_relevant_to\"\n",
    "\n",
    "\n",
    "in_dir = \"/home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/\"\n",
    "    \n",
    "train_sim_data, train_compl_data, train_search_data = None, None, None\n",
    "data_fns = [\n",
    "    os.path.join(in_dir, \"train_sim_recs.csv\"),\n",
    "    os.path.join(in_dir, \"train_compl_recs.csv\"),\n",
    "    os.path.join(in_dir, \"train_searchs.csv\"),\n",
    "]\n",
    "datas = []\n",
    "for fn in data_fns:\n",
    "    datas.append(pd.read_csv(fn, index_col=0))\n",
    "train_sim_data, train_compl_data, train_search_data = datas\n",
    "\n",
    "\n",
    "datas = []\n",
    "test_sim_data, test_compl_data, test_search_data = None, None, None\n",
    "selected_dir = os.path.join(in_dir, \"selected_test_user\")\n",
    "data_fns = [\n",
    "    os.path.join(selected_dir, \"selected_sim_data.test.pkl\"),\n",
    "    os.path.join(selected_dir, \"selected_compl_data.test.pkl\"),\n",
    "    os.path.join(selected_dir, \"selected_search_data.test.pkl\"),\n",
    "]\n",
    "for fn in data_fns:\n",
    "    with open(fn, \"rb\") as fin:\n",
    "        datas.append(pickle.load(fin))\n",
    "test_sim_data, test_compl_data, test_search_data = datas\n",
    "datas = None\n",
    "\n",
    "root_dir=\"/home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/\"\n",
    "eid_to_text = {}\n",
    "with open(os.path.join(root_dir, \"all_entities.tsv\")) as fin:\n",
    "    for line in fin:\n",
    "        eid, text = line.strip().split(\"\\t\")\n",
    "        eid_to_text[int(eid)] = text\n",
    "        \n",
    "train_sim_data[\"relation\"] = SIM_RELATION\n",
    "test_sim_data[\"relation\"] = SIM_RELATION\n",
    "train_compl_data[\"relation\"] = COMPL_RELATION\n",
    "test_compl_data[\"relation\"] = COMPL_RELATION\n",
    "train_search_data[\"relation\"] = REL_RELATION\n",
    "test_search_data[\"relation\"] = REL_RELATION\n",
    "\n",
    "train_sim_data.rename({\"aid\": \"hid\", \"sim_pids\": \"tids\"}, axis=1, inplace=True)\n",
    "test_sim_data.rename({\"aid\": \"hid\", \"sim_pids\": \"tids\"}, axis=1, inplace=True)\n",
    "train_compl_data.rename({\"aid\": \"hid\", \"compl_pids\": \"tids\"}, axis=1, inplace=True)\n",
    "test_compl_data.rename({\"aid\": \"hid\", \"compl_pids\": \"tids\"}, axis=1, inplace=True)\n",
    "train_search_data.rename({\"qid\": \"hid\", \"rel_pids\": \"tids\"}, axis=1, inplace=True)\n",
    "test_search_data.rename({\"qid\": \"hid\", \"rel_pids\": \"tids\"}, axis=1, inplace=True)\n",
    "\n",
    "train_merge_data = pd.concat([train_sim_data, train_compl_data, train_search_data])\n",
    "train_merge_data[\"date_time\"] = pd.to_datetime(train_merge_data[\"date_time\"])\n",
    "train_merge_data = train_merge_data.sort_values(by=[\"uid\", \"date_time\"])\n",
    "\n",
    "print(\"length of sim_rec train and test = {:,}, {:,}\".format(len(train_sim_data), len(test_sim_data)))\n",
    "print(\"length of compl_rec train and test = {:,}, {:,}\".format(len(train_compl_data), len(test_compl_data)))\n",
    "print(\"length of search train and test = {:,}, {:,}\".format(len(train_search_data), len(test_search_data)))\n",
    "print(\"length of train_merge_data = {:,}\".format(len(train_merge_data)))\n",
    "print(\"number of entites = {:,}\".format(len(eid_to_text)))\n",
    "\n",
    "assert set(test_sim_data.uid).issubset(set(train_sim_data.uid)) \\\n",
    "and set(test_compl_data.uid).issubset(set(train_compl_data.uid)) \\\n",
    "and set(test_search_data.uid).issubset(set(train_search_data.uid))\n",
    "assert len(train_merge_data) == len(train_sim_data) + len(train_compl_data) + len(train_search_data)\n",
    "print(\"test users for each data are subset of their corresponding train users.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "search_sequential:   0%|          | 0/815832 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_42836/3403866523.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mcontext_value_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrel_pids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mtarget_value_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrel_pids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_key_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_value_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_value_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         example = {\"uid\": uid, \"query_ids\": query_ids, \"context_key_ids\": context_key_ids, \"context_value_ids\": context_value_ids,\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "import ujson \n",
    "\n",
    "out_dir = os.path.join(in_dir, \"mixture_sequential_train_test/\")\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir) \n",
    "\n",
    "seq_examples_list = []\n",
    "prefixes_to_datas= {\n",
    "    os.path.join(out_dir, \"search_sequential\"): (train_search_data, test_search_data, \"urels.search.test.tsv\"),\n",
    "    os.path.join(out_dir, \"sim_rec_sequential\"): (train_sim_data, test_sim_data, \"urels.sim.test.tsv\"),\n",
    "    os.path.join(out_dir, \"compl_rec_sequential\"): (train_compl_data, test_compl_data, \"urels.compl.test.tsv\"),\n",
    "}\n",
    "\n",
    "for prefix, (train_data, test_data, urel_path) in prefixes_to_datas.items():\n",
    "    train_seq_examples = []\n",
    "    test_seq_examples = []\n",
    "    test_uid_to_pospids = {}\n",
    "    for uid, g in tqdm(train_data.groupby(\"uid\"), desc=prefix.split(\"/\")[-1]):\n",
    "        last_time = g.iloc[-1].date_time\n",
    "        group = train_merge_data[train_merge_data.uid==uid]\n",
    "        group = group[group.date_time <= last_time]\n",
    "        \n",
    "        qids = list(group.hid)\n",
    "        group_rel_pids = list(group.tids)\n",
    "        relations = list(group.relation)\n",
    "        \n",
    "        rel_pids = []\n",
    "        for xs in group_rel_pids:\n",
    "            rel_pids.append(random.sample(eval(xs), k=1)[0]) # only sample 1 relpid \n",
    "        assert len(qids) == len(rel_pids) == len(group)\n",
    "\n",
    "        uid = int(uid)\n",
    "        qids = [int(x) for x in qids]\n",
    "        rel_pids = [int(x) for x in rel_pids]\n",
    "\n",
    "        query_ids = qids[1:]\n",
    "        context_key_ids = qids[:-1]\n",
    "        context_value_ids = rel_pids[:-1]\n",
    "        target_value_ids = rel_pids[1:]\n",
    "        relations = relations[1:]\n",
    "        assert len(query_ids) == len(context_key_ids) == len(context_value_ids) == len(target_value_ids) == len(relations)\n",
    "\n",
    "        example = {\"uid\": uid, \"query_ids\": query_ids, \"context_key_ids\": context_key_ids, \"context_value_ids\": context_value_ids,\n",
    "                    \"target_value_ids\": target_value_ids, \"relations\": relations}\n",
    "        train_seq_examples.append(example)\n",
    "\n",
    "        # for test\n",
    "        test_row = test_data[test_data.uid == uid]\n",
    "        if len(test_row) == 0:\n",
    "            continue\n",
    "        assert len(test_row) == 1, test_row\n",
    "        \n",
    "        test_qid = int(test_row.iloc[0].hid)\n",
    "        test_relation = str(test_row.iloc[0].relation)\n",
    "\n",
    "        test_query_ids = qids[1:] + [test_qid]\n",
    "        test_context_key_ids = qids \n",
    "        test_context_value_ids = rel_pids\n",
    "        relations = relations[1:] + [test_relation]\n",
    "        assert len(test_query_ids) == len(test_context_key_ids) == len(test_context_value_ids), (len(test_query_ids), \n",
    "                                                                                len(test_context_key_ids), len(test_context_value_ids))\n",
    "\n",
    "        example = {\"uid\": uid, \"query_ids\": test_query_ids, \"context_key_ids\": test_context_key_ids, \n",
    "                   \"context_value_ids\": test_context_value_ids, \"relations\": relations}\n",
    "        test_seq_examples.append(example)\n",
    "\n",
    "        if \"search_sequential\" in prefix:\n",
    "            test_uid_to_pospids[uid] = test_row.iloc[0].tids\n",
    "        elif \"sim_rec_sequential\" in prefix:\n",
    "            test_uid_to_pospids[uid] = test_row.iloc[0].tids\n",
    "        elif \"compl_rec_sequential\" in prefix:\n",
    "            test_uid_to_pospids[uid] = test_row.iloc[0].tids\n",
    "        else:\n",
    "            raise ValueError(f\"{prefix} not valid.\")\n",
    "        \n",
    "\n",
    "    with open(prefix + \".train.json\", \"w\") as fout:\n",
    "        for line in train_seq_examples:\n",
    "            fout.write(ujson.dumps(line) + \"\\n\")\n",
    "    with open(prefix + \".test.json\", \"w\") as fout:\n",
    "        for line in test_seq_examples:\n",
    "            fout.write(ujson.dumps(line) + \"\\n\")\n",
    "    with open(os.path.join(out_dir, urel_path), \"w\") as fout:\n",
    "        for uid, pos_pids in test_uid_to_pospids.items():\n",
    "            for pos_pid in pos_pids:\n",
    "                fout.write(f\"{uid}\\tQ0\\t{pos_pid}\\t{1}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max pids = 2260877\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nrun_path = os.path.join(in_dir, \"runs/bm25.all.run\")\\ndf = pd.read_csv(run_path, sep=\" \", names=[\"hid\", \"q0\", \"tid\", \"rank\", \"score\", \"model_name\"])\\nbm25_hid_to_tids = {}\\nignore_hids = set()\\nfor hid, group in df.groupby(\"hid\"):\\n    cand_tids = list(group.tid.values)\\n    if len(cand_tids) < 10:\\n        ignore_hids.add(int(hid))\\n    else:\\n        bm25_hid_to_tids[int(hid)] = [int(x) for x in cand_tids]\\n        \\nprint(\"number of ignore hids = {}\".format(len(ignore_hids)))\\n'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PIDS = []\n",
    "with open(os.path.join(in_dir, \"collection_title.tsv\")) as fin:\n",
    "    for line in fin:\n",
    "        PIDS.append(int(line.strip().split(\"\\t\")[0]))\n",
    "print(f\"max pids = {max(PIDS)}\")\n",
    "\n",
    "def create_neg_value_ids(query_ids, pos_value_ids, relations, miss_qids, sampler=None):\n",
    "    assert type(sampler) == dict\n",
    "    assert len(query_ids) == len(pos_value_ids) == len(relations), (len(query_ids), len(pos_value_ids), len(relations))\n",
    "    neg_value_ids = []\n",
    "    for qid, pos_vid, relation in zip(query_ids, pos_value_ids, relations):\n",
    "        if relation == SIM_RELATION:\n",
    "            neg_ivd = random.sample(range(2_000_000), k=1)[0]\n",
    "            while neg_vid == pos_vid:\n",
    "                neg_vid = random.sample(range(2_000_000), k=1)[0]\n",
    "        elif relation in [COMPL_RELATION, REL_RELATION]:\n",
    "            if qid not in sampler:\n",
    "                miss_qids.add(qid)\n",
    "                neg_vid = random.sample(range(2_000_000), k=1)[0]\n",
    "                while neg_vid == pos_vid:\n",
    "                    neg_vid = random.sample(range(2_000_000), k=1)[0]\n",
    "                neg_value_ids.append(neg_vid)\n",
    "            else:\n",
    "                neg_vid = random.sample(sampler[qid], k=1)[0]\n",
    "                while neg_vid == pos_vid:\n",
    "                    neg_vid = random.sample(range(2_000_000), k=1)[0]\n",
    "                neg_value_ids.append(neg_vid)\n",
    "        else:\n",
    "            raise ValueError(\"relation = {} is not valid\".format(relation))\n",
    "    \n",
    "    assert len(neg_value_ids) == len(pos_value_ids)\n",
    "    \n",
    "    return neg_value_ids\n",
    "\n",
    "\n",
    "run_path = os.path.join(in_dir, \"runs/bm25.all.run\")\n",
    "df = pd.read_csv(run_path, sep=\" \", names=[\"hid\", \"q0\", \"tid\", \"rank\", \"score\", \"model_name\"])\n",
    "bm25_hid_to_tids = {}\n",
    "ignore_hids = set()\n",
    "for hid, group in df.groupby(\"hid\"):\n",
    "    cand_tids = list(group.tid.values)\n",
    "    if len(cand_tids) < 10:\n",
    "        ignore_hids.add(int(hid))\n",
    "    else:\n",
    "        bm25_hid_to_tids[int(hid)] = [int(x) for x in cand_tids]\n",
    "        \n",
    "print(\"number of ignore hids = {}\".format(len(ignore_hids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "search_sequential.train.json:   0%|          | 0/815832 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "(4, 4, 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_42836/920884462.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m                                                              \u001b[0mrelations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrelations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                                                              \u001b[0mmiss_qids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmiss_hids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                              sampler=bm25_hid_to_tids)\n\u001b[0m\u001b[1;32m     62\u001b[0m                         dest_example = {\"uid\": example[\"uid\"], \"query_ids\": query_ids, \"context_key_ids\": context_key_ids,\n\u001b[1;32m     63\u001b[0m                                     \u001b[0;34m\"context_value_ids\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontext_value_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_42836/4098213548.py\u001b[0m in \u001b[0;36mcreate_neg_value_ids\u001b[0;34m(query_ids, pos_value_ids, relations, miss_qids, sampler)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_neg_value_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_value_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmiss_qids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_value_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_value_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mneg_value_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mqid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_vid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelation\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_value_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: (4, 4, 5)"
     ]
    }
   ],
   "source": [
    "import ujson\n",
    "from tqdm import tqdm \n",
    "import random\n",
    "\n",
    "train_search_examples, test_search_examples, train_sim_rec_examples, test_sim_rec_examples, train_compl_rec_examples, \\\n",
    "test_compl_rec_examples = [],[],[],[],[],[]\n",
    "fn_to_example = {\n",
    "    os.path.join(out_dir, \"search_sequential.train.json\"): train_search_examples,\n",
    "    os.path.join(out_dir, \"search_sequential.test.json\"): test_search_examples,\n",
    "    os.path.join(out_dir, \"sim_rec_sequential.train.json\"): train_sim_rec_examples,\n",
    "    os.path.join(out_dir, \"sim_rec_sequential.test.json\"): test_sim_rec_examples,\n",
    "    os.path.join(out_dir, \"compl_rec_sequential.train.json\"): train_compl_rec_examples,\n",
    "    os.path.join(out_dir, \"compl_rec_sequential.test.json\"): test_compl_rec_examples,\n",
    "}\n",
    "\n",
    "for fn, data_examples in fn_to_example.items():\n",
    "    with open(fn) as fin:\n",
    "        for line in fin:\n",
    "            data_examples.append(ujson.loads(line))\n",
    "\n",
    "miss_hids = set()\n",
    "\n",
    "suffix_to_examples = {\n",
    "    \"search_sequential.train.json\": (train_search_examples),\n",
    "    \"search_sequential.test.json\": (test_search_examples),\n",
    "    \"sim_rec_sequential.train.json\": (train_sim_rec_examples),\n",
    "    \"sim_rec_sequential.test.json\": (test_sim_rec_examples),\n",
    "    \"compl_rec_sequential.train.json\": (train_compl_rec_examples),\n",
    "    \"compl_rec_sequential.test.json\": (test_compl_rec_examples),\n",
    "}\n",
    "\n",
    "history_lengths = [4, 8]\n",
    "for hist_len in history_lengths:\n",
    "    for dest_signature in [\"hlen_{}_bm25\".format(hist_len)]:\n",
    "        dest_dir = os.path.join(out_dir, dest_signature)\n",
    "        if not os.path.exists(dest_dir):\n",
    "            os.mkdir(dest_dir)\n",
    "        for suffix, data_examples in suffix_to_examples.items():\n",
    "            dest_fn = os.path.join(dest_dir, suffix)\n",
    "            with open(dest_fn, \"w\") as fout:\n",
    "                for example in tqdm(data_examples, desc=suffix):\n",
    "                    if \"train.json\" in dest_fn:\n",
    "                        start_idx = max(0, len(example[\"query_ids\"])-hist_len)\n",
    "                    elif \"test.json\":\n",
    "                        start_idx = max(0, len(example[\"query_ids\"])-hist_len-1)\n",
    "                    else:\n",
    "                        raise ValueError(f\"{suffix} is not valid.\")\n",
    "\n",
    "                    query_ids = example[\"query_ids\"][start_idx:]\n",
    "                    context_key_ids = example[\"context_key_ids\"][start_idx:]\n",
    "                    context_value_ids = example[\"context_value_ids\"][start_idx:]\n",
    "                    relations = example[\"relations\"][start_idx:]\n",
    "                    print(len(example[\"query_ids\"]), len(example[\"relations\"]))\n",
    "                    if \"train.json\" in dest_fn:\n",
    "                        target_value_ids = example[\"target_value_ids\"][start_idx:]\n",
    "                        assert \"bm25\" in dest_signature\n",
    "                        neg_value_ids = create_neg_value_ids(query_ids=query_ids, \n",
    "                                                             pos_value_ids=target_value_ids,\n",
    "                                                             relations=relations,\n",
    "                                                             miss_qids=miss_hids, \n",
    "                                                             sampler=bm25_hid_to_tids)\n",
    "                        dest_example = {\"uid\": example[\"uid\"], \"query_ids\": query_ids, \"context_key_ids\": context_key_ids,\n",
    "                                    \"context_value_ids\": context_value_ids, \n",
    "                                    \"target_value_ids\": target_value_ids, \"neg_value_ids\": neg_value_ids, \"relations\": relations}\n",
    "                    elif \"test.json\" in dest_fn:\n",
    "                        dest_example = {\"uid\": example[\"uid\"], \"query_ids\": query_ids, \"context_key_ids\": context_key_ids,\n",
    "                                    \"context_value_ids\": context_value_ids, \"relations\": relations}\n",
    "                    else:\n",
    "                        raise ValueError(f\"{suffix} is not valid.\")\n",
    "                    fout.write(ujson.dumps(dest_example) + \"\\n\")\n",
    "            if \"bm25\" in dest_signature:\n",
    "                if \"search_sequential\" in suffix or \"compl_rec_sequential\" in suffix:\n",
    "                    print(\"bm25 suffix: {}'s miss_hids = {}\".format(suffix, len(miss_hids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "815832 /home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/sequential_train_test/hlen_4_bm25/search_sequential.train.json\n",
      "{\"uid\":0,\"query_ids\":[2938044,2441693,2382801,2676693],\"context_key_ids\":[2791482,2938044,2441693,2382801],\"context_value_ids\":[1558005,246217,438838,479119],\"target_value_ids\":[246217,438838,479119,264598],\"neg_value_ids\":[1554910,1363026,1865963,640542],\"relation\":\"is_relevant_to\"}\n",
      "{\"uid\":1,\"query_ids\":[3000257,2736901,3000257,2736901],\"context_key_ids\":[2461556,3000257,2736901,3000257],\"context_value_ids\":[618773,2065684,1698588,1846797],\"target_value_ids\":[2065684,1698588,1846797,1698588],\"neg_value_ids\":[1851723,2256340,1424601,1445720],\"relation\":\"is_relevant_to\"}\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor fn in os.listdir(out_dir):\\n    if \"search\" not in fn or \"small\" in fn:\\n        continue \\n    fn = os.path.join(out_dir, fn)\\n    \\n    ! wc -l $fn \\n    ! head -n 2 $fn\\n    print(100*\"=\")\\n\\nprint(100*\"=.\")\\n\\ncheck_dir = os.path.join(out_dir, \"without_context\")\\n\\nfor fn in os.listdir(check_dir):\\n    fn = os.path.join(check_dir, fn)\\n    \\n    ! wc -l $fn \\n    ! head -n 2 $fn\\n    print(100*\"=\")\\n'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#out_dir=\"/home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/sequential_train_test/\"\n",
    "check_dir = os.path.join(out_dir, \"hlen_4_bm25\")\n",
    "\n",
    "for fn in os.listdir(check_dir):\n",
    "    fn = os.path.join(check_dir, fn)\n",
    "    if \".test.json\" in fn:\n",
    "        continue\n",
    "    if \"search_sequential\" not in fn:\n",
    "        continue\n",
    "    ! wc -l $fn \n",
    "    ! head -n 2 $fn\n",
    "    print(100*\"=\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "for fn in os.listdir(out_dir):\n",
    "    if \"search\" not in fn or \"small\" in fn:\n",
    "        continue \n",
    "    fn = os.path.join(out_dir, fn)\n",
    "    \n",
    "    ! wc -l $fn \n",
    "    ! head -n 2 $fn\n",
    "    print(100*\"=\")\n",
    "\n",
    "print(100*\"=.\")\n",
    "\n",
    "check_dir = os.path.join(out_dir, \"without_context\")\n",
    "\n",
    "for fn in os.listdir(check_dir):\n",
    "    fn = os.path.join(check_dir, fn)\n",
    "    \n",
    "    ! wc -l $fn \n",
    "    ! head -n 2 $fn\n",
    "    print(100*\"=\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uid': 148, 'query_ids': [3120745, 2299257, 3110146, 3169024, 2582666, 3191904, 2875418, 2978514, 3156648], 'context_key_ids': [2504195, 3120745, 2299257, 3110146, 3169024, 2582666, 3191904, 2875418, 2978514], 'context_value_ids': [818764, 538461, 1146204, 1738420, 926207, 727621, 2167844, 2217383, 663385]}\n",
      "query: venom steel gloves\n",
      "target_item: Venom Steel Unisex 2-Layer Rip Resistant Nitrile Multipurpose Gloves, One Size Fits All (25-Pairs) ; Work Gloves\n",
      "neg_item: Buckle-Down Venom Face Icon Black/Reds/White Black Dog Collar, Medium (26- 40 Lbs.) ; Pet Collars & Harnesses\n",
      "\n",
      "===========================================================================\n",
      "query: edger attachment\n",
      "target_item: TrimmerPlus LE720 Edger Attachment ; String Trimmer Attachments\n",
      "neg_item: Badger Link-on Cultivator Attachment Tiller Attachment ; String Trimmer Attachments\n",
      "\n",
      "===========================================================================\n",
      "query: drill bit extensions\n",
      "target_item: Southwire 3/16-in Round 54-in Drill Bit Extension ; Drill Bit Extensions\n",
      "neg_item: Drill America 6-in Cobalt Twist Drill Bit ; Twist Drill Bits\n",
      "\n",
      "===========================================================================\n",
      "query: air compressors\n",
      "target_item: CRAFTSMAN 6-Gallon Single Stage Portable Corded Electric Pancake Air Compressor ; Air Compressors\n",
      "neg_item: Hitachi 4-Gallon Single Stage Portable Electric Twin Stack Air Compressor ; Air Compressors\n",
      "\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "uid = 148\n",
    "for example in test_search_examples:\n",
    "    if example[\"uid\"] == uid:\n",
    "        print(example)\n",
    "query_ids = [2938044,2441693,2382801,2676693]\n",
    "target_vids = [246217,438838,479119,264598]\n",
    "neg_vids = [1554910,1363026,1865963,640542]\n",
    "\n",
    "for qid, tvid, nvid in zip(query_ids, target_vids, neg_vids):\n",
    "    print(\"query: {}\\ntarget_item: {}\\nneg_item: {}\\n\".format(eid_to_text[qid], eid_to_text[tvid], eid_to_text[nvid]))\n",
    "    print(75*\"=\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (1838640968.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_7829/1838640968.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    root_dir=\"/home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/\u001b[0m\n\u001b[0m                                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "root_dir=\"/home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/\n",
    "with open(os.path.join(root_dir, \"all_entities.tsv\")) as fin:\n",
    "    for line in fin:\n",
    "        eid, text = line.strip().split(\"\\t\")\n",
    "        eid_to_text[int(eid)] = text\n",
    "\n",
    "query_ids = [2417516,2374200, 2576318, 2836633 ]\n",
    "context_value_ids = [202549,1582436,1400531,520284]\n",
    "target_value_ids = [1582436,1400531,520284,27714]\n",
    "\n",
    "for qid, context_pid, target_pid in zip(query_ids, context_value_ids, target_value_ids):\n",
    "    print(\"qid: {}\\n ctxpid : {} \\n relpid : {}\".format(eid_to_text[qid],eid_to_text[context_pid], eid_to_text[target_pid]))\n",
    "    print(\"=\"*75)\n",
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compl_rec_sequential.test.json\t search_sequential.train.json\n",
      "compl_rec_sequential.train.json  sim_rec_sequential.test.json\n",
      "hlen_4_bm25\t\t\t sim_rec_sequential.train.json\n",
      "hlen_4_randneg\t\t\t urels.compl.test.tsv\n",
      "hlen_8_bm25\t\t\t urels.search.test.tsv\n",
      "hlen_8_randneg\t\t\t urels.sim.test.tsv\n",
      "search_sequential.test.json\t without_context\n"
     ]
    }
   ],
   "source": [
    "! ls /home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/sequential_train_test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compl_rec_sequential.test.json\t search_sequential.train.json\n",
      "compl_rec_sequential.train.json  sim_rec_sequential.test.json\n",
      "search_sequential.test.json\t sim_rec_sequential.train.json\n"
     ]
    }
   ],
   "source": [
    "! ls /home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/sequential_train_test/hlen_4_bm25/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-9.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9c9569641937ce4addf5496cbe81a89e9d276a9857e7a9967fd6589fdce30733"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
