{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0829b584-b0fb-4061-99c1-f4f79b80775a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "bq_in_dir=\"/home/jupyter/data_transfer/data/\"\n",
    "with open(os.path.join(bq_in_dir, \"time_anchor_10core_sim_rec_bytime.pkl\"), \"rb\") as fin:\n",
    "    user_sim_rec_df = pickle.load(fin)\n",
    "\n",
    "with open(os.path.join(bq_in_dir, \"time_anchor_5core_compl_rec_bytime.pkl\"), \"rb\") as fin:\n",
    "    user_compl_rec_df = pickle.load(fin)\n",
    "\n",
    "user_search_subdfs = []\n",
    "for fn in glob.glob(os.path.join(bq_in_dir, \"time_query_10core_search_bytime_[0-9][0-9].pkl\")):\n",
    "    with open(fn, \"rb\") as fin:\n",
    "        user_search_subdfs.append(pickle.load(fin))\n",
    "        \n",
    "user_search_df = pd.concat(user_search_subdfs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681ab115-d22f-419c-9d74-8ad9ad883c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "in_dir = \"/home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/\"\n",
    "if not os.path.exists(in_dir):\n",
    "    os.mkdir(in_dir)\n",
    "\n",
    "print(\"size of user_sim_rec_df = {:,}, user_compl_rec_df = {:,}, user_search_df = {:,}\".format(\n",
    "    len(user_sim_rec_df), len(user_compl_rec_df), len(user_search_df),\n",
    "))\n",
    "\n",
    "all_users = set(\n",
    "    list(user_sim_rec_df.customer_id.unique()) + list(user_compl_rec_df.customer_id.unique()) + list(user_search_df.customer_id.unique()))\n",
    "interacted_ivms = set()\n",
    "interacted_queries = set()\n",
    "for i, row in tqdm(user_sim_rec_df.iterrows(), total=len(user_sim_rec_df)):\n",
    "    for sim_record in row.sim_records:\n",
    "        sim_ivms = [meta_sim_ivms[\"ivm\"] for meta_sim_ivms in sim_record[\"sim_ivms\"]]\n",
    "        interacted_ivms.update(sim_ivms)\n",
    "        interacted_ivms.add(sim_record[\"anchor\"])\n",
    "        \n",
    "for i, row in tqdm(user_compl_rec_df.iterrows(), total=len(user_compl_rec_df)):\n",
    "    for compl_record in row.compl_records:\n",
    "        compl_ivms = [meta_compl_ivm[\"ivm\"] for meta_compl_ivm in compl_record[\"compl_ivms\"]]\n",
    "        interacted_ivms.update(compl_ivms)\n",
    "        interacted_ivms.add(compl_record[\"anchor\"])\n",
    "        \n",
    "for i, row in tqdm(user_search_df.iterrows(), total=len(user_search_df)):\n",
    "    for search_record in row.search_records:\n",
    "        rel_ivms = [meta_rel_ivm[\"ivm\"] for meta_rel_ivm in search_record[\"rel_ivms\"]]\n",
    "        interacted_ivms.update(rel_ivms)\n",
    "        interacted_queries.add(search_record[\"query\"])\n",
    "        \n",
    "print(\"all_users = {:,}, interacted_ivms = {:,}, interacted_queries = {:,}\".format(len(all_users), len(interacted_ivms), len(interacted_queries)))\n",
    "\n",
    "# read exisiting ivm, query map\n",
    "with open(os.path.join(in_dir, \"ivm_to_pid.pkl\"), \"rb\") as fin:\n",
    "    ivm_to_pid = pickle.load(fin)\n",
    "\n",
    "with open(os.path.join(in_dir, \"query_to_qid.pkl\"), \"rb\") as fin:\n",
    "    query_to_qid = pickle.load(fin)\n",
    "\n",
    "print(\"interacted_ivms is subset of all: \", interacted_ivms.issubset(set(ivm_to_pid.keys())))\n",
    "print(\"interacted_queries is subset of all: \", interacted_queries.issubset(set(query_to_qid.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b13030-656b-4995-8b66-49aa8766f047",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_to_uid = {user:uid for uid, user in enumerate(list(all_users))}\n",
    "\n",
    "train_sim_recs = []\n",
    "test_sim_recs = []\n",
    "\n",
    "train_compl_recs = []\n",
    "test_compl_recs = []\n",
    "\n",
    "train_searchs = []\n",
    "test_searchs = []\n",
    "\n",
    "for i, row in tqdm(user_sim_rec_df.iterrows(), total=len(user_sim_rec_df)):\n",
    "    records = []\n",
    "    uid = user_to_uid[row.customer_id]\n",
    "    for sim_record in row.sim_records:\n",
    "        aid = ivm_to_pid[sim_record[\"anchor\"]]\n",
    "        sim_pids = [ivm_to_pid[meta_sim_ivms[\"ivm\"]] for meta_sim_ivms in sim_record[\"sim_ivms\"]]\n",
    "        visit_ids = [meta_sim_ivms[\"visit_id\"] for meta_sim_ivms in sim_record[\"sim_ivms\"]]\n",
    "        assert len(set(visit_ids)) == 1, visit_ids\n",
    "        \n",
    "        records.append([uid, aid, sim_pids, sim_record[\"date_time\"], visit_ids[0]])\n",
    "    train_sim_recs += records[:-1]\n",
    "    test_sim_recs.append(records[-1])\n",
    "    \n",
    "for i, row in tqdm(user_compl_rec_df.iterrows(), total=len(user_compl_rec_df)):\n",
    "    records = []\n",
    "    uid = user_to_uid[row.customer_id]\n",
    "    for compl_record in row.compl_records:\n",
    "        aid = ivm_to_pid[compl_record[\"anchor\"]]\n",
    "        compl_pids = [ivm_to_pid[meta_compl_ivm[\"ivm\"]] for meta_compl_ivm in compl_record[\"compl_ivms\"]]\n",
    "        visit_ids = [meta_compl_ivm[\"visit_id\"] for meta_compl_ivm in compl_record[\"compl_ivms\"]]\n",
    "        assert len(set(visit_ids)) == 1, visit_ids\n",
    "        \n",
    "        records.append([uid, aid, compl_pids, compl_record[\"date_time\"], visit_ids[0]])\n",
    "    train_compl_recs += records[:-1]\n",
    "    test_compl_recs.append(records[-1])\n",
    "    \n",
    "for i, row in tqdm(user_search_df.iterrows(), total=len(user_search_df)):\n",
    "    records = []\n",
    "    uid = user_to_uid[row.customer_id]\n",
    "    for search_record in row.search_records:\n",
    "        qid = query_to_qid[search_record[\"query\"]]\n",
    "        rel_pids = [ivm_to_pid[meta_rel_ivm[\"ivm\"]] for meta_rel_ivm in search_record[\"rel_ivms\"]]\n",
    "        visit_ids = [meta_rel_ivm[\"visit_id\"] for meta_rel_ivm in search_record[\"rel_ivms\"]]\n",
    "        #assert len(set(visit_ids)) == 1, visit_ids\n",
    "        \n",
    "        records.append([uid, qid, rel_pids, search_record[\"date_time\"], visit_ids[0]])\n",
    "    train_searchs += records[:-1]\n",
    "    test_searchs.append(records[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac00436-f865-4f03-bd20-dcc93567f549",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sim_data = pd.DataFrame(train_sim_recs, columns=[\"uid\", \"aid\", \"sim_pids\", \"date_time\", \"visit_id\"])\n",
    "test_sim_data = pd.DataFrame(test_sim_recs, columns=[\"uid\", \"aid\", \"sim_pids\", \"date_time\", \"visit_id\"])\n",
    "\n",
    "train_compl_data = pd.DataFrame(train_compl_recs, columns=[\"uid\", \"aid\", \"compl_pids\", \"date_time\", \"visit_id\"])\n",
    "test_compl_data = pd.DataFrame(test_compl_recs, columns=[\"uid\", \"aid\", \"compl_pids\", \"date_time\", \"visit_id\"])\n",
    "\n",
    "train_search_data = pd.DataFrame(train_searchs, columns=[\"uid\", \"qid\", \"rel_pids\", \"date_time\", \"visit_id\"])\n",
    "test_search_data = pd.DataFrame(test_searchs, columns=[\"uid\", \"qid\", \"rel_pids\", \"date_time\", \"visit_id\"])\n",
    "\n",
    "print(\"number of train_sim_data = {:,}, test_sim_data = {:,}\".format(len(train_sim_data), len(test_sim_data)))\n",
    "print(\"number of train_compl_data = {:,}, test_compl_data = {:,}\".format(len(train_compl_data), len(test_compl_data)))\n",
    "print(\"number of train_search_data = {:,}, test_search_Data = {:,}\".format(len(train_search_data), len(test_search_data)))\n",
    "print(\"number of users in sim_rec, compl_rec, search = {:,}, {:,}, {:,}\".format(len(train_sim_data.uid.unique()),\n",
    "                                                                               len(train_compl_data.uid.unique()),\n",
    "                                                                               len(train_search_data.uid.unique())))\n",
    "assert len(train_sim_data.uid.unique()) == len(test_sim_data.uid.unique())\n",
    "assert len(train_compl_data.uid.unique()) == len(test_compl_data.uid.unique())\n",
    "assert len(train_search_data.uid.unique()) == len(test_search_data.uid.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6766919-0f5f-4d48-a0b0-ee3917e85231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "out_dir = \"/home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/\"\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "    \n",
    "fn_to_data = {\n",
    "    os.path.join(out_dir, \"train_sim_recs.csv\"): train_sim_data,\n",
    "    os.path.join(out_dir, \"test_sim_recs.csv\"): test_sim_data,\n",
    "    os.path.join(out_dir, \"train_compl_recs.csv\"): train_compl_data,\n",
    "    os.path.join(out_dir, \"test_compl_recs.csv\"): test_compl_data,\n",
    "    os.path.join(out_dir, \"train_searchs.csv\"): train_search_data,\n",
    "    os.path.join(out_dir, \"test_searchs.csv\"): test_search_data,\n",
    "}\n",
    "for fn, pd_data in fn_to_data.items():\n",
    "    pd_data.to_csv(fn)\n",
    "\n",
    "with open(os.path.join(out_dir, \"user_to_uid.pkl\"), \"wb\") as fout:\n",
    "    pickle.dump(user_to_uid, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a53f5b6-fe0f-45bb-98b2-5a777e14463a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.463264106583072\n",
      "5.330218561925879\n",
      "16.82484751762618\n",
      "1.4388484967577126 2.0612093299658296 1.2722863325588805\n",
      "[1. 1. 2. 2.]\n",
      "[1. 2. 3. 4.]\n",
      "[1. 1. 1. 2.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# statistics\n",
    "sim_rlens, compl_rlens, search_rlens = [], [], []\n",
    "for i, group in train_sim_data.groupby(\"uid\"):\n",
    "    sim_rlens.append(len(group))\n",
    "for i, group in train_compl_data.groupby(\"uid\"):\n",
    "    compl_rlens.append(len(group))\n",
    "for i, group in train_search_data.groupby(\"uid\"):\n",
    "    search_rlens.append(len(group))  \n",
    "\n",
    "rlens = [sim_rlens, compl_rlens, search_rlens]\n",
    "for rlen in rlens:\n",
    "    print(sum(rlen)/len(rlen))\n",
    "    \n",
    "sim_pid_lens = np.array([len(x) for x in list(train_sim_data.sim_pids)])\n",
    "compl_pid_lens = np.array([len(x) for x in list(train_compl_data.compl_pids)])\n",
    "search_pid_lens = np.array([len(x) for x in list(train_search_data.rel_pids)])\n",
    "print(np.mean(sim_pid_lens), np.mean(compl_pid_lens), np.mean(search_pid_lens))\n",
    "for lens in [sim_pid_lens, compl_pid_lens, search_pid_lens]:\n",
    "    print(np.quantile(lens, [0.25, 0.5, 0.75, 0.9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50625952-79d4-4a39-b5c2-95a6428c77d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique excluded sim_aids, compl_aids and qids = 7,274, 6,720, 8,273\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "random.seed(4680)\n",
    "\n",
    "selected_dir = os.path.join(in_dir, \"selected_test_user\")\n",
    "if not os.path.exists(selected_dir):\n",
    "    os.mkdir(selected_dir)\n",
    "\n",
    "user_num = 10_000\n",
    "selected_sim_users = random.sample(list(test_sim_data.uid), k=user_num)\n",
    "selected_compl_users = random.sample(list(test_compl_data.uid), k=user_num)\n",
    "selected_search_users = random.sample(list(test_search_data.uid), k=user_num)\n",
    "\n",
    "selected_sim_data = test_sim_data[np.in1d(test_sim_data.uid, selected_sim_users)]\n",
    "selected_compl_data = test_compl_data[np.in1d(test_compl_data.uid, selected_compl_users)]\n",
    "selected_search_data = test_search_data[np.in1d(test_search_data.uid, selected_search_users)]\n",
    "\n",
    "rm_aid_to_simpids, rm_aid_to_complpids, rm_qid_to_relpids = defaultdict(set), defaultdict(set), defaultdict(set)\n",
    "\n",
    "for aid, sim_pids in zip(selected_sim_data.aid, selected_sim_data.sim_pids):\n",
    "    rm_aid_to_simpids[aid].update(sim_pids)\n",
    "    \n",
    "for aid, compl_pids in zip(selected_compl_data.aid, selected_compl_data.compl_pids):\n",
    "    rm_aid_to_complpids[aid].update(compl_pids)\n",
    "\n",
    "for qid, rel_pids in zip(selected_search_data.qid, selected_search_data.rel_pids):\n",
    "    rm_qid_to_relpids[qid].update(rel_pids)\n",
    "    \n",
    "print(\"unique excluded sim_aids, compl_aids and qids = {:,}, {:,}, {:,}\".format(\n",
    "    len(rm_qid_to_relpids), len(rm_aid_to_complpids), len(rm_aid_to_simpids)))\n",
    "    \n",
    "exclude_sim_aids = list(rm_aid_to_simpids.keys())\n",
    "exclude_compl_aids = list(rm_aid_to_complpids.keys())\n",
    "exclude_qids = list(rm_qid_to_relpids.keys())\n",
    "\n",
    "fn_to_data = {\n",
    "    \"selected_sim_data.test.pkl\": selected_sim_data,\n",
    "    \"selected_compl_data.test.pkl\": selected_compl_data,\n",
    "    \"selected_search_data.test.pkl\": selected_search_data,\n",
    "}\n",
    "for fn, data in fn_to_data.items():\n",
    "    fn = os.path.join(selected_dir, fn)\n",
    "    data.to_pickle(fn)\n",
    "    \n",
    "fn_to_data = {\n",
    "    \"aid_to_simpid.test.tsv\": rm_aid_to_simpids,\n",
    "    \"aid_to_complpid.test.tsv\": rm_aid_to_complpids,\n",
    "    \"qid_to_relpid.test.tsv\": rm_qid_to_relpids,\n",
    "}\n",
    "for fn, data in fn_to_data.items():\n",
    "    fn = os.path.join(selected_dir, fn)\n",
    "    with open(fn, \"w\") as fout:\n",
    "        for qid, pos_pids in data.items():\n",
    "            for pid in pos_pids:\n",
    "                fout.write(f\"{qid}\\t{pid}\\n\")\n",
    "                \n",
    "eid_to_text = {}\n",
    "with open(\"/home/jupyter/unity_jointly_rec_and_search/datasets/unified_user/all_entities.tsv\") as fin:\n",
    "    for line in fin:\n",
    "        eid, text = line.strip().split(\"\\t\")\n",
    "        eid_to_text[int(eid)] = text\n",
    "                \n",
    "fn_to_data = {\n",
    "    \"anchors.sim.test.tsv\": exclude_sim_aids,\n",
    "    \"anchors.compl.test.tsv\": exclude_compl_aids,\n",
    "    \"queries.search.test.tsv\": exclude_qids,\n",
    "}\n",
    "\n",
    "for fn, data in fn_to_data.items():\n",
    "    fn = os.path.join(selected_dir, fn)\n",
    "    with open(fn, \"w\") as fout:\n",
    "        for eid in data:\n",
    "            fout.write(f\"{eid}\\t{eid_to_text[eid]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454603d6-8821-4522-b1ad-1bf63fd97e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "for fn in os.listdir(selected_dir):\n",
    "    fn = os.path.join(selected_dir, fn)\n",
    "    if fn.endswith(\".pkl\"):\n",
    "        continue\n",
    "    ! wc -l $fn\n",
    "    ! head -n 2 $fn\n",
    "    print(75*\"=\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "597d0068-e046-4478-a4c2-ea1886e9b2e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Freedom Newport 3-ft H x 8-ft W White Vinyl Gothic Fence Panel ; Vinyl Fencing',\n",
       " 'Freedom 6-ft H x 3-in W White Vinyl Fence Gate Kit ; Vinyl Fencing')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eid_to_text[278428], eid_to_text[2174624]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9788f6aa-fee9-467a-87e7-df943ce1d6bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-9.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9c9569641937ce4addf5496cbe81a89e9d276a9857e7a9967fd6589fdce30733"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
