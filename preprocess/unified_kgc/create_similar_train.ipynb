{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d09d7e7-e60c-4c88-9e9e-b985c399716e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "client = bigquery.Client()\n",
    "print(\"Client creating using default project: {}\".format(client.project))\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT * \n",
    "    FROM `gcp-ushi-digital-ds-qa.new_hansi_dataset.comp_rec_ClicksData_2core`;\n",
    "    \"\"\"\n",
    "query_job = client.query(query)\n",
    "compl_rec_df = query_job.to_dataframe()\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT * \n",
    "    FROM `gcp-ushi-digital-ds-qa.new_hansi_dataset.hansi_rec_ClicksData_5core`;\n",
    "    \"\"\"\n",
    "query_job = client.query(query)\n",
    "sim_rec_df = query_job.to_dataframe()\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM `gcp-ushi-digital-ds-qa.new_hansi_dataset.search_ClicksData_1year_5core`;\n",
    "\"\"\"\n",
    "query_job = client.query(query)\n",
    "search_df = query_job.to_dataframe()\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT * \n",
    "    FROM `gcp-ushi-digital-ds-qa.hansi_dataset.all_products_info`;\n",
    "    \"\"\"\n",
    "query_job = client.query(query)\n",
    "product_df = query_job.to_dataframe()\n",
    "print(\"product_df = {:,}\".format(len(product_df)))\n",
    "\n",
    "all_products = set(product_df.product_id)\n",
    "anchors = set(compl_rec_df.anchor)\n",
    "compl_ivms = set(compl_rec_df.ivm)\n",
    "all_compl_ivms = anchors.union(compl_ivms)\n",
    "\n",
    "print(\"================================ For anchor_to_compl_ivms: ===================================\")\n",
    "print(\"number of unique product = {:,}, anchors = {:,}, complementary_compl_ivms = {:,}\".format(len(all_products), len(anchors), len(compl_ivms)))\n",
    "assert len(all_products & anchors) == len(anchors) and len(all_products & compl_ivms) == len(compl_ivms),(\n",
    "    len(all_products & anchors), len(anchors), len(all_products & compl_ivms), len(compl_ivms)\n",
    ")\n",
    "\n",
    "all_sim_ivms = set(sim_rec_df.anchor).union(set(sim_rec_df.ivm))\n",
    "print(\"================================ After updating anchor_to_similar_ivms: ===================================\")\n",
    "print(\"all_compl_ivms = {:,}, all_sim_ivms = {:,}\".format(len(all_compl_ivms), len(all_sim_ivms)))\n",
    "print(\"sim_compl_intersect = {:,} ({:.3f})\".format(len(all_compl_ivms & all_sim_ivms), len(all_compl_ivms & all_sim_ivms) / len(all_compl_ivms)))\n",
    "print(\"all_ivms = {:,}\".format(len(all_compl_ivms | all_sim_ivms)))\n",
    "all_ivms = all_compl_ivms | all_sim_ivms\n",
    "\n",
    "assert len(all_products & all_ivms) == len(all_ivms), (len(all_products & all_ivms), len(all_ivms))\n",
    "\n",
    "query_to_ivms = search_df.groupby(\"query\")[\"ivm\"].apply(list)\n",
    "ivm_to_queries = search_df.groupby(\"ivm\")[\"query\"].apply(list)\n",
    "query_lengths = np.array([len(x) for x in ivm_to_queries.values])\n",
    "all_queries = set(search_df[\"query\"])\n",
    "print(\"all queries = {:,}\".format(len(all_queries)))\n",
    "print(\"total ivms (queries) = {:,}, length >=3 = {:,}, length >= 5 = {:,}\".format(\n",
    "    len(query_lengths), np.sum(query_lengths >=3), np.sum(query_lengths >= 5) ))\n",
    "\n",
    "anchor_to_compl_ivms = compl_rec_df.groupby(\"anchor\")[\"ivm\"].apply(list)\n",
    "compl_ivms_length = np.array([len(x) for x in anchor_to_compl_ivms.values])\n",
    "print(\"================================ For anchor_to_compl_ivms: ===================================\")\n",
    "print(\"total_compl_ivms = {:,}, length >=3 = {:,}, length >= 5 = {:,}\".format(len(compl_ivms_length), np.sum(compl_ivms_length >=3), np.sum(compl_ivms_length >= 5) ))\n",
    "\n",
    "anchor_to_sim_ivms = sim_rec_df.groupby(\"anchor\")[\"ivm\"].apply(list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2adbe9dd-4ed3-4342-9f48-b4558df45531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "in_dir = \"/home/jupyter/jointly_rec_and_search/datasets/unified_kgc/\"\n",
    "\n",
    "with open(os.path.join(in_dir, \"ivm_to_pid.pkl\"), \"rb\") as fin:\n",
    "    ivm_to_pid = pickle.load(fin)\n",
    "with open(os.path.join(in_dir, \"query_to_qid.pkl\"), \"rb\") as fin:\n",
    "    query_to_qid = pickle.load(fin)\n",
    "\n",
    "\n",
    "pid_to_qids = {ivm_to_pid[ivm]: [query_to_qid[query] for query in queries] for ivm, queries in ivm_to_queries.items()}\n",
    "aid_to_complpids = {ivm_to_pid[anchor]: [ivm_to_pid[prod] for prod in products] for anchor, products in anchor_to_compl_ivms.items()}\n",
    "aid_to_simpids = {ivm_to_pid[anchor]: [ivm_to_pid[prod] for prod in products] for anchor, products in anchor_to_sim_ivms.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9936526e-0588-435d-8910-25e644bea00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start create graph\n",
    "import random \n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "\n",
    "random.seed(4680)\n",
    "\n",
    "val_test_aids = random.sample(aid_to_simpids.keys(), int(0.2*len(aid_to_simpids)))\n",
    "val_aids = val_test_aids[:int(0.5*len(val_test_aids))]\n",
    "test_aids = val_test_aids[int(0.5*len(val_test_aids)):]\n",
    "train_aid_to_simpids, val_aid_to_simpids, test_aid_to_simpids = {}, {}, {}\n",
    "for aid, simpids in tqdm(aid_to_simpids.items(), total=len(aid_to_simpids)):\n",
    "    if aid in val_aids:\n",
    "        val_aid_to_simpids[aid] = simpids\n",
    "    elif aid in test_aids:\n",
    "        test_aid_to_simpids[aid] = simpids\n",
    "    else:\n",
    "        train_aid_to_simpids[aid] = simpids\n",
    "\n",
    "print(\"number of aid_to_simpids  train = {:,}, val = {:,}, test = {:,}\".format(len(train_aid_to_simpids), \n",
    "                                                                              len(val_aid_to_simpids), len(test_aid_to_simpids)))\n",
    "assert len( set(train_aid_to_simpids.keys()) & set(val_aid_to_simpids.keys()) & set(test_aid_to_simpids.keys()) ) == 0\n",
    "\n",
    "G = nx.MultiDiGraph()\n",
    "SIM_RELATION = \"is_similar_to\"\n",
    "COMPL_RELATION = \"is_complementary_to\"\n",
    "REL_RELATION = \"is_relevant_to\"\n",
    "\n",
    "for aid, sim_pids in train_aid_to_simpids.items():\n",
    "    triples = [(aid, sim_pid, {\"type\":SIM_RELATION}) for sim_pid in sim_pids]\n",
    "    G.add_edges_from(triples)\n",
    "    \n",
    "for aid, compl_pids in aid_to_complpids.items():\n",
    "    triples = [(aid, compl_pid, {\"type\":COMPL_RELATION}) for compl_pid in compl_pids]\n",
    "    G.add_edges_from(triples)\n",
    "    \n",
    "for pid, qids in pid_to_qids.items():\n",
    "    triples = [(pid, qid, {\"type\": REL_RELATION}) for qid in qids]\n",
    "    G.add_edges_from(triples)\n",
    "\n",
    "multi_edge_pairs = []\n",
    "for n, nbrs_dict in G.adj.items():\n",
    "    for nbr_node, edge_attrs in nbrs_dict.items():\n",
    "        assert len(edge_attrs) == 1 or len(edge_attrs) == 2\n",
    "        if len(edge_attrs) == 2:\n",
    "            multi_edge_pairs.append((n, nbr_node))\n",
    "            \n",
    "print(\"number of edges = {:,}, number of multi-attr edges = {:,}, ({:.3f})\".format(G.number_of_edges(), len(multi_edge_pairs), \n",
    "                                                                                   len(multi_edge_pairs)/G.number_of_edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102d5727-1fa9-4478-867e-aea6e29a06a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_triples(hid, pos_tid, miss_hids, duplicate_pairs, eid_to_text, sampler=None):\n",
    "    if sampler != None:\n",
    "        assert type(sampler) == dict, type(sampler)\n",
    "        if hid not in sampler:\n",
    "            miss_hids.append(hid)\n",
    "            return 0\n",
    "    if eid_to_text[hid] == eid_to_text[pos_tid]:\n",
    "        duplicate_pairs.append((hid, pos_tid))\n",
    "        return 0\n",
    "    \n",
    "    if sampler != None:\n",
    "        neg_tid = random.sample(sampler[hid], k=1)[0]\n",
    "        while neg_tid == pos_tid:\n",
    "            neg_tid = random.sample(sampler[hid], k=1)[0]\n",
    "    else:\n",
    "        neg_tid = random.sample(range(2_000_000), k=1)[0]\n",
    "        while neg_tid == pos_tid:\n",
    "            neg_tid = random.sample(range(2_000_000), k=1)[0]\n",
    "            \n",
    "    return (hid, pos_tid, neg_tid)\n",
    "\n",
    "\n",
    "eid_to_text = {}\n",
    "with open(os.path.join(in_dir, \"all_entities.tsv\")) as fin:\n",
    "    for line in fin:\n",
    "        eid, text = line.strip().split(\"\\t\")\n",
    "        eid_to_text[int(eid)] = text\n",
    "        \n",
    "run_path = os.path.join(in_dir, \"runs/bm25.all.run\")\n",
    "df = pd.read_csv(run_path, sep=\" \", names=[\"hid\", \"q0\", \"tid\", \"rank\", \"score\", \"model_name\"])\n",
    "bm25_hid_to_tids = {}\n",
    "ignore_hids = set()\n",
    "for hid, group in df.groupby(\"hid\"):\n",
    "    cand_tids = list(group.tid.values)\n",
    "    if len(cand_tids) < 10:\n",
    "        ignore_hids.add(int(hid))\n",
    "    else:\n",
    "        bm25_hid_to_tids[int(hid)] = [int(x) for x in cand_tids]\n",
    "        \n",
    "print(\"number of ignore hids = {}\".format(len(ignore_hids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c13e205-0ea1-487a-ba9c-0c85d5ef2fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max5_h2sp = {}\n",
    "max5_h2cp = {}\n",
    "max5_h2q = {}\n",
    "\n",
    "for head_node, nbrs_dict in tqdm(G.adj.items(), total=G.number_of_nodes()):\n",
    "    sim_pids = []\n",
    "    compl_pids = []\n",
    "    rel_qids = []\n",
    "    for tail_node, edge_attrs in nbrs_dict.items():\n",
    "        assert len(edge_attrs) == 1 or len(edge_attrs) == 2\n",
    "        relations = []\n",
    "        for no, edge_attr in edge_attrs.items():\n",
    "            relations.append(edge_attr[\"type\"])\n",
    "        for rel in relations:\n",
    "            assert rel in [SIM_RELATION, COMPL_RELATION, REL_RELATION]\n",
    "            if rel in SIM_RELATION:\n",
    "                sim_pids.append(tail_node)\n",
    "            if rel in COMPL_RELATION:\n",
    "                compl_pids.append(tail_node)\n",
    "            if rel in REL_RELATION:\n",
    "                rel_qids.append(tail_node)\n",
    "    if len(sim_pids) != 0:\n",
    "        max5_h2sp[head_node] = random.sample(sim_pids, k=len(sim_pids))[:5]\n",
    "    if len(compl_pids) != 0:\n",
    "        max5_h2cp[head_node] = random.sample(compl_pids, k=len(compl_pids))[:5]\n",
    "    if len(rel_qids) != 0:\n",
    "        max5_h2q[head_node] = random.sample(rel_qids, k=len(rel_qids))[:5]\n",
    "        \n",
    "max5_s2sp = {}\n",
    "max5_s2cp = {}\n",
    "max5_s2q = {}\n",
    "for _, head_nodes in tqdm(max5_h2sp.items(), total=len(max5_h2sp)):\n",
    "    for head_node in head_nodes:\n",
    "        sim_pids = []\n",
    "        compl_pids = []\n",
    "        rel_qids = []\n",
    "        for tail_node, edge_attrs in G.adj[head_node].items():\n",
    "            assert len(edge_attrs) == 1 or len(edge_attrs) == 2\n",
    "            relations = []\n",
    "            for no, edge_attr in edge_attrs.items():\n",
    "                relations.append(edge_attr[\"type\"])\n",
    "            for rel in relations:\n",
    "                if rel in SIM_RELATION:\n",
    "                    sim_pids.append(tail_node)\n",
    "                if rel in COMPL_RELATION:\n",
    "                    compl_pids.append(tail_node)\n",
    "                if rel in REL_RELATION:\n",
    "                    rel_qids.append(tail_node)\n",
    "        if len(sim_pids) != 0:\n",
    "            max5_s2sp[head_node] = random.sample(sim_pids, k=len(sim_pids))[:5]\n",
    "        if len(compl_pids) != 0:\n",
    "            max5_s2cp[head_node] = random.sample(compl_pids, k=len(compl_pids))[:5]\n",
    "        if len(rel_qids) != 0:\n",
    "            max5_s2q[head_node] = random.sample(rel_qids, k=len(rel_qids))[:5]\n",
    "        \n",
    "miss_hids = []\n",
    "duplicate_pairs = []\n",
    "\n",
    "h2sp_triples = []\n",
    "h2cp_triples = []\n",
    "q2h_triples = []\n",
    "for hid, tail_ids in max5_h2sp.items():\n",
    "    for pos_tid in tail_ids:\n",
    "        triple = create_triples(hid, pos_tid, miss_hids, duplicate_pairs, eid_to_text)\n",
    "        if triple != 0:\n",
    "            h2sp_triples.append(triple)\n",
    "print(\"miss_hids = {:,}, duplicate_pairs = {:,}\".format(len(miss_hids), len(duplicate_pairs)))\n",
    "print(\"=\"*75)\n",
    "for hid, tail_ids in max5_h2cp.items():\n",
    "    for pos_tid in tail_ids:\n",
    "        triple = create_triples(hid, pos_tid, miss_hids, duplicate_pairs, eid_to_text, sampler=bm25_hid_to_tids)\n",
    "        if triple != 0:\n",
    "            h2cp_triples.append(triple)\n",
    "print(\"miss_hids = {:,}, duplicate_pairs = {:,}\".format(len(miss_hids), len(duplicate_pairs)))\n",
    "print(\"=\"*75)\n",
    "for pos_tid, head_ids in max5_h2q.items():\n",
    "    for hid in head_ids:\n",
    "        triple = create_triples(hid, pos_tid, miss_hids, duplicate_pairs, eid_to_text, sampler=bm25_hid_to_tids)\n",
    "        if triple != 0:\n",
    "            q2h_triples.append(triple)\n",
    "print(\"miss_hids = {:,}, duplicate_pairs = {:,}\".format(len(miss_hids), len(duplicate_pairs)))\n",
    "print(\"=\"*75)\n",
    "\n",
    "s2sp_triples = []\n",
    "s2cp_triples = []\n",
    "q2s_triples = []\n",
    "for hid, tail_ids in max5_s2sp.items():\n",
    "    for pos_tid in tail_ids:\n",
    "        triple = create_triples(hid, pos_tid, miss_hids, duplicate_pairs, eid_to_text)\n",
    "        if triple != 0:\n",
    "            s2sp_triples.append(triple)\n",
    "print(\"miss_hids = {:,}, duplicate_pairs = {:,}\".format(len(miss_hids), len(duplicate_pairs)))\n",
    "print(\"=\"*75)\n",
    "for hid, tail_ids in max5_s2cp.items():\n",
    "    for pos_tid in tail_ids:\n",
    "        triple = create_triples(hid, pos_tid, miss_hids, duplicate_pairs, eid_to_text, sampler=bm25_hid_to_tids)\n",
    "        if triple != 0:\n",
    "            s2cp_triples.append(triple)\n",
    "print(\"miss_hids = {:,}, duplicate_pairs = {:,}\".format(len(miss_hids), len(duplicate_pairs)))\n",
    "print(\"=\"*75)\n",
    "for pos_tid, head_ids in max5_s2q.items():\n",
    "    for hid in head_ids:\n",
    "        triple = create_triples(hid, pos_tid, miss_hids, duplicate_pairs, eid_to_text, sampler=bm25_hid_to_tids)\n",
    "        if triple != 0:\n",
    "            q2s_triples.append(triple)\n",
    "print(\"miss_hids = {:,}, duplicate_pairs = {:,}\".format(len(miss_hids), len(duplicate_pairs)))\n",
    "print(\"=\"*75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5f8079e0-bb52-49c1-a4d5-913b15ea3a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = os.path.join(in_dir, \"sim_train/\")\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "\n",
    "fn_to_tripleNrel = {\n",
    "    \"a2sp.train.tsv\": (h2sp_triples, SIM_RELATION),\n",
    "    \"a2cp.train.tsv\": (h2cp_triples, COMPL_RELATION),\n",
    "    \"q2a.train.tsv\": (q2h_triples, REL_RELATION),\n",
    "    \"s2sp.train.tsv\": (s2sp_triples, SIM_RELATION),\n",
    "    \"s2cp.train.tsv\": (s2cp_triples, COMPL_RELATION),\n",
    "    \"q2s.train.tsv\": (q2s_triples, REL_RELATION)\n",
    "}\n",
    "\n",
    "for fn, (triples, relation) in fn_to_tripleNrel.items():\n",
    "    with open(os.path.join(out_dir, fn), \"w\") as fout:\n",
    "        for (hid, pos_tid, neg_tid) in triples:\n",
    "            fout.write(f\"{hid}\\t{pos_tid}\\t{neg_tid}\\t{relation}\\n\")\n",
    "            \n",
    "out_dir = os.path.join(in_dir, \"sim_test/\")\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "\n",
    "fn_to_aids = {\n",
    "    \"anchors.train.tsv\": list(train_aid_to_simpids.keys()),\n",
    "    \"anchors.val.tsv\": list(val_aid_to_simpids.keys()),\n",
    "    \"anchors.test.tsv\": list(test_aid_to_simpids.keys())\n",
    "}\n",
    "for fn, aids in fn_to_aids.items():\n",
    "    with open(os.path.join(out_dir, fn), \"w\") as fout:\n",
    "        for aid in aids:\n",
    "            text = eid_to_text[aid]\n",
    "            fout.write(f\"{aid}\\t{text}\\t{SIM_RELATION}\\n\")\n",
    "            \n",
    "fn_to_arels = {\n",
    "    \"arels.train.tsv\": [(aid, pid) for aid, simpids in train_aid_to_simpids.items() for pid in simpids],\n",
    "    \"arels.val.tsv\": [(aid, pid) for aid, simpids in val_aid_to_simpids.items() for pid in simpids],\n",
    "    \"arels.test.tsv\": [(aid, pid) for aid, simpids in test_aid_to_simpids.items() for pid in simpids],\n",
    "}\n",
    "for fn, arels in fn_to_arels.items():\n",
    "    with open(os.path.join(out_dir, fn), \"w\") as fout:\n",
    "        for (aid, pid) in arels:\n",
    "            fout.write(f\"{aid}\\tQ0\\t{pid}\\t{1}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c93f481-a474-470b-bf7a-b910d4a47691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "out_dir = os.path.join(in_dir, \"sim_train/\")\n",
    "for path in os.listdir(out_dir):\n",
    "    path = os.path.join(out_dir, path)\n",
    "    ! wc -l $path\n",
    "    ! head -n 3 $path\n",
    "    ! tail -n 3 $path\n",
    "    print(\"=\"*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b378de5b-cadf-4b68-bba5-950036ddb570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3061845\tspeakman shower fixtures\n",
      "2233677\tSpeakman Chelsea Polished Chrome Dual Shower Head 2.5-GPM (9.5-LPM) ; Shower Heads\n",
      "2054671\tSpeakman Brushed Nickel Shower Hose ; Bathroom & Shower Faucet Accessories\n"
     ]
    }
   ],
   "source": [
    "hid, pos_tid, neg_tid = (3061845,2233677,2054671)\n",
    "\n",
    "! grep -P \"^{hid}\\t\" \"/home/jupyter/jointly_rec_and_search/datasets/unified_kgc/all_entities.tsv\"\n",
    "! grep -P \"^{pos_tid}\\t\" \"/home/jupyter/jointly_rec_and_search/datasets/unified_kgc/all_entities.tsv\"\n",
    "! grep -P \"^{neg_tid}\\t\" \"/home/jupyter/jointly_rec_and_search/datasets/unified_kgc/all_entities.tsv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "78e304c6-fe88-4e86-8209-6673fe2b5a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1048567, 331101, 1150236),\n",
       " (1048567, 1144496, 1481931),\n",
       " (1144496, 789117, 792800),\n",
       " (1144496, 1777815, 1648864),\n",
       " (1363017, 2098989, 1720379),\n",
       " (1363017, 524725, 605724),\n",
       " (1363017, 731516, 963135),\n",
       " (1363017, 745636, 564390),\n",
       " (1363017, 935334, 1226274),\n",
       " (731516, 2098989, 1257665)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dir = os.path.join(in_dir, \"sim_test/\")\n",
    "for path in os.listdir(out_dir):\n",
    "    path = os.path.join(out_dir, path)\n",
    "    ! wc -l $path\n",
    "    ! head -n 3 $path\n",
    "    ! tail -n 3 $path\n",
    "    print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a96fa8d1-593f-4094-8f1e-d7631f62f24e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360744, 172991)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max5_h2q), len(max5_h2sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13250e20-8709-43aa-9e7a-a66225ba1ddf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-9.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
