{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fa4706-7d67-4cc3-a825-b0e753e5caf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "client = bigquery.Client()\n",
    "print(\"Client creating using default project: {}\".format(client.project))\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT * \n",
    "    FROM `gcp-ushi-digital-ds-qa.new_hansi_dataset.comp_rec_ClicksData_2core`;\n",
    "    \"\"\"\n",
    "query_job = client.query(query)\n",
    "compl_rec_df = query_job.to_dataframe()\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT * \n",
    "    FROM `gcp-ushi-digital-ds-qa.new_hansi_dataset.hansi_rec_ClicksData_5core`;\n",
    "    \"\"\"\n",
    "query_job = client.query(query)\n",
    "sim_rec_df = query_job.to_dataframe()\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM `gcp-ushi-digital-ds-qa.new_hansi_dataset.search_ClicksData_1year_5core`;\n",
    "\"\"\"\n",
    "query_job = client.query(query)\n",
    "search_df = query_job.to_dataframe()\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT * \n",
    "    FROM `gcp-ushi-digital-ds-qa.hansi_dataset.all_products_info`;\n",
    "    \"\"\"\n",
    "query_job = client.query(query)\n",
    "product_df = query_job.to_dataframe()\n",
    "print(\"product_df = {:,}\".format(len(product_df)))\n",
    "\n",
    "all_products = set(product_df.product_id)\n",
    "anchors = set(compl_rec_df.anchor)\n",
    "compl_ivms = set(compl_rec_df.ivm)\n",
    "all_compl_ivms = anchors.union(compl_ivms)\n",
    "\n",
    "print(\"================================ For anchor_to_compl_ivms: ===================================\")\n",
    "print(\"number of unique product = {:,}, anchors = {:,}, complementary_compl_ivms = {:,}\".format(len(all_products), len(anchors), len(compl_ivms)))\n",
    "assert len(all_products & anchors) == len(anchors) and len(all_products & compl_ivms) == len(compl_ivms),(\n",
    "    len(all_products & anchors), len(anchors), len(all_products & compl_ivms), len(compl_ivms)\n",
    ")\n",
    "\n",
    "all_sim_ivms = set(sim_rec_df.anchor).union(set(sim_rec_df.ivm))\n",
    "print(\"================================ After updating anchor_to_similar_ivms: ===================================\")\n",
    "print(\"all_compl_ivms = {:,}, all_sim_ivms = {:,}\".format(len(all_compl_ivms), len(all_sim_ivms)))\n",
    "print(\"sim_compl_intersect = {:,} ({:.3f})\".format(len(all_compl_ivms & all_sim_ivms), len(all_compl_ivms & all_sim_ivms) / len(all_compl_ivms)))\n",
    "print(\"all_ivms = {:,}\".format(len(all_compl_ivms | all_sim_ivms)))\n",
    "all_ivms = all_compl_ivms | all_sim_ivms\n",
    "\n",
    "assert len(all_products & all_ivms) == len(all_ivms), (len(all_products & all_ivms), len(all_ivms))\n",
    "\n",
    "query_to_ivms = search_df.groupby(\"query\")[\"ivm\"].apply(list)\n",
    "ivm_to_queries = search_df.groupby(\"ivm\")[\"query\"].apply(list)\n",
    "query_lengths = np.array([len(x) for x in ivm_to_queries.values])\n",
    "all_queries = set(search_df[\"query\"])\n",
    "print(\"all queries = {:,}\".format(len(all_queries)))\n",
    "assert len(all_queries) == len(query_to_ivms), len(query_to_ivms)\n",
    "print(\"total ivms (queries) = {:,}, length >=3 = {:,}, length >= 5 = {:,}\".format(\n",
    "    len(query_lengths), np.sum(query_lengths >=3), np.sum(query_lengths >= 5) ))\n",
    "\n",
    "anchor_to_compl_ivms = compl_rec_df.groupby(\"anchor\")[\"ivm\"].apply(list)\n",
    "compl_ivms_length = np.array([len(x) for x in anchor_to_compl_ivms.values])\n",
    "print(\"================================ For anchor_to_compl_ivms: ===================================\")\n",
    "print(\"total_compl_ivms = {:,}, length >=3 = {:,}, length >= 5 = {:,}\".format(len(compl_ivms_length), np.sum(compl_ivms_length >=3), np.sum(compl_ivms_length >= 5) ))\n",
    "\n",
    "anchor_to_sim_ivms = sim_rec_df.groupby(\"anchor\")[\"ivm\"].apply(list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2adbe9dd-4ed3-4342-9f48-b4558df45531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "in_dir = \"/home/jupyter/jointly_rec_and_search/datasets/unified_kgc/\"\n",
    "\n",
    "with open(os.path.join(in_dir, \"ivm_to_pid.pkl\"), \"rb\") as fin:\n",
    "    ivm_to_pid = pickle.load(fin)\n",
    "with open(os.path.join(in_dir, \"query_to_qid.pkl\"), \"rb\") as fin:\n",
    "    query_to_qid = pickle.load(fin)\n",
    "\n",
    "\n",
    "pid_to_qids = {ivm_to_pid[ivm]: [query_to_qid[query] for query in queries] for ivm, queries in ivm_to_queries.items()}\n",
    "qid_to_pids = {query_to_qid[query]: [ivm_to_pid[ivm] for ivm in ivms] for query, ivms in query_to_ivms.items()}\n",
    "aid_to_complpids = {ivm_to_pid[anchor]: [ivm_to_pid[prod] for prod in products] for anchor, products in anchor_to_compl_ivms.items()}\n",
    "aid_to_simpids = {ivm_to_pid[anchor]: [ivm_to_pid[prod] for prod in products] for anchor, products in anchor_to_sim_ivms.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd471b50-3442-4247-bc2d-52a95eb4832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start create graph\n",
    "\n",
    "import random \n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "random.seed(4680)\n",
    "\n",
    "# for similar items\n",
    "val_test_aids = random.sample(aid_to_simpids.keys(), int(0.2*len(aid_to_simpids)))\n",
    "val_aids = val_test_aids[:int(0.5*len(val_test_aids))]\n",
    "test_aids = val_test_aids[int(0.5*len(val_test_aids)):]\n",
    "train_aid_to_simpids, val_aid_to_simpids, test_aid_to_simpids = {}, {}, {}\n",
    "for aid, simpids in tqdm(aid_to_simpids.items(), total=len(aid_to_simpids)):\n",
    "    if aid in val_aids:\n",
    "        val_aid_to_simpids[aid] = simpids\n",
    "    elif aid in test_aids:\n",
    "        test_aid_to_simpids[aid] = simpids\n",
    "    else:\n",
    "        train_aid_to_simpids[aid] = simpids\n",
    "        \n",
    "# for complementary items\n",
    "val_test_aids = random.sample(aid_to_complpids.keys(), int(0.2*len(aid_to_complpids)))\n",
    "val_aids = val_test_aids[:int(0.5*len(val_test_aids))]\n",
    "test_aids = val_test_aids[int(0.5*len(val_test_aids)):]\n",
    "train_aid_to_complpids, val_aid_to_complpids, test_aid_to_complpids = {}, {}, {}\n",
    "for aid, complpids in tqdm(aid_to_complpids.items(), total=len(aid_to_complpids)):\n",
    "    if aid in val_aids:\n",
    "        val_aid_to_complpids[aid] = complpids\n",
    "    elif aid in test_aids:\n",
    "        test_aid_to_complpids[aid] = complpids\n",
    "    else:\n",
    "        train_aid_to_complpids[aid] = complpids\n",
    "\n",
    "# for queries\n",
    "qid_pids_pairs = list(qid_to_pids.items())\n",
    "random.shuffle(qid_pids_pairs)\n",
    "train_qid_to_pids = {qid: pids for qid, pids in qid_pids_pairs[:int(0.8*len(qid_pids_pairs))]}\n",
    "val_qid_to_pids = {qid: pids for qid, pids in qid_pids_pairs[int(0.8*len(qid_pids_pairs)): int(0.9*len(qid_pids_pairs))]}\n",
    "test_qid_to_pids = {qid: pids for qid, pids in qid_pids_pairs[int(0.9*len(qid_pids_pairs)):]}\n",
    "\n",
    "\n",
    "print(\"number of aid_to_simpids  train = {:,}, val = {:,}, test = {:,}\".format(len(train_aid_to_simpids), \n",
    "                                                                              len(val_aid_to_simpids), len(test_aid_to_simpids)))\n",
    "print(\"number of aid_to_complpids train = {:,}, val = {:,}, test = {:,}\".format(len(train_aid_to_complpids), \n",
    "                                                                              len(val_aid_to_complpids), len(test_aid_to_complpids)))\n",
    "print(\"number of qid_to_pids train = {:,}, val = {:,}, test = {:,}\".format(len(train_qid_to_pids), \n",
    "                                                                              len(val_qid_to_pids), len(test_qid_to_pids)))\n",
    "assert len( set(train_aid_to_simpids.keys()) & set(val_aid_to_simpids.keys()) & set(test_aid_to_simpids.keys()) ) == 0\n",
    "assert len( set(train_aid_to_complpids.keys()) & set(val_aid_to_complpids.keys()) & set(test_aid_to_complpids.keys())) == 0\n",
    "assert len( set(train_qid_to_pids.keys()) & set(val_qid_to_pids.keys()) & set(test_qid_to_pids.keys())) == 0\n",
    "\n",
    "G = nx.MultiDiGraph()\n",
    "SIM_RELATION = \"is_similar_to\"\n",
    "COMPL_RELATION = \"is_complementary_to\"\n",
    "REL_RELATION = \"is_relevant_to\"\n",
    "\n",
    "for aid, sim_pids in train_aid_to_simpids.items():\n",
    "    triples = [(aid, sim_pid, {\"type\":SIM_RELATION}) for sim_pid in sim_pids]\n",
    "    G.add_edges_from(triples)\n",
    "    \n",
    "for aid, compl_pids in train_aid_to_complpids.items():\n",
    "    triples = [(aid, compl_pid, {\"type\":COMPL_RELATION}) for compl_pid in compl_pids]\n",
    "    G.add_edges_from(triples)\n",
    "    \n",
    "for qid, pids in train_qid_to_pids.items():\n",
    "    triples = [(pid, qid, {\"type\": REL_RELATION}) for pid in pids]\n",
    "    G.add_edges_from(triples)\n",
    "    \n",
    "multi_edge_pairs = []\n",
    "for n, nbrs_dict in tqdm(G.adj.items(), total=G.number_of_nodes()):\n",
    "    for nbr_node, edge_attrs in nbrs_dict.items():\n",
    "        assert len(edge_attrs) == 1 or len(edge_attrs) == 2\n",
    "        if len(edge_attrs) == 2:\n",
    "            multi_edge_pairs.append((n, nbr_node))\n",
    "            \n",
    "print(\"number of edges = {:,}, number of multi-attr edges = {:,}, ({:.3f})\".format(G.number_of_edges(), len(multi_edge_pairs), \n",
    "                                                                                   len(multi_edge_pairs)/G.number_of_edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d399c635-627a-42b0-9073-5b2e6f0b6909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ignore hids = 6644\n"
     ]
    }
   ],
   "source": [
    "def create_triples(hid, pos_tid, miss_hids, duplicate_pairs, eid_to_text, sampler=None):\n",
    "    if sampler != None:\n",
    "        assert type(sampler) == dict, type(sampler)\n",
    "        if hid not in sampler:\n",
    "            miss_hids.append(hid)\n",
    "            return 0\n",
    "    if eid_to_text[hid] == eid_to_text[pos_tid]:\n",
    "        duplicate_pairs.append((hid, pos_tid))\n",
    "        return 0\n",
    "    \n",
    "    if sampler != None:\n",
    "        neg_tid = random.sample(sampler[hid], k=1)[0]\n",
    "        while neg_tid == pos_tid:\n",
    "            neg_tid = random.sample(sampler[hid], k=1)[0]\n",
    "    else:\n",
    "        neg_tid = random.sample(range(2_000_000), k=1)[0]\n",
    "        while neg_tid == pos_tid:\n",
    "            neg_tid = random.sample(range(2_000_000), k=1)[0]\n",
    "            \n",
    "    return (hid, pos_tid, neg_tid)\n",
    "\n",
    "\n",
    "eid_to_text = {}\n",
    "with open(os.path.join(in_dir, \"all_entities.tsv\")) as fin:\n",
    "    for line in fin:\n",
    "        eid, text = line.strip().split(\"\\t\")\n",
    "        eid_to_text[int(eid)] = text\n",
    "        \n",
    "run_path = os.path.join(in_dir, \"runs/bm25.all.run\")\n",
    "df = pd.read_csv(run_path, sep=\" \", names=[\"hid\", \"q0\", \"tid\", \"rank\", \"score\", \"model_name\"])\n",
    "bm25_hid_to_tids = {}\n",
    "ignore_hids = set()\n",
    "for hid, group in df.groupby(\"hid\"):\n",
    "    cand_tids = list(group.tid.values)\n",
    "    if len(cand_tids) < 10:\n",
    "        ignore_hids.add(int(hid))\n",
    "    else:\n",
    "        bm25_hid_to_tids[int(hid)] = [int(x) for x in cand_tids]\n",
    "        \n",
    "print(\"number of ignore hids = {}\".format(len(ignore_hids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8935aca0-43d8-4649-9965-e66a049d4e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1160206/1160206 [00:21<00:00, 54410.64it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miss_hids = 0, duplicate_pairs = 26,772\n",
      "===========================================================================\n",
      "miss_hids = 0, duplicate_pairs = 27,103\n",
      "===========================================================================\n",
      "miss_hids = 34,511, duplicate_pairs = 27,103\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "max5_h2sp = {}\n",
    "max5_h2cp = {}\n",
    "max5_h2q = {}\n",
    "\n",
    "for head_node, nbrs_dict in tqdm(G.adj.items(), total=G.number_of_nodes()):\n",
    "    sim_pids = []\n",
    "    compl_pids = []\n",
    "    rel_qids = []\n",
    "    for tail_node, edge_attrs in nbrs_dict.items():\n",
    "        assert len(edge_attrs) == 1 or len(edge_attrs) == 2\n",
    "        relations = []\n",
    "        for no, edge_attr in edge_attrs.items():\n",
    "            relations.append(edge_attr[\"type\"])\n",
    "        for rel in relations:\n",
    "            assert rel in [SIM_RELATION, COMPL_RELATION, REL_RELATION]\n",
    "            if rel in SIM_RELATION:\n",
    "                sim_pids.append(tail_node)\n",
    "            if rel in COMPL_RELATION:\n",
    "                compl_pids.append(tail_node)\n",
    "            if rel in REL_RELATION:\n",
    "                rel_qids.append(tail_node)\n",
    "    if len(sim_pids) != 0:\n",
    "        max5_h2sp[head_node] = random.sample(sim_pids, k=len(sim_pids))[:5]\n",
    "    if len(compl_pids) != 0:\n",
    "        max5_h2cp[head_node] = random.sample(compl_pids, k=len(compl_pids))[:5]\n",
    "    if len(rel_qids) != 0:\n",
    "        max5_h2q[head_node] = random.sample(rel_qids, k=len(rel_qids))[:5]\n",
    "        \n",
    "miss_hids = []\n",
    "duplicate_pairs = []\n",
    "\n",
    "h2sp_triples = []\n",
    "h2cp_triples = []\n",
    "q2h_triples = []\n",
    "for hid, tail_ids in max5_h2sp.items():\n",
    "    for pos_tid in tail_ids:\n",
    "        triple = create_triples(hid, pos_tid, miss_hids, duplicate_pairs, eid_to_text)\n",
    "        if triple != 0:\n",
    "            h2sp_triples.append(triple)\n",
    "print(\"miss_hids = {:,}, duplicate_pairs = {:,}\".format(len(miss_hids), len(duplicate_pairs)))\n",
    "print(\"=\"*75)\n",
    "for hid, tail_ids in max5_h2cp.items():\n",
    "    for pos_tid in tail_ids:\n",
    "        triple = create_triples(hid, pos_tid, miss_hids, duplicate_pairs, eid_to_text, sampler=bm25_hid_to_tids)\n",
    "        if triple != 0:\n",
    "            h2cp_triples.append(triple)\n",
    "print(\"miss_hids = {:,}, duplicate_pairs = {:,}\".format(len(miss_hids), len(duplicate_pairs)))\n",
    "print(\"=\"*75)\n",
    "for pos_tid, head_ids in max5_h2q.items():\n",
    "    for hid in head_ids:\n",
    "        triple = create_triples(hid, pos_tid, miss_hids, duplicate_pairs, eid_to_text, sampler=bm25_hid_to_tids)\n",
    "        if triple != 0:\n",
    "            q2h_triples.append(triple)\n",
    "print(\"miss_hids = {:,}, duplicate_pairs = {:,}\".format(len(miss_hids), len(duplicate_pairs)))\n",
    "print(\"=\"*75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1dddaa7-9c07-48eb-872b-f294986ca783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miss_hids = 187,065, duplicate_pairs = 27,103\n",
      "===========================================================================\n",
      "miss_hids = 249,763, duplicate_pairs = 27,103\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "max5_q2p = {}\n",
    "max2_q2p = {}\n",
    "max5_q2p_triples = []\n",
    "max2_q2p_triples = []\n",
    "for qid, pids in train_qid_to_pids.items():\n",
    "    max5_q2p[qid] = random.sample(pids, k=len(pids))[:5]\n",
    "    max2_q2p[qid] = random.sample(pids, k=len(pids))[:2]\n",
    "    \n",
    "for qid, pos_pids in max5_q2p.items():\n",
    "    for pos_pid in pos_pids:\n",
    "        triple = create_triples(qid, pos_pid, miss_hids, duplicate_pairs, eid_to_text, sampler=bm25_hid_to_tids)\n",
    "        if triple != 0:\n",
    "            max5_q2p_triples.append(triple)\n",
    "print(\"miss_hids = {:,}, duplicate_pairs = {:,}\".format(len(miss_hids), len(duplicate_pairs)))\n",
    "print(\"=\"*75)\n",
    "\n",
    "for qid, pos_pids in max2_q2p.items():\n",
    "    for pos_pid in pos_pids:\n",
    "        triple = create_triples(qid, pos_pid, miss_hids, duplicate_pairs, eid_to_text, sampler=bm25_hid_to_tids)\n",
    "        if triple != 0:\n",
    "            max2_q2p_triples.append(triple)\n",
    "print(\"miss_hids = {:,}, duplicate_pairs = {:,}\".format(len(miss_hids), len(duplicate_pairs)))\n",
    "print(\"=\"*75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f8079e0-bb52-49c1-a4d5-913b15ea3a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "out_dir = os.path.join(in_dir, \"unified_train/\")\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "\n",
    "with open(os.path.join(out_dir, \"train_graph.pkl\"), \"wb\") as fout:\n",
    "    pickle.dump(G, fout)\n",
    "\n",
    "fn_to_tripleNrel = {\n",
    "    \"a2sp.train.tsv\": (h2sp_triples, SIM_RELATION),\n",
    "    \"a2cp.train.tsv\": (h2cp_triples, COMPL_RELATION),\n",
    "    \"q2a.train.tsv\": (q2h_triples, REL_RELATION),\n",
    "    \n",
    "    \"q2a.50.train.tsv\": (random.sample(q2h_triples, k=int(0.5*len(q2h_triples))), REL_RELATION),\n",
    "    \"q2a.17.train.tsv\": (random.sample(q2h_triples, k=int(0.17*len(q2h_triples))), REL_RELATION),\n",
    "    \n",
    "    \"a2sp.50.train.tsv\": (random.sample(h2sp_triples, k=int(0.5*len(h2sp_triples))), SIM_RELATION)\n",
    "}\n",
    "\n",
    "for fn, (triples, relation) in fn_to_tripleNrel.items():\n",
    "    with open(os.path.join(out_dir, fn), \"w\") as fout:\n",
    "        for (hid, pos_tid, neg_tid) in triples:\n",
    "            fout.write(f\"{hid}\\t{pos_tid}\\t{neg_tid}\\t{relation}\\n\")\n",
    "            \n",
    "out_dir = os.path.join(in_dir, \"unified_test/\")\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "\n",
    "# for similar items\n",
    "fn_to_aids = {\n",
    "    \"anchors.train.sim.tsv\": list(train_aid_to_simpids.keys()),\n",
    "    \"anchors.val.sim.tsv\": list(val_aid_to_simpids.keys()),\n",
    "    \"anchors.test.sim.tsv\": list(test_aid_to_simpids.keys()),\n",
    "    \"anchors.test.sim.small.tsv\": random.sample(list(test_aid_to_simpids.keys()), k=10000)\n",
    "}\n",
    "for fn, aids in fn_to_aids.items():\n",
    "    with open(os.path.join(out_dir, fn), \"w\") as fout:\n",
    "        for aid in aids:\n",
    "            text = eid_to_text[aid]\n",
    "            fout.write(f\"{aid}\\t{text}\\t{SIM_RELATION}\\n\")\n",
    "            \n",
    "fn_to_arels = {\n",
    "    \"arels.train.sim.tsv\": [(aid, pid) for aid, simpids in train_aid_to_simpids.items() for pid in simpids],\n",
    "    \"arels.val.sim.tsv\": [(aid, pid) for aid, simpids in val_aid_to_simpids.items() for pid in simpids],\n",
    "    \"arels.test.sim.tsv\": [(aid, pid) for aid, simpids in test_aid_to_simpids.items() for pid in simpids],\n",
    "}\n",
    "for fn, arels in fn_to_arels.items():\n",
    "    with open(os.path.join(out_dir, fn), \"w\") as fout:\n",
    "        for (aid, pid) in arels:\n",
    "            fout.write(f\"{aid}\\tQ0\\t{pid}\\t{1}\\n\")\n",
    "            \n",
    "# for complementary items\n",
    "fn_to_aids = {\n",
    "    \"anchors.train.compl.tsv\": list(train_aid_to_complpids.keys()),\n",
    "    \"anchors.val.compl.tsv\": list(val_aid_to_complpids.keys()),\n",
    "    \"anchors.test.compl.tsv\": list(test_aid_to_complpids.keys()),\n",
    "}\n",
    "for fn, aids in fn_to_aids.items():\n",
    "    with open(os.path.join(out_dir, fn), \"w\") as fout:\n",
    "        for aid in aids:\n",
    "            text = eid_to_text[aid]\n",
    "            fout.write(f\"{aid}\\t{text}\\t{COMPL_RELATION}\\n\")\n",
    "fn_to_arels = {\n",
    "    \"arels.train.compl.tsv\": [(aid, pid) for aid, complpids in train_aid_to_complpids.items() for pid in complpids],\n",
    "    \"arels.val.compl.tsv\": [(aid, pid) for aid, complpids in val_aid_to_complpids.items() for pid in complpids],\n",
    "    \"arels.test.compl.tsv\": [(aid, pid) for aid, complpids in test_aid_to_complpids.items() for pid in complpids]\n",
    "}\n",
    "for fn, arels in fn_to_arels.items():\n",
    "    with open(os.path.join(out_dir, fn), \"w\") as fout:\n",
    "        for (aid, pid) in arels:\n",
    "            fout.write(f\"{aid}\\tQ0\\t{pid}\\t{1}\\n\")\n",
    "            \n",
    "# for queries\n",
    "fn_to_qids = {\n",
    "    \"queries.train.tsv\": list(train_qid_to_pids.keys()),\n",
    "    \"queries.val.tsv\": list(val_qid_to_pids.keys()),\n",
    "    \"queries.test.tsv\": list(test_qid_to_pids.keys()),\n",
    "    \"queries.test.small.tsv\": random.sample(list(test_qid_to_pids.keys()), k=10000)\n",
    "}\n",
    "for fn, qids in fn_to_qids.items():\n",
    "    with open(os.path.join(out_dir, fn), \"w\") as fout:\n",
    "        for qid in qids:\n",
    "            text = eid_to_text[qid]\n",
    "            fout.write(f\"{qid}\\t{text}\\t{REL_RELATION}\\n\")\n",
    "            \n",
    "            \n",
    "    \n",
    "            \n",
    "fn_to_qrels = {\n",
    "    \"qrels.train.tsv\": [(qid, pid) for qid, pids in train_qid_to_pids.items() for pid in pids],\n",
    "    \"qrels.val.tsv\": [(qid, pid) for qid, pids in val_qid_to_pids.items() for pid in pids],\n",
    "    \"qrels.test.tsv\": [(qid, pid) for (qid, pids) in test_qid_to_pids.items() for pid in pids],\n",
    "}\n",
    "\n",
    "for fn, qrels in fn_to_qrels.items():\n",
    "    with open(os.path.join(out_dir, fn), \"w\") as fout:\n",
    "        for (qid, pid) in qrels:\n",
    "            fout.write(f\"{qid}\\tQ0\\t{pid}\\t{1}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f2200dc5-3324-4c10-9b5e-16cec465837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = os.path.join(in_dir, \"unified_train/\")\n",
    "extra_fn_to_tripleNrel = {\n",
    "    \"max2_qorient_q2p.train.tsv\": (max2_q2p_triples, REL_RELATION),\n",
    "    \"max5_qorient_q2p.train.tsv\": (max5_q2p_triples, REL_RELATION),\n",
    "}\n",
    "for fn, (triples, relation) in extra_fn_to_tripleNrel.items():\n",
    "    with open(os.path.join(out_dir, fn), \"w\") as fout:\n",
    "        for (hid, pos_tid, neg_tid) in triples:\n",
    "            fout.write(f\"{hid}\\t{pos_tid}\\t{neg_tid}\\t{relation}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258cecf7-0d58-4d94-afef-9545dc3597d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"query oriented sampling max2, unique queries = {:,}, unique items = {:,}\".format(\n",
    "    len(set([q for (q, _, _) in max2_q2p_triples])), len(set([p for (_, p, _) in max2_q2p_triples]))))\n",
    "print(\"query oriented sampling max5, unique queries = {:,}, unique items = {:,}\".format(\n",
    "    len(set([q for (q, _, _) in max5_q2p_triples])), len(set([p for (_, p, _) in max5_q2p_triples]))))\n",
    "print(\"item oriented sampling, unique queries = {:,}, unique items = {:,}\".format(\n",
    "    len(set([q for (q, _, _) in q2h_triples])), len(set([p for (_, p, _) in q2h_triples]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4ba3f5-2edf-4daf-bf3f-8e73db2a3163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "out_dir = os.path.join(in_dir, \"unified_train/\")\n",
    "for path in os.listdir(out_dir):\n",
    "    path = os.path.join(out_dir, path)\n",
    "    ! wc -l $path\n",
    "    ! head -n 3 $path\n",
    "    ! tail -n 3 $path\n",
    "    print(\"=\"*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b378de5b-cadf-4b68-bba5-950036ddb570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2381059\tgoal zero solar generator\n",
      "2117076\tGoal Zero Yeti 500X 500-Watt Hour Portable Solar Generator ; Portable Solar Generators\n",
      "437646\tNATURE'S GENERATOR Elite Series 1200-Watt Hour Portable Solar Generator ; Portable Solar Generators\n"
     ]
    }
   ],
   "source": [
    "hid, pos_tid, neg_tid = (2381059,2117076,437646)\n",
    "\n",
    "! grep -P \"^{hid}\\t\" \"/home/jupyter/jointly_rec_and_search/datasets/unified_kgc/all_entities.tsv\"\n",
    "! grep -P \"^{pos_tid}\\t\" \"/home/jupyter/jointly_rec_and_search/datasets/unified_kgc/all_entities.tsv\"\n",
    "! grep -P \"^{neg_tid}\\t\" \"/home/jupyter/jointly_rec_and_search/datasets/unified_kgc/all_entities.tsv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956ceaa8-97f5-44a3-bd29-47f8816a014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = os.path.join(in_dir, \"unified_test/\")\n",
    "for path in os.listdir(out_dir):\n",
    "    path = os.path.join(out_dir, path)\n",
    "    ! wc -l $path\n",
    "    ! head -n 3 $path\n",
    "    ! tail -n 3 $path\n",
    "    print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a96fa8d1-593f-4094-8f1e-d7631f62f24e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(336188, 172991)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max5_h2q), len(max5_h2sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13250e20-8709-43aa-9e7a-a66225ba1ddf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-9.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
