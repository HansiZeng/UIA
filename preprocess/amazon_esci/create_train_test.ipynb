{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17304e94-e9ef-4e3f-8bd6-13bf08c02015",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 308664/308664 [00:01<00:00, 160232.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of edges = 1,245,760, number of multi-attr edges = 493, (0.000)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "from collections import defaultdict\n",
    "import random \n",
    "\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "random.seed(4680)\n",
    "\n",
    "in_dir = \"/home/jupyter/unity_jointly_rec_and_search/datasets/amazon_esci_dataset/data/processed/public/task_1_query_product_ranking/\"\n",
    "datas = []\n",
    "         \n",
    "fns = [\n",
    "    \"train_aid_to_simpids.pkl\",\n",
    "    \"val_aid_to_simpids.pkl\",\n",
    "    \"test_aid_to_simpids.pkl\",\n",
    "    \n",
    "    \"train_aid_to_complpids.pkl\",\n",
    "    \"val_aid_to_complpids.pkl\",\n",
    "    \"test_aid_to_complpids\",\n",
    "    \n",
    "    \"train_qid_to_relpids.pkl\",\n",
    "    \"val_qid_to_relpids.pkl\",\n",
    "    \"test_qid_to_relpids\",\n",
    "    ]\n",
    "for fn in fns:\n",
    "    with open(os.path.join(in_dir, fn), \"rb\") as fin:\n",
    "        datas.append(pkl.load(fin))\n",
    "\n",
    "train_aid_to_simpids, val_aid_to_simpids,test_aid_to_simpids, train_aid_to_complpids, val_aid_to_complpids, test_aid_to_complpids, train_qid_to_pids, val_qid_to_pids,test_qid_to_pids = datas\n",
    "\n",
    "G = nx.MultiDiGraph()\n",
    "SIM_RELATION = \"is_similar_to\"\n",
    "COMPL_RELATION = \"is_complementary_to\"\n",
    "REL_RELATION = \"is_relevant_to\"\n",
    "\n",
    "for aid, sim_pids in train_aid_to_simpids.items():\n",
    "    triples = [(aid, sim_pid, {\"type\":SIM_RELATION}) for sim_pid in sim_pids]\n",
    "    G.add_edges_from(triples)\n",
    "    \n",
    "for aid, compl_pids in train_aid_to_complpids.items():\n",
    "    triples = [(aid, compl_pid, {\"type\":COMPL_RELATION}) for compl_pid in compl_pids]\n",
    "    G.add_edges_from(triples)\n",
    "    \n",
    "for qid, pids in train_qid_to_pids.items():\n",
    "    triples = [(pid, qid, {\"type\": REL_RELATION}) for pid in pids]\n",
    "    G.add_edges_from(triples)\n",
    "    \n",
    "multi_edge_pairs = []\n",
    "for n, nbrs_dict in tqdm(G.adj.items(), total=G.number_of_nodes()):\n",
    "    for nbr_node, edge_attrs in nbrs_dict.items():\n",
    "        assert len(edge_attrs) == 1 or len(edge_attrs) == 2\n",
    "        if len(edge_attrs) == 2:\n",
    "            multi_edge_pairs.append((n, nbr_node))\n",
    "            \n",
    "print(\"number of edges = {:,}, number of multi-attr edges = {:,}, ({:.3f})\".format(G.number_of_edges(), len(multi_edge_pairs), \n",
    "                                                                                   len(multi_edge_pairs)/G.number_of_edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c520f50e-46d3-4a50-8c68-09d6ccdaf412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ignore hids = 1242\n"
     ]
    }
   ],
   "source": [
    "def create_triples(hid, pos_tid, miss_hids, duplicate_pairs, eid_to_text, sampler=None):\n",
    "    if sampler != None:\n",
    "        assert type(sampler) == dict, type(sampler)\n",
    "        if hid not in sampler:\n",
    "            miss_hids.append(hid)\n",
    "            return 0\n",
    "    if eid_to_text[hid] == eid_to_text[pos_tid]:\n",
    "        duplicate_pairs.append((hid, pos_tid))\n",
    "        return 0\n",
    "    \n",
    "    if sampler != None:\n",
    "        neg_tid = random.sample(sampler[hid], k=1)[0]\n",
    "        while neg_tid == pos_tid:\n",
    "            neg_tid = random.sample(sampler[hid], k=1)[0]\n",
    "    else:\n",
    "        neg_tid = random.sample(range(480_000), k=1)[0]\n",
    "        while neg_tid == pos_tid:\n",
    "            neg_tid = random.sample(range(480_000), k=1)[0]\n",
    "            \n",
    "    return (hid, pos_tid, neg_tid)\n",
    "\n",
    "\n",
    "eid_to_text = {}\n",
    "with open(os.path.join(in_dir, \"all_entities.tsv\")) as fin:\n",
    "    for line in fin:\n",
    "        eid, text = line.strip().split(\"\\t\")\n",
    "        eid_to_text[int(eid)] = text\n",
    "        \n",
    "run_path = os.path.join(in_dir, \"runs/bm25.all.run\")\n",
    "df = pd.read_csv(run_path, sep=\" \", names=[\"hid\", \"q0\", \"tid\", \"rank\", \"score\", \"model_name\"])\n",
    "bm25_hid_to_tids = {}\n",
    "ignore_hids = set()\n",
    "for hid, group in df.groupby(\"hid\"):\n",
    "    cand_tids = list(group.tid.values)\n",
    "    if len(cand_tids) < 10:\n",
    "        ignore_hids.add(int(hid))\n",
    "    else:\n",
    "        bm25_hid_to_tids[int(hid)] = [int(x) for x in cand_tids]\n",
    "        \n",
    "print(\"number of ignore hids = {}\".format(len(ignore_hids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05d2639f-6623-4b2e-aa6b-09bb9143ff03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 308664/308664 [00:05<00:00, 53787.87it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miss_hids = 0, duplicate_pairs = 206\n",
      "===========================================================================\n",
      "miss_hids = 78, duplicate_pairs = 220\n",
      "===========================================================================\n",
      "miss_hids = 2,598, duplicate_pairs = 221\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "max5_h2sp = {}\n",
    "max5_h2cp = {}\n",
    "max5_h2q = {}\n",
    "\n",
    "for head_node, nbrs_dict in tqdm(G.adj.items(), total=G.number_of_nodes()):\n",
    "    sim_pids = []\n",
    "    compl_pids = []\n",
    "    rel_qids = []\n",
    "    for tail_node, edge_attrs in nbrs_dict.items():\n",
    "        assert len(edge_attrs) == 1 or len(edge_attrs) == 2\n",
    "        relations = []\n",
    "        for no, edge_attr in edge_attrs.items():\n",
    "            relations.append(edge_attr[\"type\"])\n",
    "        for rel in relations:\n",
    "            assert rel in [SIM_RELATION, COMPL_RELATION, REL_RELATION]\n",
    "            if rel in SIM_RELATION:\n",
    "                sim_pids.append(tail_node)\n",
    "            if rel in COMPL_RELATION:\n",
    "                compl_pids.append(tail_node)\n",
    "            if rel in REL_RELATION:\n",
    "                rel_qids.append(tail_node)\n",
    "    if len(sim_pids) != 0:\n",
    "        max5_h2sp[head_node] = random.sample(sim_pids, k=len(sim_pids))[:5]\n",
    "    if len(compl_pids) != 0:\n",
    "        max5_h2cp[head_node] = random.sample(compl_pids, k=len(compl_pids))[:5]\n",
    "    if len(rel_qids) != 0:\n",
    "        max5_h2q[head_node] = random.sample(rel_qids, k=len(rel_qids))[:5]\n",
    "        \n",
    "miss_hids = []\n",
    "duplicate_pairs = []\n",
    "\n",
    "h2sp_triples = []\n",
    "h2cp_triples = []\n",
    "q2h_triples = []\n",
    "for hid, tail_ids in max5_h2sp.items():\n",
    "    for pos_tid in tail_ids:\n",
    "        triple = create_triples(hid, pos_tid, miss_hids, duplicate_pairs, eid_to_text)\n",
    "        if triple != 0:\n",
    "            h2sp_triples.append(triple)\n",
    "print(\"miss_hids = {:,}, duplicate_pairs = {:,}\".format(len(miss_hids), len(duplicate_pairs)))\n",
    "print(\"=\"*75)\n",
    "for hid, tail_ids in max5_h2cp.items():\n",
    "    for pos_tid in tail_ids:\n",
    "        triple = create_triples(hid, pos_tid, miss_hids, duplicate_pairs, eid_to_text, sampler=bm25_hid_to_tids)\n",
    "        if triple != 0:\n",
    "            h2cp_triples.append(triple)\n",
    "print(\"miss_hids = {:,}, duplicate_pairs = {:,}\".format(len(miss_hids), len(duplicate_pairs)))\n",
    "print(\"=\"*75)\n",
    "for pos_tid, head_ids in max5_h2q.items():\n",
    "    for hid in head_ids:\n",
    "        triple = create_triples(hid, pos_tid, miss_hids, duplicate_pairs, eid_to_text, sampler=bm25_hid_to_tids)\n",
    "        if triple != 0:\n",
    "            q2h_triples.append(triple)\n",
    "print(\"miss_hids = {:,}, duplicate_pairs = {:,}\".format(len(miss_hids), len(duplicate_pairs)))\n",
    "print(\"=\"*75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2870fd3d-476c-4d0e-af76-511ed6275a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miss_hids = 3,949, duplicate_pairs = 222\n",
      "===========================================================================\n",
      "miss_hids = 4,562, duplicate_pairs = 223\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "max5_q2p = {}\n",
    "max2_q2p = {}\n",
    "max5_q2p_triples = []\n",
    "max2_q2p_triples = []\n",
    "for qid, pids in train_qid_to_pids.items():\n",
    "    max5_q2p[qid] = random.sample(pids, k=len(pids))[:5]\n",
    "    max2_q2p[qid] = random.sample(pids, k=len(pids))[:2]\n",
    "    \n",
    "for qid, pos_pids in max5_q2p.items():\n",
    "    for pos_pid in pos_pids:\n",
    "        triple = create_triples(qid, pos_pid, miss_hids, duplicate_pairs, eid_to_text, sampler=bm25_hid_to_tids)\n",
    "        if triple != 0:\n",
    "            max5_q2p_triples.append(triple)\n",
    "print(\"miss_hids = {:,}, duplicate_pairs = {:,}\".format(len(miss_hids), len(duplicate_pairs)))\n",
    "print(\"=\"*75)\n",
    "\n",
    "for qid, pos_pids in max2_q2p.items():\n",
    "    for pos_pid in pos_pids:\n",
    "        triple = create_triples(qid, pos_pid, miss_hids, duplicate_pairs, eid_to_text, sampler=bm25_hid_to_tids)\n",
    "        if triple != 0:\n",
    "            max2_q2p_triples.append(triple)\n",
    "print(\"miss_hids = {:,}, duplicate_pairs = {:,}\".format(len(miss_hids), len(duplicate_pairs)))\n",
    "print(\"=\"*75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49c50a10-dc3f-468f-9c09-61b1378e1049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query oriented sampling max2, unique queries = 16,392, unique items = 31,144\n",
      "query oriented sampling max5, unique queries = 16,392, unique items = 67,724\n",
      "item oriented sampling, unique queries = 16,389, unique items = 134,228\n"
     ]
    }
   ],
   "source": [
    "print(\"query oriented sampling max2, unique queries = {:,}, unique items = {:,}\".format(\n",
    "    len(set([q for (q, _, _) in max2_q2p_triples])), len(set([p for (_, p, _) in max2_q2p_triples]))))\n",
    "print(\"query oriented sampling max5, unique queries = {:,}, unique items = {:,}\".format(\n",
    "    len(set([q for (q, _, _) in max5_q2p_triples])), len(set([p for (_, p, _) in max5_q2p_triples]))))\n",
    "print(\"item oriented sampling, unique queries = {:,}, unique items = {:,}\".format(\n",
    "    len(set([q for (q, _, _) in q2h_triples])), len(set([p for (_, p, _) in q2h_triples]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6402501-e4cc-4451-ae4d-b9cbe75e1f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(481721, 101280, 143317, 31664, 70024, 2089, 14630, 4884)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(h2sp_triples), len(h2cp_triples), len(q2h_triples), len(max2_q2p_triples), len(max5_q2p_triples), \\\n",
    "len(test_qid_to_pids), len(test_aid_to_simpids), len(test_aid_to_complpids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "513ddd43-551d-451f-b6f1-a8aef7685b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "out_dir = os.path.join(in_dir, \"unified_train/\")\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "\n",
    "with open(os.path.join(out_dir, \"train_graph.pkl\"), \"wb\") as fout:\n",
    "    pickle.dump(G, fout)\n",
    "\n",
    "fn_to_tripleNrel = {\n",
    "    \"a2sp.train.tsv\": (h2sp_triples, SIM_RELATION),\n",
    "    \"a2cp.train.tsv\": (h2cp_triples, COMPL_RELATION),\n",
    "    \"q2a.train.tsv\": (q2h_triples, REL_RELATION),\n",
    "    \"max2_qorient_q2p.train.tsv\": (max2_q2p_triples, REL_RELATION),\n",
    "    \"max5_qorient_q2p.train.tsv\": (max5_q2p_triples, REL_RELATION),\n",
    "    \n",
    "    \"q2a.50.train.tsv\": (random.sample(q2h_triples, k=int(0.5*len(q2h_triples))), REL_RELATION),\n",
    "    \"q2a.17.train.tsv\": (random.sample(q2h_triples, k=int(0.17*len(q2h_triples))), REL_RELATION),\n",
    "    \n",
    "    \"a2sp.50.train.tsv\": (random.sample(h2sp_triples, k=int(0.5*len(h2sp_triples))), SIM_RELATION)\n",
    "}\n",
    "\n",
    "for fn, (triples, relation) in fn_to_tripleNrel.items():\n",
    "    with open(os.path.join(out_dir, fn), \"w\") as fout:\n",
    "        for (hid, pos_tid, neg_tid) in triples:\n",
    "            fout.write(f\"{hid}\\t{pos_tid}\\t{neg_tid}\\t{relation}\\n\")\n",
    "            \n",
    "out_dir = os.path.join(in_dir, \"unified_test/\")\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "\n",
    "# for similar items\n",
    "fn_to_aids = {\n",
    "    \"anchors.train.sim.tsv\": list(train_aid_to_simpids.keys()),\n",
    "    \"anchors.val.sim.tsv\": list(val_aid_to_simpids.keys()),\n",
    "    \"anchors.test.sim.tsv\": list(test_aid_to_simpids.keys()),\n",
    "}\n",
    "for fn, aids in fn_to_aids.items():\n",
    "    with open(os.path.join(out_dir, fn), \"w\") as fout:\n",
    "        for aid in aids:\n",
    "            text = eid_to_text[aid]\n",
    "            fout.write(f\"{aid}\\t{text}\\t{SIM_RELATION}\\n\")\n",
    "            \n",
    "fn_to_arels = {\n",
    "    \"arels.train.sim.tsv\": [(aid, pid) for aid, simpids in train_aid_to_simpids.items() for pid in simpids],\n",
    "    \"arels.val.sim.tsv\": [(aid, pid) for aid, simpids in val_aid_to_simpids.items() for pid in simpids],\n",
    "    \"arels.test.sim.tsv\": [(aid, pid) for aid, simpids in test_aid_to_simpids.items() for pid in simpids],\n",
    "}\n",
    "for fn, arels in fn_to_arels.items():\n",
    "    with open(os.path.join(out_dir, fn), \"w\") as fout:\n",
    "        for (aid, pid) in arels:\n",
    "            fout.write(f\"{aid}\\tQ0\\t{pid}\\t{1}\\n\")\n",
    "            \n",
    "# for complementary items\n",
    "fn_to_aids = {\n",
    "    \"anchors.train.compl.tsv\": list(train_aid_to_complpids.keys()),\n",
    "    \"anchors.val.compl.tsv\": list(val_aid_to_complpids.keys()),\n",
    "    \"anchors.test.compl.tsv\": list(test_aid_to_complpids.keys()),\n",
    "}\n",
    "for fn, aids in fn_to_aids.items():\n",
    "    with open(os.path.join(out_dir, fn), \"w\") as fout:\n",
    "        for aid in aids:\n",
    "            text = eid_to_text[aid]\n",
    "            fout.write(f\"{aid}\\t{text}\\t{COMPL_RELATION}\\n\")\n",
    "fn_to_arels = {\n",
    "    \"arels.train.compl.tsv\": [(aid, pid) for aid, complpids in train_aid_to_complpids.items() for pid in complpids],\n",
    "    \"arels.val.compl.tsv\": [(aid, pid) for aid, complpids in val_aid_to_complpids.items() for pid in complpids],\n",
    "    \"arels.test.compl.tsv\": [(aid, pid) for aid, complpids in test_aid_to_complpids.items() for pid in complpids]\n",
    "}\n",
    "for fn, arels in fn_to_arels.items():\n",
    "    with open(os.path.join(out_dir, fn), \"w\") as fout:\n",
    "        for (aid, pid) in arels:\n",
    "            fout.write(f\"{aid}\\tQ0\\t{pid}\\t{1}\\n\")\n",
    "            \n",
    "# for queries\n",
    "fn_to_qids = {\n",
    "    \"queries.train.tsv\": list(train_qid_to_pids.keys()),\n",
    "    \"queries.val.tsv\": list(val_qid_to_pids.keys()),\n",
    "    \"queries.test.tsv\": list(test_qid_to_pids.keys()),\n",
    "}\n",
    "for fn, qids in fn_to_qids.items():\n",
    "    with open(os.path.join(out_dir, fn), \"w\") as fout:\n",
    "        for qid in qids:\n",
    "            text = eid_to_text[qid]\n",
    "            fout.write(f\"{qid}\\t{text}\\t{REL_RELATION}\\n\")\n",
    "            \n",
    "            \n",
    "    \n",
    "            \n",
    "fn_to_qrels = {\n",
    "    \"qrels.train.tsv\": [(qid, pid) for qid, pids in train_qid_to_pids.items() for pid in pids],\n",
    "    \"qrels.val.tsv\": [(qid, pid) for qid, pids in val_qid_to_pids.items() for pid in pids],\n",
    "    \"qrels.test.tsv\": [(qid, pid) for (qid, pids) in test_qid_to_pids.items() for pid in pids],\n",
    "}\n",
    "\n",
    "for fn, qrels in fn_to_qrels.items():\n",
    "    with open(os.path.join(out_dir, fn), \"w\") as fout:\n",
    "        for (qid, pid) in qrels:\n",
    "            fout.write(f\"{qid}\\tQ0\\t{pid}\\t{1}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f765171-1c19-4e20-8883-a3c03a0d18f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240860 /home/jupyter/unity_jointly_rec_and_search/datasets/amazon_esci_dataset/data/processed/public/task_1_query-product_ranking/unified_train/a2sp.50.train.tsv\n",
      "224793\t224795\t451839\tis_similar_to\n",
      "444401\t404653\t47706\tis_similar_to\n",
      "6061\t6060\t39532\tis_similar_to\n",
      "256329\t467741\t356894\tis_similar_to\n",
      "161768\t2820\t160752\tis_similar_to\n",
      "292196\t394419\t371576\tis_similar_to\n",
      "====================================================================================================\n",
      "70024 /home/jupyter/unity_jointly_rec_and_search/datasets/amazon_esci_dataset/data/processed/public/task_1_query-product_ranking/unified_train/max5_qorient_q2p.train.tsv\n",
      "483187\t13535\t403042\tis_relevant_to\n",
      "483187\t13530\t378779\tis_relevant_to\n",
      "483187\t13528\t334862\tis_relevant_to\n",
      "487692\t70931\t98065\tis_relevant_to\n",
      "494832\t165332\t2691\tis_relevant_to\n",
      "494832\t165328\t360286\tis_relevant_to\n",
      "====================================================================================================\n",
      "155080 /home/jupyter/unity_jointly_rec_and_search/datasets/amazon_esci_dataset/data/processed/public/task_1_query-product_ranking/unified_train/train_graph.pkl\n",
      "�\u0003cnetworkx.classes.multidigraph\n",
      "MultiDiGraph\n",
      "q\u0000)�q\u0001}q\u0002(X\u0015\u0000\u0000\u0000edge_key_dict_factoryq\u0003cbuiltins\n",
      "\u0000uJ\u0018r\u0007\u0000}r�#4\u0000(J0#\u0001\u0000j\u0011�\u000f\u0000J1#\u0001\u0000j\u001d",
      "�\u000f\u0000J3#\u0001\u0000j�m/\u0000uJ3#\u0001\u0000}r�#4\u0000J{�\u0007\u0000}r�#4\u0000(J��\u0004\u0000j�g!\u0000J;�\u0003\u0000j\u0001n/\u0000J<�\u0003\u0000j\u0004n/\u0000J=�\u0003\u0000j�g!\u0000J^�\u0003\u0000j�g!\u0000uJ;�\u0003\u0000}r�#4\u0000J<�\u0003\u0000}r�#4\u0000J/f\u0007\u0000}r�#4\u0000(M\u0010�jQ�\n",
      "\u0000M\u0015�j`�\n",
      "\u0000M\u001e",
      "�n/\u0000M\u0017�j\u000b",
      "n/\u0000uM\u001e",
      "�}r�#4\u0000M\u0017�}r�#4\u0000J\f",
      "q\u0007\u0000}r�#4\u0000(J\u0011\u0015\u0001\u0000j5W\u000f\u0000J\u0012\u0015\u0001\u0000jVW\u000f\u0000J\u0013\u0015\u0001\u0000jmW\u000f\u0000J\u0014\u0015\u0001\u0000j\u000fn/\u0000J\u0019\u0015\u0001\u0000j�W\u000f\u0000J\u001a\u0015\u0001\u0000j\u0012n/\u0000uJ\u0014\u0015\u0001\u0000}r�#4\u0000J\u001a\u0015\u0001\u0000}r�#4\u0000JЅ\u0002\u0000}r�#4\u0000J��\u0007\u0000}r�#4\u0000(JЅ\u0002\u0000j\u0015n/\u0000Jԅ\u0002\u0000j�0\u0019\u0000uuX\u0005\u0000\u0000\u0000_succr�#4\u0000jȵ\u0004\u0000ub.====================================================================================================\n",
      "481721 /home/jupyter/unity_jointly_rec_and_search/datasets/amazon_esci_dataset/data/processed/public/task_1_query-product_ranking/unified_train/a2sp.train.tsv\n",
      "0\t263895\t330719\tis_similar_to\n",
      "0\t282678\t113470\tis_similar_to\n",
      "0\t263892\t9779\tis_similar_to\n",
      "317242\t317256\t211317\tis_similar_to\n",
      "317242\t317267\t190424\tis_similar_to\n",
      "317242\t439481\t160487\tis_similar_to\n",
      "====================================================================================================\n",
      "143317 /home/jupyter/unity_jointly_rec_and_search/datasets/amazon_esci_dataset/data/processed/public/task_1_query-product_ranking/unified_train/q2a.train.tsv\n",
      "482198\t0\t251198\tis_relevant_to\n",
      "501060\t263873\t257693\tis_relevant_to\n",
      "501060\t263875\t251207\tis_relevant_to\n",
      "487692\t70932\t158682\tis_relevant_to\n",
      "487692\t70938\t66779\tis_relevant_to\n",
      "494832\t165328\t133542\tis_relevant_to\n",
      "====================================================================================================\n",
      "wc: /home/jupyter/unity_jointly_rec_and_search/datasets/amazon_esci_dataset/data/processed/public/task_1_query-product_ranking/unified_train/.ipynb_checkpoints: Is a directory\n",
      "0 /home/jupyter/unity_jointly_rec_and_search/datasets/amazon_esci_dataset/data/processed/public/task_1_query-product_ranking/unified_train/.ipynb_checkpoints\n",
      "head: error reading '/home/jupyter/unity_jointly_rec_and_search/datasets/amazon_esci_dataset/data/processed/public/task_1_query-product_ranking/unified_train/.ipynb_checkpoints': Is a directory\n",
      "tail: error reading '/home/jupyter/unity_jointly_rec_and_search/datasets/amazon_esci_dataset/data/processed/public/task_1_query-product_ranking/unified_train/.ipynb_checkpoints': Is a directory\n",
      "====================================================================================================\n",
      "24363 /home/jupyter/unity_jointly_rec_and_search/datasets/amazon_esci_dataset/data/processed/public/task_1_query-product_ranking/unified_train/q2a.17.train.tsv\n",
      "496628\t189669\t69608\tis_relevant_to\n",
      "494395\t159428\t44597\tis_relevant_to\n",
      "493192\t221995\t15576\tis_relevant_to\n",
      "498705\t218305\t219970\tis_relevant_to\n",
      "502818\t309441\t9378\tis_relevant_to\n",
      "500756\t432962\t266279\tis_relevant_to\n",
      "====================================================================================================\n",
      "101280 /home/jupyter/unity_jointly_rec_and_search/datasets/amazon_esci_dataset/data/processed/public/task_1_query-product_ranking/unified_train/a2cp.train.tsv\n",
      "0\t263879\t317593\tis_complementary_to\n",
      "263873\t263870\t257699\tis_complementary_to\n",
      "263873\t263877\t257692\tis_complementary_to\n",
      "317245\t317244\t183153\tis_complementary_to\n",
      "317245\t317231\t413446\tis_complementary_to\n",
      "317245\t317234\t379123\tis_complementary_to\n",
      "====================================================================================================\n",
      "31664 /home/jupyter/unity_jointly_rec_and_search/datasets/amazon_esci_dataset/data/processed/public/task_1_query-product_ranking/unified_train/max2_qorient_q2p.train.tsv\n",
      "483187\t13533\t101013\tis_relevant_to\n",
      "483187\t13524\t145452\tis_relevant_to\n",
      "494286\t157907\t116713\tis_relevant_to\n",
      "487692\t70932\t35149\tis_relevant_to\n",
      "494832\t165332\t986\tis_relevant_to\n",
      "494832\t165328\t194725\tis_relevant_to\n",
      "====================================================================================================\n",
      "71658 /home/jupyter/unity_jointly_rec_and_search/datasets/amazon_esci_dataset/data/processed/public/task_1_query-product_ranking/unified_train/q2a.50.train.tsv\n",
      "502295\t293996\t150401\tis_relevant_to\n",
      "494609\t162208\t437970\tis_relevant_to\n",
      "496026\t181514\t331328\tis_relevant_to\n",
      "494848\t165562\t224157\tis_relevant_to\n",
      "493580\t148365\t164698\tis_relevant_to\n",
      "486395\t54128\t150109\tis_relevant_to\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "out_dir = os.path.join(in_dir, \"unified_train/\")\n",
    "for path in os.listdir(out_dir):\n",
    "    path = os.path.join(out_dir, path)\n",
    "    ! wc -l $path\n",
    "    ! head -n 3 $path\n",
    "    ! tail -n 3 $path\n",
    "    print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ece00e61-565c-4afc-a69b-6ee77029422c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon Basics Woodcased #2 Pencils, Unsharpened, HB Lead - Box of 144, Bulk Box\n",
      "Environmentally friendly black wood pencils, (30 pcs per barrel) triangle grip pen design, graphite HB lead core with eraser, suitable for children and adults to write sketches and paint\n",
      "Going, Going Gong / Clever Levers\n"
     ]
    }
   ],
   "source": [
    "hid, pos_tid, neg_tid = (0,263895,330719)\n",
    "print(eid_to_text[hid])\n",
    "print(eid_to_text[pos_tid])\n",
    "print(eid_to_text[neg_tid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad2f26e-dd21-40de-be2a-3dbef8ea321f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-9.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
