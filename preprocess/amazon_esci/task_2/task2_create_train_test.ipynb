{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17304e94-e9ef-4e3f-8bd6-13bf08c02015",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 843496/843496 [00:04<00:00, 174902.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of edges = 2,746,193, number of multi-attr edges = 1,090, (0.000)\n",
      "max pids = 1216069\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "from collections import defaultdict\n",
    "import random \n",
    "\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "random.seed(4680)\n",
    "\n",
    "in_dir = \"/home/jupyter/unity_jointly_rec_and_search/datasets/amazon_esci_dataset/task_2_multiclass_product_classification/\"\n",
    "datas = []\n",
    "         \n",
    "fns = [\n",
    "    \"train_aid_to_simpids.pkl\",\n",
    "    \"train_aid_to_complpids.pkl\",\n",
    "    \"train_qid_to_relpids.pkl\",\n",
    "    ]\n",
    "for fn in fns:\n",
    "    with open(os.path.join(in_dir, fn), \"rb\") as fin:\n",
    "        datas.append(pkl.load(fin))\n",
    "\n",
    "train_aid_to_simpids, train_aid_to_complpids, train_qid_to_pids = datas\n",
    "\n",
    "G = nx.MultiDiGraph()\n",
    "SIM_RELATION = \"is_similar_to\"\n",
    "COMPL_RELATION = \"is_complementary_to\"\n",
    "REL_RELATION = \"is_relevant_to\"\n",
    "\n",
    "for aid, sim_pids in train_aid_to_simpids.items():\n",
    "    triples = [(aid, sim_pid, {\"type\":SIM_RELATION}) for sim_pid in sim_pids]\n",
    "    G.add_edges_from(triples)\n",
    "    \n",
    "for aid, compl_pids in train_aid_to_complpids.items():\n",
    "    triples = [(aid, compl_pid, {\"type\":COMPL_RELATION}) for compl_pid in compl_pids]\n",
    "    G.add_edges_from(triples)\n",
    "    \n",
    "for qid, pids in train_qid_to_pids.items():\n",
    "    triples = [(pid, qid, {\"type\": REL_RELATION}) for pid in pids]\n",
    "    G.add_edges_from(triples)\n",
    "    \n",
    "multi_edge_pairs = []\n",
    "for n, nbrs_dict in tqdm(G.adj.items(), total=G.number_of_nodes()):\n",
    "    for nbr_node, edge_attrs in nbrs_dict.items():\n",
    "        assert len(edge_attrs) == 1 or len(edge_attrs) == 2\n",
    "        if len(edge_attrs) == 2:\n",
    "            multi_edge_pairs.append((n, nbr_node))\n",
    "            \n",
    "print(\"number of edges = {:,}, number of multi-attr edges = {:,}, ({:.3f})\".format(G.number_of_edges(), len(multi_edge_pairs), \n",
    "                                                                                   len(multi_edge_pairs)/G.number_of_edges()))\n",
    "\n",
    "PIDS = []\n",
    "with open(os.path.join(in_dir, \"collection_title.tsv\")) as fin:\n",
    "    for line in fin:\n",
    "        PIDS.append(int(line.strip().split(\"\\t\")[0]))\n",
    "print(f\"max pids = {max(PIDS)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c520f50e-46d3-4a50-8c68-09d6ccdaf412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ignore hids = 2354\n"
     ]
    }
   ],
   "source": [
    "eid_to_text = {}\n",
    "with open(os.path.join(in_dir, \"all_entities.tsv\")) as fin:\n",
    "    for line in fin:\n",
    "        eid, text = line.strip().split(\"\\t\")\n",
    "        eid_to_text[int(eid)] = text\n",
    "        \n",
    "run_path = os.path.join(in_dir, \"runs/bm25.all.run\")\n",
    "df = pd.read_csv(run_path, sep=\" \", names=[\"hid\", \"q0\", \"tid\", \"rank\", \"score\", \"model_name\"])\n",
    "bm25_hid_to_tids = {}\n",
    "ignore_hids = set()\n",
    "for hid, group in df.groupby(\"hid\"):\n",
    "    cand_tids = list(group.tid.values)\n",
    "    if len(cand_tids) < 10:\n",
    "        ignore_hids.add(int(hid))\n",
    "    else:\n",
    "        bm25_hid_to_tids[int(hid)] = [int(x) for x in cand_tids]\n",
    "        \n",
    "print(\"number of ignore hids = {}\".format(len(ignore_hids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2870fd3d-476c-4d0e-af76-511ed6275a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miss_hids = 3,697, triples with simnegs = 302,538, triples with complnegs = 31,483, triples with bm25negs = 266,550\n",
      "total triples = 600,571\n",
      "===========================================================================\n",
      "miss_hids = 2,834, triples with simnegs = 237,638, triples with complnegs = 23,994, triples with bm25negs = 190,436\n",
      "total triples = 452,068\n",
      "===========================================================================\n",
      "miss_hids = 1,643, triples with simnegs = 135,694, triples with complnegs = 13,351, triples with bm25negs = 100,286\n",
      "total triples = 249,331\n",
      "===========================================================================\n",
      "miss_hids = 694, triples with simnegs = 58,798, triples with complnegs = 5,662, triples with bm25negs = 41,450\n",
      "total triples = 105,910\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "MAX_PID = max(PIDS)\n",
    "def create_mixture_triples_for_search(hid, pos_tid, hid_to_simpids_sampler, hid_to_complpids_sampler, bm25_hid_to_tid_sampler,\n",
    "                                     miss_hids, hids_have_simnegs, hids_have_complnegs, hids_have_bm25negs):\n",
    "    if pos_tid in hid_to_simpids_sampler:\n",
    "        neg_tid = random.sample(hid_to_simpids_sampler[pos_tid], k=1)[0]\n",
    "        assert neg_tid != pos_tid\n",
    "        hids_have_simnegs.append(hid)\n",
    "    elif pos_tid in hid_to_complpids_sampler:\n",
    "        neg_tid = random.sample(hid_to_complpids_sampler[pos_tid], k=1)[0]\n",
    "        assert neg_tid != pos_tid\n",
    "        hids_have_complnegs.append(hid)\n",
    "    elif hid in bm25_hid_to_tid_sampler:\n",
    "        neg_tid = random.sample(bm25_hid_to_tid_sampler[hid], k=1)[0]\n",
    "        while neg_tid == pos_tid:\n",
    "            neg_tid = random.sample(bm25_hid_to_tid_sampler[hid], k=1)[0]\n",
    "        hids_have_bm25negs.append(hid)\n",
    "    else:\n",
    "        miss_hids.append(hid)\n",
    "        return 0 \n",
    "    \n",
    "    return (hid, pos_tid, neg_tid)\n",
    "\n",
    "max15_q2p = {}\n",
    "max10_q2p = {}\n",
    "max5_q2p = {}\n",
    "max2_q2p = {}\n",
    "\n",
    "max15_q2p_triples = []\n",
    "max10_q2p_triples = []\n",
    "max5_q2p_triples = []\n",
    "max2_q2p_triples = []\n",
    "for qid, pids in train_qid_to_pids.items():\n",
    "    max15_q2p[qid] = random.sample(pids, k=len(pids))[:15]\n",
    "    max10_q2p[qid] = random.sample(pids, k=len(pids))[:10]\n",
    "    max5_q2p[qid] = random.sample(pids, k=len(pids))[:5]\n",
    "    max2_q2p[qid] = random.sample(pids, k=len(pids))[:2]\n",
    "    \n",
    "miss_hids, hids_have_simnegs, hids_have_complnegs, hids_have_bm25negs = [], [], [], []\n",
    "total_triples = 0   \n",
    "for qid, pos_pids in max15_q2p.items():\n",
    "    for pos_pid in pos_pids:\n",
    "        triple = create_mixture_triples_for_search(qid, pos_pid, \n",
    "                                                   hid_to_simpids_sampler=train_aid_to_simpids,\n",
    "                                                   hid_to_complpids_sampler=train_aid_to_complpids, \n",
    "                                                   bm25_hid_to_tid_sampler=bm25_hid_to_tids,\n",
    "                                                   miss_hids=miss_hids, \n",
    "                                                   hids_have_simnegs=hids_have_simnegs, \n",
    "                                                   hids_have_complnegs=hids_have_complnegs, \n",
    "                                                   hids_have_bm25negs=hids_have_bm25negs)\n",
    "        if triple != 0:\n",
    "            max15_q2p_triples.append(triple)\n",
    "            total_triples += 1\n",
    "print(\"miss_hids = {:,}, triples with simnegs = {:,}, triples with complnegs = {:,}, triples with bm25negs = {:,}\".format(\n",
    "    len(miss_hids), len(hids_have_simnegs), len(hids_have_complnegs), len(hids_have_bm25negs)\n",
    "))\n",
    "print(\"total triples = {:,}\".format(total_triples))\n",
    "print(\"=\"*75)\n",
    "\n",
    "miss_hids, hids_have_simnegs, hids_have_complnegs, hids_have_bm25negs = [], [], [], []\n",
    "total_triples = 0   \n",
    "for qid, pos_pids in max10_q2p.items():\n",
    "    for pos_pid in pos_pids:\n",
    "        triple = create_mixture_triples_for_search(qid, pos_pid, \n",
    "                                                   hid_to_simpids_sampler=train_aid_to_simpids,\n",
    "                                                   hid_to_complpids_sampler=train_aid_to_complpids, \n",
    "                                                   bm25_hid_to_tid_sampler=bm25_hid_to_tids,\n",
    "                                                   miss_hids=miss_hids, \n",
    "                                                   hids_have_simnegs=hids_have_simnegs, \n",
    "                                                   hids_have_complnegs=hids_have_complnegs, \n",
    "                                                   hids_have_bm25negs=hids_have_bm25negs)\n",
    "        if triple != 0:\n",
    "            max10_q2p_triples.append(triple)\n",
    "            total_triples += 1\n",
    "print(\"miss_hids = {:,}, triples with simnegs = {:,}, triples with complnegs = {:,}, triples with bm25negs = {:,}\".format(\n",
    "    len(miss_hids), len(hids_have_simnegs), len(hids_have_complnegs), len(hids_have_bm25negs)\n",
    "))\n",
    "print(\"total triples = {:,}\".format(total_triples))\n",
    "print(\"=\"*75)\n",
    "\n",
    "miss_hids, hids_have_simnegs, hids_have_complnegs, hids_have_bm25negs = set(), set(), set(), set()\n",
    "total_triples = 0   \n",
    "for qid, pos_pids in max5_q2p.items():\n",
    "    for pos_pid in pos_pids:\n",
    "        triple = create_mixture_triples_for_search(qid, pos_pid, \n",
    "                                                   hid_to_simpids_sampler=train_aid_to_simpids,\n",
    "                                                   hid_to_complpids_sampler=train_aid_to_complpids, \n",
    "                                                   bm25_hid_to_tid_sampler=bm25_hid_to_tids,\n",
    "                                                   miss_hids=miss_hids, \n",
    "                                                   hids_have_simnegs=hids_have_simnegs, \n",
    "                                                   hids_have_complnegs=hids_have_complnegs, \n",
    "                                                   hids_have_bm25negs=hids_have_bm25negs)\n",
    "        if triple != 0:\n",
    "            max5_q2p_triples.append(triple)\n",
    "            total_triples += 1\n",
    "print(\"miss_hids = {:,}, triples with simnegs = {:,}, triples with complnegs = {:,}, triples with bm25negs = {:,}\".format(\n",
    "    len(miss_hids), len(hids_have_simnegs), len(hids_have_complnegs), len(hids_have_bm25negs)\n",
    "))\n",
    "print(\"total triples = {:,}\".format(total_triples))\n",
    "print(\"=\"*75)\n",
    "\n",
    "miss_hids, hids_have_simnegs, hids_have_complnegs, hids_have_bm25negs = set(), set(), set(), set()\n",
    "total_triples = 0   \n",
    "for qid, pos_pids in max2_q2p.items():\n",
    "    for pos_pid in pos_pids:\n",
    "        triple = create_mixture_triples_for_search(qid, pos_pid, \n",
    "                                                   hid_to_simpids_sampler=train_aid_to_simpids,\n",
    "                                                   hid_to_complpids_sampler=train_aid_to_complpids, \n",
    "                                                   bm25_hid_to_tid_sampler=bm25_hid_to_tids,\n",
    "                                                   miss_hids=miss_hids, \n",
    "                                                   hids_have_simnegs=hids_have_simnegs, \n",
    "                                                   hids_have_complnegs=hids_have_complnegs, \n",
    "                                                   hids_have_bm25negs=hids_have_bm25negs)\n",
    "        if triple != 0:\n",
    "            max2_q2p_triples.append(triple)\n",
    "            total_triples += 1\n",
    "print(\"miss_hids = {:,}, triples with simnegs = {:,}, triples with complnegs = {:,}, triples with bm25negs = {:,}\".format(\n",
    "    len(miss_hids), len(hids_have_simnegs), len(hids_have_complnegs), len(hids_have_bm25negs)\n",
    "))\n",
    "print(\"total triples = {:,}\".format(total_triples))\n",
    "print(\"=\"*75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c50a10-dc3f-468f-9c09-61b1378e1049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique aids for simpids = 315,575, unique aids for complpids = 82,580\n",
      "query oriented sampling max2, unique queries = 54,237, unique items = 101,142\n",
      "query oriented sampling max5, unique queries = 54,310, unique items = 226,564\n",
      "query oriented sampling max10, unique queries = 54,331, unique items = 386,440\n",
      "query oriented sampling max15, unique queries = 54,338, unique items = 491,323\n"
     ]
    }
   ],
   "source": [
    "print(\"unique aids for simpids = {:,}, unique aids for complpids = {:,}\".format(len(max5_h2sp), len(max5_h2cp)))\n",
    "print(\"query oriented sampling max2, unique queries = {:,}, unique items = {:,}\".format(\n",
    "    len(set([q for (q, _, _) in max2_q2p_triples])), len(set([p for (_, p, _) in max2_q2p_triples]))))\n",
    "print(\"query oriented sampling max5, unique queries = {:,}, unique items = {:,}\".format(\n",
    "    len(set([q for (q, _, _) in max5_q2p_triples])), len(set([p for (_, p, _) in max5_q2p_triples]))))\n",
    "print(\"query oriented sampling max10, unique queries = {:,}, unique items = {:,}\".format(\n",
    "    len(set([q for (q, _, _) in max10_q2p_triples])), len(set([p for (_, p, _) in max10_q2p_triples]))))\n",
    "print(\"query oriented sampling max15, unique queries = {:,}, unique items = {:,}\".format(\n",
    "    len(set([q for (q, _, _) in max15_q2p_triples])), len(set([p for (_, p, _) in max15_q2p_triples]))))\n",
    "\n",
    "print(\"h2sp triples = {:,}, h2cp triples = {:,}\".format(len(h2sp_triples), len(h2cp_triples)))\n",
    "print(\"max2_q2p_triples = {:,}, max5_q2p _triples = {:,}, max10_q2p_triples = {:,}, max15_q2p_triples = {:,}\".format(\n",
    "    len(max2_q2p_triples), len(max5_q2p_triples), len(max10_q2p_triples), len(max15_q2p_triples)\n",
    "))total_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "513ddd43-551d-451f-b6f1-a8aef7685b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "out_dir = os.path.join(in_dir, \"unified_train/\")\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "\n",
    "with open(os.path.join(out_dir, \"train_graph.pkl\"), \"wb\") as fout:\n",
    "    pickle.dump(G, fout)\n",
    "\n",
    "fn_to_tripleNrel = {\n",
    "    \"a2sp.train.tsv\": (h2sp_triples, SIM_RELATION),\n",
    "    \"a2cp.train.tsv\": (h2cp_triples, COMPL_RELATION),\n",
    "    \"max2_qorient_q2p.train.tsv\": (max2_q2p_triples, REL_RELATION),\n",
    "    \"max5_qorient_q2p.train.tsv\": (max5_q2p_triples, REL_RELATION),\n",
    "    \"max10_qorient_q2p.train.tsv\": (max10_q2p_triples, REL_RELATION),\n",
    "    \"max15_qorient_q2p.train.tsv\": (max10_q2p_triples, REL_RELATION),\n",
    "    \n",
    "    \"a2sp.50.train.tsv\": (random.sample(h2sp_triples, k=int(0.5*len(h2sp_triples))), SIM_RELATION),\n",
    "    \"a2sp.25.train.tsv\": (random.sample(h2sp_triples, k=int(0.25*len(h2sp_triples))), SIM_RELATION)\n",
    "}\n",
    "\n",
    "for fn, (triples, relation) in fn_to_tripleNrel.items():\n",
    "    with open(os.path.join(out_dir, fn), \"w\") as fout:\n",
    "        for (hid, pos_tid, neg_tid) in triples:\n",
    "            fout.write(f\"{hid}\\t{pos_tid}\\t{neg_tid}\\t{relation}\\n\")\n",
    "            \n",
    "out_dir = os.path.join(in_dir, \"unified_test/\")\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "\n",
    "# for similar items\n",
    "fn_to_aids = {\n",
    "    \"anchors.train.sim.tsv\": list(train_aid_to_simpids.keys()),\n",
    "    \"anchors.val.sim.tsv\": list(val_aid_to_simpids.keys()),\n",
    "    \"anchors.test.sim.tsv\": list(test_aid_to_simpids.keys()),\n",
    "}\n",
    "for fn, aids in fn_to_aids.items():\n",
    "    with open(os.path.join(out_dir, fn), \"w\") as fout:\n",
    "        for aid in aids:\n",
    "            text = eid_to_text[aid]\n",
    "            fout.write(f\"{aid}\\t{text}\\t{SIM_RELATION}\\n\")\n",
    "            \n",
    "fn_to_arels = {\n",
    "    \"arels.train.sim.tsv\": [(aid, pid) for aid, simpids in train_aid_to_simpids.items() for pid in simpids],\n",
    "    \"arels.val.sim.tsv\": [(aid, pid) for aid, simpids in val_aid_to_simpids.items() for pid in simpids],\n",
    "    \"arels.test.sim.tsv\": [(aid, pid) for aid, simpids in test_aid_to_simpids.items() for pid in simpids],\n",
    "}\n",
    "for fn, arels in fn_to_arels.items():\n",
    "    with open(os.path.join(out_dir, fn), \"w\") as fout:\n",
    "        for (aid, pid) in arels:\n",
    "            fout.write(f\"{aid}\\tQ0\\t{pid}\\t{1}\\n\")\n",
    "            \n",
    "# for complementary items\n",
    "fn_to_aids = {\n",
    "    \"anchors.train.compl.tsv\": list(train_aid_to_complpids.keys()),\n",
    "    \"anchors.val.compl.tsv\": list(val_aid_to_complpids.keys()),\n",
    "    \"anchors.test.compl.tsv\": list(test_aid_to_complpids.keys()),\n",
    "}\n",
    "for fn, aids in fn_to_aids.items():\n",
    "    with open(os.path.join(out_dir, fn), \"w\") as fout:\n",
    "        for aid in aids:\n",
    "            text = eid_to_text[aid]\n",
    "            fout.write(f\"{aid}\\t{text}\\t{COMPL_RELATION}\\n\")\n",
    "fn_to_arels = {\n",
    "    \"arels.train.compl.tsv\": [(aid, pid) for aid, complpids in train_aid_to_complpids.items() for pid in complpids],\n",
    "    \"arels.val.compl.tsv\": [(aid, pid) for aid, complpids in val_aid_to_complpids.items() for pid in complpids],\n",
    "    \"arels.test.compl.tsv\": [(aid, pid) for aid, complpids in test_aid_to_complpids.items() for pid in complpids]\n",
    "}\n",
    "for fn, arels in fn_to_arels.items():\n",
    "    with open(os.path.join(out_dir, fn), \"w\") as fout:\n",
    "        for (aid, pid) in arels:\n",
    "            fout.write(f\"{aid}\\tQ0\\t{pid}\\t{1}\\n\")\n",
    "            \n",
    "# for queries\n",
    "fn_to_qids = {\n",
    "    \"queries.train.tsv\": list(train_qid_to_pids.keys()),\n",
    "    \"queries.val.tsv\": list(val_qid_to_pids.keys()),\n",
    "    \"queries.test.tsv\": list(test_qid_to_pids.keys()),\n",
    "}\n",
    "for fn, qids in fn_to_qids.items():\n",
    "    with open(os.path.join(out_dir, fn), \"w\") as fout:\n",
    "        for qid in qids:\n",
    "            text = eid_to_text[qid]\n",
    "            fout.write(f\"{qid}\\t{text}\\t{REL_RELATION}\\n\")\n",
    "            \n",
    "            \n",
    "    \n",
    "            \n",
    "fn_to_qrels = {\n",
    "    \"qrels.train.tsv\": [(qid, pid) for qid, pids in train_qid_to_pids.items() for pid in pids],\n",
    "    \"qrels.val.tsv\": [(qid, pid) for qid, pids in val_qid_to_pids.items() for pid in pids],\n",
    "    \"qrels.test.tsv\": [(qid, pid) for (qid, pids) in test_qid_to_pids.items() for pid in pids],\n",
    "}\n",
    "\n",
    "for fn, qrels in fn_to_qrels.items():\n",
    "    with open(os.path.join(out_dir, fn), \"w\") as fout:\n",
    "        for (qid, pid) in qrels:\n",
    "            fout.write(f\"{qid}\\tQ0\\t{pid}\\t{1}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f765171-1c19-4e20-8883-a3c03a0d18f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448988 /home/jupyter/unity_jointly_rec_and_search/datasets/amazon_esci_dataset/task_2_multiclass_product_classification/unified_train/max15_qorient_q2p.train.tsv\n",
      "1267636\t517032\t943546\tis_relevant_to\n",
      "1267636\t517040\t145583\tis_relevant_to\n",
      "1267636\t517038\t1131982\tis_relevant_to\n",
      "1281425\t695445\t1195271\tis_relevant_to\n",
      "1281425\t971808\t465588\tis_relevant_to\n",
      "1281425\t695443\t643557\tis_relevant_to\n",
      "====================================================================================================\n",
      "600567 /home/jupyter/unity_jointly_rec_and_search/datasets/amazon_esci_dataset/task_2_multiclass_product_classification/unified_train/max15_qorient_q2p.scbnegs.train.tsv\n",
      "1267636\t517038\t515292\tis_relevant_to\n",
      "1267636\t1093650\t517038\tis_relevant_to\n",
      "1267636\t517040\t1141802\tis_relevant_to\n",
      "1281425\t971808\t1195242\tis_relevant_to\n",
      "1281425\t695448\t775789\tis_relevant_to\n",
      "1281425\t953046\t775794\tis_relevant_to\n",
      "====================================================================================================\n",
      "533871 /home/jupyter/unity_jointly_rec_and_search/datasets/amazon_esci_dataset/task_2_multiclass_product_classification/unified_train/a2sp.50.train.tsv\n",
      "721795\t721774\t19743\tis_similar_to\n",
      "634415\t634416\t1040981\tis_similar_to\n",
      "208590\t1054152\t609814\tis_similar_to\n",
      "720910\t720903\t1025565\tis_similar_to\n",
      "433992\t433988\t510716\tis_similar_to\n",
      "25948\t25949\t162661\tis_similar_to\n",
      "====================================================================================================\n",
      "247472 /home/jupyter/unity_jointly_rec_and_search/datasets/amazon_esci_dataset/task_2_multiclass_product_classification/unified_train/max5_qorient_q2p.train.tsv\n",
      "1267636\t517041\t6416\tis_relevant_to\n",
      "1267636\t517038\t140588\tis_relevant_to\n",
      "1267636\t517032\t1154640\tis_relevant_to\n",
      "1281425\t973124\t971819\tis_relevant_to\n",
      "1281425\t695448\t680205\tis_relevant_to\n",
      "1281425\t695443\t393420\tis_relevant_to\n",
      "====================================================================================================\n",
      "1067742 /home/jupyter/unity_jointly_rec_and_search/datasets/amazon_esci_dataset/task_2_multiclass_product_classification/unified_train/a2sp.train.tsv\n",
      "881159\t19\t583329\tis_similar_to\n",
      "881159\t953826\t1073032\tis_similar_to\n",
      "881159\t881221\t825710\tis_similar_to\n",
      "766742\t766752\t631407\tis_similar_to\n",
      "766742\t786789\t122934\tis_similar_to\n",
      "766742\t766747\t1209879\tis_similar_to\n",
      "====================================================================================================\n",
      "266935 /home/jupyter/unity_jointly_rec_and_search/datasets/amazon_esci_dataset/task_2_multiclass_product_classification/unified_train/a2sp.25.train.tsv\n",
      "545716\t545719\t851472\tis_similar_to\n",
      "518272\t1203065\t1187239\tis_similar_to\n",
      "584016\t584019\t228534\tis_similar_to\n",
      "1148451\t1148446\t382674\tis_similar_to\n",
      "9211\t9217\t563847\tis_similar_to\n",
      "566886\t566890\t307990\tis_similar_to\n",
      "====================================================================================================\n",
      "184362 /home/jupyter/unity_jointly_rec_and_search/datasets/amazon_esci_dataset/task_2_multiclass_product_classification/unified_train/a2cp.train.tsv\n",
      "881159\t1033049\t907793\tis_complementary_to\n",
      "881159\t953818\t1146657\tis_complementary_to\n",
      "881159\t1032053\t873269\tis_complementary_to\n",
      "766746\t766744\t177583\tis_complementary_to\n",
      "766746\t766741\t98958\tis_complementary_to\n",
      "766746\t766726\t284294\tis_complementary_to\n",
      "====================================================================================================\n",
      "105906 /home/jupyter/unity_jointly_rec_and_search/datasets/amazon_esci_dataset/task_2_multiclass_product_classification/unified_train/max2_qorient_q2p.scbnegs.train.tsv\n",
      "1267636\t517030\t515433\tis_relevant_to\n",
      "1267636\t517039\t428761\tis_relevant_to\n",
      "1228232\t116172\t116161\tis_relevant_to\n",
      "1224704\t82036\t5376\tis_relevant_to\n",
      "1281425\t1195273\t187194\tis_relevant_to\n",
      "1281425\t695448\t762340\tis_relevant_to\n",
      "====================================================================================================\n",
      "105047 /home/jupyter/unity_jointly_rec_and_search/datasets/amazon_esci_dataset/task_2_multiclass_product_classification/unified_train/max2_qorient_q2p.train.tsv\n",
      "1267636\t517039\t144646\tis_relevant_to\n",
      "1267636\t517035\t517031\tis_relevant_to\n",
      "1228232\t116160\t530761\tis_relevant_to\n",
      "1224704\t968044\t690092\tis_relevant_to\n",
      "1281425\t695443\t692297\tis_relevant_to\n",
      "1281425\t953046\t762326\tis_relevant_to\n",
      "====================================================================================================\n",
      "448988 /home/jupyter/unity_jointly_rec_and_search/datasets/amazon_esci_dataset/task_2_multiclass_product_classification/unified_train/max10_qorient_q2p.train.tsv\n",
      "1267636\t517032\t943546\tis_relevant_to\n",
      "1267636\t517040\t145583\tis_relevant_to\n",
      "1267636\t517038\t1131982\tis_relevant_to\n",
      "1281425\t695445\t1195271\tis_relevant_to\n",
      "1281425\t971808\t465588\tis_relevant_to\n",
      "1281425\t695443\t643557\tis_relevant_to\n",
      "====================================================================================================\n",
      "249381 /home/jupyter/unity_jointly_rec_and_search/datasets/amazon_esci_dataset/task_2_multiclass_product_classification/unified_train/max5_qorient_q2p.scbnegs.train.tsv\n",
      "1267636\t517033\t453383\tis_relevant_to\n",
      "1267636\t892198\t515292\tis_relevant_to\n",
      "1267636\t517035\t758065\tis_relevant_to\n",
      "1281425\t953046\t1195259\tis_relevant_to\n",
      "1281425\t695443\t1195250\tis_relevant_to\n",
      "1281425\t971808\t762336\tis_relevant_to\n",
      "====================================================================================================\n",
      "452082 /home/jupyter/unity_jointly_rec_and_search/datasets/amazon_esci_dataset/task_2_multiclass_product_classification/unified_train/max10_qorient_q2p.scbnegs.train.tsv\n",
      "1267636\t517034\t892201\tis_relevant_to\n",
      "1267636\t517035\t943534\tis_relevant_to\n",
      "1267636\t517036\t224234\tis_relevant_to\n",
      "1281425\t973124\t762341\tis_relevant_to\n",
      "1281425\t695445\t973116\tis_relevant_to\n",
      "1281425\t971808\t187235\tis_relevant_to\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "import os\n",
    "in_dir=\"/home/jupyter/unity_jointly_rec_and_search/datasets/amazon_esci_dataset/task_2_multiclass_product_classification/\"\n",
    "\n",
    "eid_to_text = {}\n",
    "with open(os.path.join(in_dir, \"all_entities.tsv\")) as fin:\n",
    "    for line in fin:\n",
    "        eid, text = line.strip().split(\"\\t\")\n",
    "        eid_to_text[int(eid)] = text\n",
    "        \n",
    "out_dir = os.path.join(in_dir, \"unified_train/\")\n",
    "for path in os.listdir(out_dir):\n",
    "    path = os.path.join(out_dir, path)\n",
    "    if path.endswith(\"tsv\"):\n",
    "        ! wc -l $path\n",
    "        ! head -n 3 $path\n",
    "        ! tail -n 3 $path\n",
    "        print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ece00e61-565c-4afc-a69b-6ee77029422c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIC Xtra Fun Cased Pencil, 2 Lead, Assorted Barrel Colors, 48-Count\n",
      "Wood-Cased #2 HB Pencils, Yellow, Pre-sharpened, Class Pack, 1000 pencils\n",
      "Meet Perfect Office Chair with Wheels Cheap Desk Chair Swivel Task Chairs Mid Back No Arms Ergonomic Mesh Chair Small Computer Chair Modern Height Adjustable for Home Space-Saving, Black\n"
     ]
    }
   ],
   "source": [
    "hid, pos_tid, neg_tid = (14 ,953828,973116)\n",
    "print(eid_to_text[hid])\n",
    "print(eid_to_text[pos_tid])\n",
    "print(eid_to_text[neg_tid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad2f26e-dd21-40de-be2a-3dbef8ea321f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-9.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
