{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b282b7c0-ca4d-4f3e-8288-6c7050d41037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import gzip \n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import ujson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05ab5a91-311a-4e10-8f7a-9349f17a9317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_char(value):\n",
    "    l_temp = []\n",
    "    \n",
    "    value = re.sub(\"</?span[^>]*>\", \"\", str(value)) # remove special tag token </span>\n",
    "    value = re.sub(\n",
    "        '[\\[\\],!.;#$^*\\_——<>/=%&?@\"&\\'-:]', ' ', str(value))\n",
    "    l_temp = [i for i in value.split()]\n",
    "    return l_temp\n",
    "\n",
    "\n",
    "def _remove_dup(l):\n",
    "    \"\"\" Remove duplicated words, first remove front ones. \"\"\"\n",
    "    l_temp = []\n",
    "\n",
    "    i = len(l) - 1\n",
    "    while i >= 0:\n",
    "        l[i] = l[i].lower()\n",
    "        if l[i] not in l_temp:\n",
    "            l_temp.append(l[i])\n",
    "        i = i - 1\n",
    "\n",
    "    l_temp.reverse()\n",
    "    return l_temp\n",
    "\n",
    "def get_df(path, keep_columns=None):\n",
    "    \"\"\" Apply raw data to pandas DataFrame. \"\"\"\n",
    "    i = 0\n",
    "    df = {}\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for line in g:\n",
    "        df[i] = ujson.loads(line)\n",
    "        i += 1\n",
    "    if keep_columns != None:\n",
    "        assert type(keep_columns) == list\n",
    "        return pd.DataFrame.from_dict(df, orient='index')[keep_columns]\n",
    "    else:\n",
    "        return pd.DataFrame.from_dict(df, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a75fa36-b7c3-4e6d-b59b-8e969f8a0ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27030/27030 [00:00<00:00, 783811.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of aids has viewpids, buypids = 30,029, 27,030\n",
      "viewpids, buypids, only_buypids' average lengths are = 5.063, 4.727, 4.227\n"
     ]
    }
   ],
   "source": [
    "in_dir = \"/home/jupyter/unity_jointly_rec_and_search/datasets/amazon_review_datasets/\"\n",
    "dataset_name = \"Cell_Phones_and_Accessories\"\n",
    "meta_path = os.path.join(in_dir, f\"meta_{dataset_name}.json.gz\")\n",
    "review_path = os.path.join(in_dir, f\"{dataset_name}.json.gz\")\n",
    "\n",
    "# preprocess meta_df\n",
    "meta_df = get_df(meta_path)\n",
    "\n",
    "meta_df.fillna('unknown', inplace=True)\n",
    "replace_pattern = re.escape(\";+/\\.?\")\n",
    "replace_pattern = f\"[{replace_pattern}\\n]+\"\n",
    "meta_df['title'] = meta_df['title'] \\\n",
    "    .str.replace(\"<\\w+>\", \" \", regex=True) \\\n",
    "    .str.replace(\"</\\w+>\", \" \", regex=True) \\\n",
    "    .str.strip() \\\n",
    "    .str.replace(\"\\t\", \" \") \\\n",
    "    .str.replace(\"\\n\", \" \") \n",
    "meta_df['description'] = meta_df['description'].apply(lambda x: \" \".join(x))\n",
    "meta_df['description'] = meta_df['description'] \\\n",
    "    .str.replace(\"<\\w+>\", \" \", regex=True) \\\n",
    "    .str.replace(\"</\\w+>\", \" \", regex=True) \\\n",
    "    .str.strip() \\\n",
    "    .str.replace(\"\\t\", \" \") \\\n",
    "    .str.replace(\"\\n\", \" \") \n",
    "\n",
    "meta_df = meta_df.drop_duplicates(subset=\"asin\")\n",
    "meta_df = meta_df[meta_df[\"category\"].apply(lambda x: len(x) > 1)]\n",
    "\n",
    "asin_to_pid = {asin: pid for pid, asin in enumerate(meta_df[\"asin\"].unique())}\n",
    "asin_to_categories = {asin: categories for asin, categories in zip(meta_df.asin, meta_df.category)}\n",
    "valid_asins = set(asin_to_categories.keys())\n",
    "asin_to_query = {}\n",
    "for asin, categories in asin_to_categories.items():\n",
    "    qs = map(_remove_char, categories)\n",
    "    qs = [x for q in qs for x in q]\n",
    "    query = \" \".join(_remove_dup(qs))\n",
    "    asin_to_query[asin] = query\n",
    "\n",
    "pid_to_title = {}\n",
    "pid_to_description = {}\n",
    "aid_to_viewpids = {}\n",
    "aid_to_buypids = {}\n",
    "for idx, row in meta_df.iterrows():\n",
    "    pid_to_title[asin_to_pid[row.asin]] = row.title\n",
    "    pid_to_description[asin_to_pid[row.asin]] = row.description\n",
    "    \n",
    "    valid_view = valid_asins.intersection(set(row.also_view))\n",
    "    if len(valid_view) > 0:\n",
    "        aid_to_viewpids[asin_to_pid[row.asin]] = set(asin_to_pid[asin] for asin in list(valid_view))\n",
    "        \n",
    "    valid_buy = valid_asins.intersection(set(row.also_buy))\n",
    "    if len(valid_buy) > 0:\n",
    "        aid_to_buypids[asin_to_pid[row.asin]] = set(asin_to_pid[asin] for asin in list(valid_buy))\n",
    "        \n",
    "assert len(pid_to_title) == len(pid_to_description) == len(meta_df)\n",
    "\n",
    "aid_to_only_buypids = {}\n",
    "for aid, buypids in tqdm(aid_to_buypids.items(), total=len(aid_to_buypids)):\n",
    "    if aid in aid_to_viewpids:\n",
    "        only_buypids = buypids.difference(aid_to_viewpids[aid])\n",
    "        aid_to_only_buypids[aid] = only_buypids\n",
    "    else:\n",
    "        aid_to_only_buypids[aid] = buypids\n",
    "\n",
    "print(\"number of aids has viewpids, buypids = {:,}, {:,}\".format(len(aid_to_viewpids), len(aid_to_buypids)))\n",
    "print(\"viewpids, buypids, only_buypids' average lengths are = {:.3f}, {:.3f}, {:.3f}\".format(\n",
    "    np.mean([len(xs) for xs in aid_to_viewpids.values()]), np.mean([len(xs) for xs in aid_to_buypids.values()]),\n",
    "    np.mean([len(xs) for xs in aid_to_only_buypids.values()])\n",
    "))\n",
    "\n",
    "# preprocess review_df\n",
    "review_df = get_df(review_path, keep_columns=[\"reviewerID\", \"asin\", \"unixReviewTime\", \"vote\", \"reviewText\"])\n",
    "review_df = review_df[review_df[\"asin\"].isin(valid_asins)]\n",
    "review_df[\"query\"] = review_df[\"asin\"].apply(lambda x: asin_to_query[x])\n",
    "review_df = review_df[review_df.groupby('query')['query'].transform('size') >= 3] \n",
    "\n",
    "review_df[\"pid\"] = review_df[\"asin\"].apply(lambda x: asin_to_pid[x])\n",
    "review_df[\"vote\"] = review_df[\"vote\"].fillna(value=\"0\")\n",
    "review_df[\"vote\"] = review_df[\"vote\"].apply(lambda x: int(x.replace(',','')))\n",
    "query_to_qid = {query: qid + len(asin_to_pid) for qid, query in enumerate(review_df[\"query\"].unique())}\n",
    "review_df[\"qid\"] = review_df[\"query\"].apply(lambda x: query_to_qid[x])\n",
    "\n",
    "user_review_df = review_df[review_df.groupby('reviewerID')['reviewerID'].transform('size') >= 5] \\\n",
    "                .reset_index(drop=True).sort_values(by=[\"reviewerID\", \"unixReviewTime\"])\n",
    "user_to_uid = {user:uid for uid, user in enumerate(user_review_df.reviewerID.unique())}\n",
    "user_review_df[\"uid\"] = user_review_df[\"reviewerID\"].apply(lambda x: user_to_uid[x])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "371cb537-91d9-476d-8ccc-36f7016f6663",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 215338/215338 [00:46<00:00, 4597.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train_user_review, test_user_review = 1,368,611, 215,338\n",
      "number of unique users = 215,338\n",
      "total number of pids = 533,871, number of pid has review = 217,162\n",
      "number of interacted (query, item) pairs in review_df = 532,969, in user_review_df = 239,464\n"
     ]
    }
   ],
   "source": [
    "train_user_review_df = []\n",
    "test_user_review_df = []\n",
    "for i, group in tqdm(user_review_df.groupby(\"uid\")):\n",
    "    assert len(group) >= 5\n",
    "    train_user_review_df.append(group.iloc[:-1])\n",
    "    test_user_review_df.append(group.iloc[[-1]])\n",
    "\n",
    "train_user_review_df = pd.concat(train_user_review_df).reset_index(drop=True)\n",
    "test_user_review_df = pd.concat(test_user_review_df).reset_index(drop=True)\n",
    "assert len(train_user_review_df) + len(test_user_review_df) == len(user_review_df)\n",
    "assert len(train_user_review_df.uid.unique()) == len(test_user_review_df.uid.unique())\n",
    "\n",
    "pid_to_review = {}\n",
    "for pid, group in train_user_review_df.groupby(\"pid\"):\n",
    "    assert pid == group.pid.iloc[0]\n",
    "    if all(group.vote == 0):\n",
    "        review = np.random.choice(group.reviewText, size=1)[0]\n",
    "    else:\n",
    "        review = group.reviewText.iloc[np.argmax(group.vote)]\n",
    "    pid_to_review[pid] = review\n",
    "\n",
    "print(\"number of train_user_review, test_user_review = {:,}, {:,}\".format(len(train_user_review_df), len(test_user_review_df)))\n",
    "print(\"number of unique users = {:,}\".format(len(user_review_df.uid.unique())))\n",
    "print(\"total number of pids = {:,}, number of pid has review = {:,}\".format(len(pid_to_title), len(pid_to_review)))\n",
    "print(\"number of interacted (query, item) pairs in review_df = {:,}, in user_review_df = {:,}\".format(\n",
    "    len(review_df.pid.unique()), len(user_review_df.pid.unique())\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f35b8cc-36a4-403b-af6d-c2bfec373b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30029/30029 [00:05<00:00, 5839.56it/s]\n",
      "100%|██████████| 27030/27030 [00:03<00:00, 7168.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique qids, pids for search, average length = 593, 524,434, 718.287\n",
      "unique aids, pids for sim_rec, average length = 24,024, 19,252, 5.063\n",
      "unique aids, pids for compl_rec, average length = 21,624, 14,195, 4.227\n",
      "relpids and simpids intersect rate = 0.062\n",
      "relpids and complpids intersect rate = 0.050\n",
      "simpids and complpids intersect rate = 0.347\n",
      "total number of gt pairs for train_qid_to_relpids and excluded_qid_to_relpids = 524,434, 446,338\n",
      "unique qids for for train_qid_to_relpids and excluded_qid_to_relpids = 593, 593\n"
     ]
    }
   ],
   "source": [
    "# for unified_kgc train and test\n",
    "from collections import defaultdict\n",
    "import random\n",
    "random.seed(4680)\n",
    "\n",
    "aid_to_simpids = aid_to_viewpids\n",
    "aid_to_complpids = aid_to_only_buypids\n",
    "\n",
    "# simpids\n",
    "val_test_aids = random.sample(aid_to_simpids.keys(), int(0.2*len(aid_to_simpids)))\n",
    "val_aids = val_test_aids[:int(0.5*len(val_test_aids))]\n",
    "test_aids = val_test_aids[int(0.5*len(val_test_aids)):]\n",
    "train_aid_to_simpids, val_aid_to_simpids, test_aid_to_simpids = {}, {}, {}\n",
    "for aid, simpids in tqdm(aid_to_simpids.items(), total=len(aid_to_simpids)):\n",
    "    if aid in val_aids:\n",
    "        val_aid_to_simpids[aid] = simpids\n",
    "    elif aid in test_aids:\n",
    "        test_aid_to_simpids[aid] = simpids\n",
    "    else:\n",
    "        train_aid_to_simpids[aid] = simpids\n",
    "        \n",
    "# complpids\n",
    "val_test_aids = random.sample(aid_to_complpids.keys(), int(0.2*len(aid_to_complpids)))\n",
    "val_aids = val_test_aids[:int(0.5*len(val_test_aids))]\n",
    "test_aids = val_test_aids[int(0.5*len(val_test_aids)):]\n",
    "train_aid_to_complpids, val_aid_to_complpids, test_aid_to_complpids = {}, {}, {}\n",
    "for aid, complpids in tqdm(aid_to_complpids.items(), total=len(aid_to_complpids)):\n",
    "    if aid in val_aids:\n",
    "        val_aid_to_complpids[aid] = complpids\n",
    "    elif aid in test_aids:\n",
    "        test_aid_to_complpids[aid] = complpids\n",
    "    else:\n",
    "        train_aid_to_complpids[aid] = complpids\n",
    "        \n",
    "# relpids\n",
    "qid_to_relpids = defaultdict(set)\n",
    "for qid, pid in zip(review_df.qid, review_df.pid):\n",
    "    qid_to_relpids[qid].add(pid)\n",
    "    \n",
    "qid_pids_pairs = list(qid_to_relpids.items())\n",
    "random.shuffle(qid_pids_pairs)\n",
    "train_qid_to_relpids = {qid: pids for qid, pids in qid_pids_pairs[:int(0.8*len(qid_pids_pairs))]}\n",
    "val_qid_to_relpids = {qid: pids for qid, pids in qid_pids_pairs[int(0.8*len(qid_pids_pairs)): int(0.9*len(qid_pids_pairs))]}\n",
    "test_qid_to_relpids = {qid: pids for qid, pids in qid_pids_pairs[int(0.9*len(qid_pids_pairs)):]}\n",
    "\n",
    "# check\n",
    "assert len( set(train_aid_to_simpids.keys()) & set(val_aid_to_simpids.keys()) & set(test_aid_to_simpids.keys()) ) == 0\n",
    "assert len( set(train_aid_to_complpids.keys()) & set(val_aid_to_complpids.keys()) & set(test_aid_to_complpids.keys())) == 0\n",
    "assert len( set(train_qid_to_relpids.keys()) & set(val_qid_to_relpids.keys()) & set(test_qid_to_relpids.keys())) == 0\n",
    "\n",
    "\n",
    "print(\"unique qids, pids for search, average length = {:,}, {:,}, {:.3f}\".format(\n",
    "    len(train_qid_to_relpids), \n",
    "    len(set([x for xs in train_qid_to_relpids.values() for x in xs])),\n",
    "    np.mean([len(xs) for xs in qid_to_relpids.values()])))\n",
    "print(\"unique aids, pids for sim_rec, average length = {:,}, {:,}, {:.3f}\".format(\n",
    "    len(train_aid_to_simpids), \n",
    "    len(set([x for xs in aid_to_simpids.values() for x in xs])),\n",
    "    np.mean([len(xs) for xs in aid_to_simpids.values()])))\n",
    "print(\"unique aids, pids for compl_rec, average length = {:,}, {:,}, {:.3f}\".format(\n",
    "    len(train_aid_to_complpids), \n",
    "    len(set([x for xs in aid_to_complpids.values() for x in xs])),\n",
    "    np.mean([len(xs) for xs in aid_to_complpids.values()])))\n",
    "\n",
    "unique_relpids = set([x for xs in train_qid_to_relpids.values() for x in xs])\n",
    "unique_simpids = set([x for xs in aid_to_simpids.values() for x in xs]).union(set(train_aid_to_simpids.keys()))\n",
    "unique_complpids = set([x for xs in aid_to_complpids.values() for x in xs]).union(set(train_aid_to_complpids.keys()))\n",
    "print(\"relpids and simpids intersect rate = {:.3f}\".format(\n",
    "    len(unique_relpids.intersection(unique_simpids)) / len(unique_relpids.union(unique_simpids))\n",
    "))\n",
    "print(\"relpids and complpids intersect rate = {:.3f}\".format(\n",
    "    len(unique_relpids.intersection(unique_complpids)) / len(unique_relpids.union(unique_complpids))\n",
    "))\n",
    "print(\"simpids and complpids intersect rate = {:.3f}\".format(\n",
    "   len( unique_simpids.intersection(unique_complpids)) / len(unique_simpids.union(unique_complpids))\n",
    "))\n",
    "\n",
    "# for user train and test\n",
    "test_user_qid_to_relpids = defaultdict(set)\n",
    "excluded_train_qid_to_relpids = {}\n",
    "for i, row in test_user_review_df.iterrows():\n",
    "    qid, pid = int(row.qid), int(row.pid)\n",
    "    test_user_qid_to_relpids[qid].add(pid)\n",
    "for qid, relpids in train_qid_to_relpids.items():\n",
    "    if qid in test_user_qid_to_relpids:\n",
    "        excluded_train_qid_to_relpids[qid] = relpids.difference(test_user_qid_to_relpids[qid])\n",
    "    else:\n",
    "        excluded_train_qid_to_relpids[qid] = relpids\n",
    "        \n",
    "print(\"total number of gt pairs for train_qid_to_relpids and excluded_qid_to_relpids = {:,}, {:,}\".format(\n",
    "    sum([len(xs) for xs in train_qid_to_relpids.values()]), sum([len(xs) for xs in excluded_train_qid_to_relpids.values()])\n",
    "))\n",
    "print(\"unique qids for for train_qid_to_relpids and excluded_qid_to_relpids = {:,}, {:,}\".format(\n",
    "    len(train_qid_to_relpids.keys()), len(excluded_train_qid_to_relpids.keys())\n",
    "))\n",
    "train_qid_to_relpids = excluded_train_qid_to_relpids\n",
    "excluded_train_qid_to_relpids = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1760dc57-3c4e-4e8b-908d-6f7246287b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to disk\n",
    "import os \n",
    "import copy\n",
    "import pickle as pkl\n",
    "\n",
    "import re\n",
    "space_pattern = re.compile(r'\\s+')\n",
    "\n",
    "pid_to_title = {pid: title.strip() for pid, title in pid_to_title.items()}\n",
    "query_to_qid = {query.strip(): qid for query, qid in query_to_qid.items()}\n",
    "clean_pid_to_review = {}\n",
    "for pid, review in pid_to_review.items():\n",
    "    if type(review) == float:\n",
    "        continue\n",
    "    clean_pid_to_review[pid] = re.sub(space_pattern, \" \", review)\n",
    "    clean_pid_to_review[pid] = re.sub('<[^<]+?>', \" \", clean_pid_to_review[pid])[:1000]\n",
    "pid_to_review = clean_pid_to_review\n",
    "clean_pid_to_review = None\n",
    "\n",
    "def add_query_prefix(text):\n",
    "    return \"query: \" + text\n",
    "\n",
    "def add_product_prefix(text):\n",
    "    return \"product: \" + text\n",
    "\n",
    "out_dir = os.path.join(in_dir, dataset_name)\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "    \n",
    "with open(os.path.join(out_dir, \"collection_title.tsv\"), \"w\") as fout:\n",
    "    for pid, title in pid_to_title.items():\n",
    "        title = add_product_prefix(title)\n",
    "        fout.write(f\"{pid}\\t{title}\\n\")\n",
    "\n",
    "with open(os.path.join(out_dir, \"colletion_title_review.tsv\"), \"w\") as fout:\n",
    "    for pid, title in pid_to_title.items():\n",
    "        text = title\n",
    "        if pid in pid_to_review:\n",
    "            review = pid_to_review[pid]\n",
    "            text = text + \" ; \" + review \n",
    "        text = add_product_prefix(text)\n",
    "        fout.write(f\"{pid}\\t{text}\\n\")\n",
    "        \n",
    "        \n",
    "with open(os.path.join(out_dir, \"product.jsonl\"), \"w\") as fout:\n",
    "        for pid, title in pid_to_title.items():\n",
    "            text = title\n",
    "            if pid in pid_to_review:\n",
    "                review = pid_to_review[pid]\n",
    "                text = text + \" ; \" + review\n",
    "            text = add_product_prefix(text)\n",
    "            example = {\"id\": pid, \"contents\": text}\n",
    "            fout.write(ujson.dumps(example) + \"\\n\")\n",
    "            \n",
    "with open(os.path.join(out_dir, \"all_queries.tsv\"), \"w\") as fout:\n",
    "    for query, qid in query_to_qid.items():\n",
    "        query = add_query_prefix(query)\n",
    "        fout.write(f\"{qid}\\t{query}\\n\")\n",
    "            \n",
    "with open(os.path.join(out_dir, \"all_entities.tsv\"), \"w\") as fout:\n",
    "    for pid, title in pid_to_title.items():\n",
    "        text = title\n",
    "        if pid in pid_to_review:\n",
    "            review = pid_to_review[pid]\n",
    "            text = text + \" ; \" + review\n",
    "        text = add_product_prefix(text)\n",
    "        fout.write(f\"{pid}\\t{text}\\n\")\n",
    "    for query, qid in query_to_qid.items():\n",
    "        query = add_query_prefix(query)\n",
    "        fout.write(f\"{qid}\\t{query}\\n\")\n",
    "        \n",
    "with open(os.path.join(out_dir, \"asin_to_pid.pkl\"), \"wb\") as fout:\n",
    "    pkl.dump(asin_to_pid, fout)\n",
    "\n",
    "with open(os.path.join(out_dir, \"query_to_qid.pkl\"), \"wb\") as fout:\n",
    "    pkl.dump(query_to_qid, fout)\n",
    "    \n",
    "fn_to_data = {\n",
    "    \"train_aid_to_simpids.pkl\": train_aid_to_simpids,\n",
    "    \"val_aid_to_simpids.pkl\": val_aid_to_simpids,\n",
    "    \"test_aid_to_simpids.pkl\": test_aid_to_simpids,\n",
    "    \n",
    "    \"train_aid_to_complpids.pkl\": train_aid_to_complpids,\n",
    "    \"val_aid_to_complpids.pkl\": val_aid_to_complpids,\n",
    "    \"test_aid_to_complpids.pkl\": test_aid_to_complpids,\n",
    "    \n",
    "    \"train_qid_to_relpids.pkl\": train_qid_to_relpids,\n",
    "    \"val_qid_to_relpids.pkl\": val_qid_to_relpids,\n",
    "    \"test_qid_to_relpids.pkl\": test_qid_to_relpids,\n",
    "    \n",
    "    \"train_user_review_df.pkl\": train_user_review_df,\n",
    "    \"test_user_review_df.pkl\": test_user_review_df,\n",
    "}\n",
    "\n",
    "for fn, data in fn_to_data.items():\n",
    "    fn = os.path.join(out_dir, fn)\n",
    "    with open(fn, \"wb\") as fout:\n",
    "        pkl.dump(data, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9708091d-8b55-43d2-9b84-a542db0bba9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fn in os.listdir(out_dir):\n",
    "    if not fn.endswith(\"tsv\"):\n",
    "        continue\n",
    "    \n",
    "    fn = os.path.join(out_dir, fn)\n",
    "    ! wc -l $fn\n",
    "    ! head -n 3 $fn\n",
    "    ! tail -n 3 $fn \n",
    "    print(\"=\"*75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aaf732-c2bb-4c03-9665-95f3220c00ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pid_to_review[16741]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c112f5d-f7e9-4ad9-baf9-ed9dcfc4b267",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-9.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
