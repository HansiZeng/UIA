{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of search train and test = 1,368,611, 215,338\n",
      "number of entites = 534,613\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pickle as pkl\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "in_dir = \"/home/jupyter/unity_jointly_rec_and_search/datasets/amazon_review_datasets/\"\n",
    "dataset_name = \"Cell_Phones_and_Accessories\"\n",
    "in_dir = os.path.join(in_dir, dataset_name)\n",
    "    \n",
    "with open(os.path.join(in_dir, \"train_user_review_df.pkl\"), \"rb\") as fin:\n",
    "    train_search_data = pkl.load(fin)\n",
    "\n",
    "with open(os.path.join(in_dir, \"test_user_review_df.pkl\"), \"rb\") as fin:\n",
    "    test_search_data = pkl.load(fin)\n",
    "\n",
    "eid_to_text = {}\n",
    "with open(os.path.join(in_dir, \"all_entities.tsv\")) as fin:\n",
    "    for line in fin:\n",
    "        #print(line)\n",
    "        eid, text = line.strip().split(\"\\t\")\n",
    "        eid_to_text[int(eid)] = text\n",
    "\n",
    "print(\"length of search train and test = {:,}, {:,}\".format(len(train_search_data), len(test_search_data)))\n",
    "print(\"number of entites = {:,}\".format(len(eid_to_text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "search_sequential: 100%|██████████| 215338/215338 [03:23<00:00, 1056.17it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "import ujson \n",
    "\n",
    "out_dir = os.path.join(in_dir, \"sequential_train_test/\")\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir) \n",
    "\n",
    "seq_examples_list = []\n",
    "prefixes_to_datas= {\n",
    "    os.path.join(out_dir, \"search_sequential\"): (train_search_data, test_search_data, \"urels.search.test.tsv\"),\n",
    "}\n",
    "\n",
    "for prefix, (train_data, test_data, urel_path) in prefixes_to_datas.items():\n",
    "    train_seq_examples = []\n",
    "    test_seq_examples = []\n",
    "    test_uid_to_pospids = {}\n",
    "    for uid, group in tqdm(train_data.groupby(\"uid\"), desc=prefix.split(\"/\")[-1]):\n",
    "        if \"search_sequential\" in prefix:\n",
    "            qids = [int(x) for x in list(group.qid)]\n",
    "            rel_pids = [int(x) for x in list(group.pid)]\n",
    "        else:\n",
    "            raise ValueError(f\"{prefix} not valid.\")\n",
    "        \n",
    "        assert len(qids) == len(rel_pids) == len(group)\n",
    "\n",
    "        uid = int(uid)\n",
    "\n",
    "        query_ids = qids[1:]\n",
    "        context_key_ids = qids[:-1]\n",
    "        context_value_ids = rel_pids[:-1]\n",
    "        target_value_ids = rel_pids[1:]\n",
    "        assert len(query_ids) == len(context_key_ids) == len(context_value_ids) == len(target_value_ids)\n",
    "\n",
    "        example = {\"uid\": uid, \"query_ids\": query_ids, \"context_key_ids\": context_key_ids, \"context_value_ids\": context_value_ids,\n",
    "                    \"target_value_ids\": target_value_ids}\n",
    "        train_seq_examples.append(example)\n",
    "\n",
    "        # for test\n",
    "        test_row = test_data[test_data.uid == uid]\n",
    "        if len(test_row) == 0:\n",
    "            continue\n",
    "        assert len(test_row) == 1, test_row\n",
    "        \n",
    "        if \"search_sequential\" in prefix:\n",
    "            test_qid = int(test_row.iloc[0].qid)\n",
    "        else:\n",
    "            raise ValueError(f\"{prefix} not valid.\") \n",
    "\n",
    "        test_query_ids = qids[1:] + [test_qid]\n",
    "        test_context_key_ids = qids \n",
    "        test_context_value_ids = rel_pids\n",
    "        assert len(test_query_ids) == len(test_context_key_ids) == len(test_context_value_ids), (len(test_query_ids), \n",
    "                                                                                len(test_context_key_ids), len(test_context_value_ids))\n",
    "\n",
    "        example = {\"uid\": uid, \"query_ids\": test_query_ids, \"context_key_ids\": test_context_key_ids, \"context_value_ids\": test_context_value_ids}\n",
    "        test_seq_examples.append(example)\n",
    "\n",
    "        if \"search_sequential\" in prefix:\n",
    "            test_uid_to_pospids[uid] = int(test_row.iloc[0].pid)\n",
    "        else:\n",
    "            raise ValueError(f\"{prefix} not valid.\")\n",
    "        \n",
    "    \n",
    "    with open(prefix + \".train.json\", \"w\") as fout:\n",
    "        for line in train_seq_examples:\n",
    "            fout.write(ujson.dumps(line) + \"\\n\")\n",
    "    with open(prefix + \".test.json\", \"w\") as fout:\n",
    "        for line in test_seq_examples:\n",
    "            fout.write(ujson.dumps(line) + \"\\n\")\n",
    "    with open(os.path.join(out_dir, urel_path), \"w\") as fout:\n",
    "        for uid, pos_pid in test_uid_to_pospids.items():\n",
    "            fout.write(f\"{uid}\\tQ0\\t{pos_pid}\\t{1}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max pids = 533870\n",
      "number of ignore hids = 0\n"
     ]
    }
   ],
   "source": [
    "PIDS = []\n",
    "with open(os.path.join(in_dir, \"collection_title.tsv\")) as fin:\n",
    "    for line in fin:\n",
    "        PIDS.append(int(line.strip().split(\"\\t\")[0]))\n",
    "print(f\"max pids = {max(PIDS)}\")\n",
    "\n",
    "def create_neg_value_ids(query_ids, pos_value_ids, miss_qids, sampler=None):\n",
    "    assert type(sampler) == dict\n",
    "    assert len(query_ids) == len(pos_value_ids)\n",
    "    neg_value_ids = []\n",
    "    for qid, pos_vid in zip(query_ids, pos_value_ids):\n",
    "        if qid not in sampler:\n",
    "            miss_qids.add(qid)\n",
    "            neg_vid = random.sample(range(len(PIDS)), k=1)[0]\n",
    "            while neg_vid == pos_vid:\n",
    "                neg_vid = random.sample(range(len(PIDS)), k=1)[0]\n",
    "            neg_value_ids.append(neg_vid)\n",
    "        else:\n",
    "            neg_vid = random.sample(sampler[qid], k=1)[0]\n",
    "            while neg_vid == pos_vid:\n",
    "                neg_vid = random.sample(range(len(PIDS)), k=1)[0]\n",
    "            neg_value_ids.append(neg_vid)\n",
    "    \n",
    "    assert len(neg_value_ids) == len(pos_value_ids)\n",
    "    \n",
    "    return neg_value_ids\n",
    "\n",
    "run_path = os.path.join(in_dir, \"runs/bm25.all.run\")\n",
    "df = pd.read_csv(run_path, sep=\" \", names=[\"hid\", \"q0\", \"tid\", \"rank\", \"score\", \"model_name\"])\n",
    "bm25_hid_to_tids = {}\n",
    "ignore_hids = set()\n",
    "for hid, group in df.groupby(\"hid\"):\n",
    "    cand_tids = list(group.tid.values)\n",
    "    if len(cand_tids) < 10:\n",
    "        ignore_hids.add(int(hid))\n",
    "    else:\n",
    "        bm25_hid_to_tids[int(hid)] = [int(x) for x in cand_tids]\n",
    "        \n",
    "print(\"number of ignore hids = {}\".format(len(ignore_hids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "search_sequential.train.json: 100%|██████████| 215338/215338 [00:03<00:00, 58500.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bm25 suffix: search_sequential.train.json's miss_hids = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "search_sequential.test.json: 100%|██████████| 215338/215338 [00:00<00:00, 219833.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bm25 suffix: search_sequential.test.json's miss_hids = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "search_sequential.train.json: 100%|██████████| 215338/215338 [00:04<00:00, 48351.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bm25 suffix: search_sequential.train.json's miss_hids = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "search_sequential.test.json: 100%|██████████| 215338/215338 [00:01<00:00, 209526.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bm25 suffix: search_sequential.test.json's miss_hids = 0\n"
     ]
    }
   ],
   "source": [
    "import ujson\n",
    "from tqdm import tqdm \n",
    "import random\n",
    "\n",
    "REL_RELATION = \"is_relevant_to\"\n",
    "\n",
    "out_dir = os.path.join(in_dir, \"sequential_train_test/\")\n",
    "train_search_examples, test_search_examples = [],[]\n",
    "fn_to_example = {\n",
    "    os.path.join(out_dir, \"search_sequential.train.json\"): train_search_examples,\n",
    "    os.path.join(out_dir, \"search_sequential.test.json\"): test_search_examples,\n",
    "}\n",
    "\n",
    "for fn, data_examples in fn_to_example.items():\n",
    "    with open(fn) as fin:\n",
    "        for line in fin:\n",
    "            data_examples.append(ujson.loads(line))\n",
    "            \n",
    "miss_hids = set()\n",
    "\n",
    "suffix_to_examples = {\n",
    "    \"search_sequential.train.json\": (train_search_examples,REL_RELATION),\n",
    "    \"search_sequential.test.json\": (test_search_examples,REL_RELATION) ,\n",
    "}\n",
    "\n",
    "history_lengths = [4,8]\n",
    "for hist_len in history_lengths:\n",
    "    for dest_signature in [\"hlen_{}_bm25\".format(hist_len)]:\n",
    "        dest_dir = os.path.join(out_dir, dest_signature)\n",
    "        if not os.path.exists(dest_dir):\n",
    "            os.mkdir(dest_dir)\n",
    "        for suffix, (data_examples, relation) in suffix_to_examples.items():\n",
    "            dest_fn = os.path.join(dest_dir, suffix)\n",
    "            with open(dest_fn, \"w\") as fout:\n",
    "                for example in tqdm(data_examples, desc=suffix):\n",
    "                    if \"train.json\" in dest_fn:\n",
    "                        start_idx = max(0, len(example[\"query_ids\"])-hist_len)\n",
    "                    elif \"test.json\":\n",
    "                        start_idx = max(0, len(example[\"query_ids\"])-hist_len-1)\n",
    "                    else:\n",
    "                        raise ValueError(f\"{suffix} is not valid.\")\n",
    "\n",
    "                    query_ids = example[\"query_ids\"][start_idx:]\n",
    "                    context_key_ids = example[\"context_key_ids\"][start_idx:]\n",
    "                    context_value_ids = example[\"context_value_ids\"][start_idx:]\n",
    "                    if \"train.json\" in dest_fn:\n",
    "                        target_value_ids = example[\"target_value_ids\"][start_idx:]\n",
    "                        if \"bm25\" in dest_signature:\n",
    "                            if \"search_sequential\" in suffix:\n",
    "                                neg_value_ids = create_neg_value_ids(query_ids=query_ids, \n",
    "                                                                     pos_value_ids=target_value_ids, \n",
    "                                                                     miss_qids=miss_hids, \n",
    "                                                                     sampler=bm25_hid_to_tids)\n",
    "                            else:\n",
    "                                raise ValueError(f\"suffix: {suffix} is not valid.\")             \n",
    "                        else:\n",
    "                            raise ValueError(f\"dest signature: {dest_signature} is not valid.\")\n",
    "                        dest_example = {\"uid\": example[\"uid\"], \"query_ids\": query_ids, \"context_key_ids\": context_key_ids,\n",
    "                                    \"context_value_ids\": context_value_ids, \n",
    "                                    \"target_value_ids\": target_value_ids, \"neg_value_ids\": neg_value_ids, \"relation\": relation}\n",
    "                    elif \"test.json\" in dest_fn:\n",
    "                        dest_example = {\"uid\": example[\"uid\"], \"query_ids\": query_ids, \"context_key_ids\": context_key_ids,\n",
    "                                    \"context_value_ids\": context_value_ids, \"relation\": relation}\n",
    "                    else:\n",
    "                        raise ValueError(f\"{suffix} is not valid.\")\n",
    "                    fout.write(ujson.dumps(dest_example) + \"\\n\")\n",
    "            if \"bm25\" in dest_signature:\n",
    "                if \"search_sequential\" in suffix or \"compl_rec_sequential\" in suffix:\n",
    "                    print(\"bm25 suffix: {}'s miss_hids = {}\".format(suffix, len(miss_hids)))\n",
    "\n",
    "\n",
    "dest_dir = os.path.join(out_dir, \"without_context/\")\n",
    "if not os.path.exists(dest_dir):\n",
    "    os.mkdir(dest_dir)\n",
    "fn_to_example = {\n",
    "    os.path.join(dest_dir, \"uid_queries.test.search.tsv\"): (test_search_examples, REL_RELATION)\n",
    "}\n",
    "for fn, (test_examples, relation) in fn_to_example.items():\n",
    "    with open(fn, \"w\") as fout:\n",
    "        for example in test_examples:\n",
    "            uid, query = example[\"uid\"], eid_to_text[example[\"query_ids\"][-1]]\n",
    "            fout.write(f\"{uid}\\t{query}\\t{relation}\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215338 /home/jupyter/unity_jointly_rec_and_search/datasets/amazon_review_datasets/Cell_Phones_and_Accessories/sequential_train_test/search_sequential.train.json\n",
      "{\"uid\":0,\"query_ids\":[533872,533879,533872],\"context_key_ids\":[533872,533872,533879],\"context_value_ids\":[36258,181323,273874],\"target_value_ids\":[181323,273874,317230]}\n",
      "{\"uid\":1,\"query_ids\":[533872,533871,533884,533871],\"context_key_ids\":[533883,533872,533871,533884],\"context_value_ids\":[29109,123568,266605,60890],\"target_value_ids\":[123568,266605,60890,193511]}\n",
      "====================================================================================================\n",
      "215338 /home/jupyter/unity_jointly_rec_and_search/datasets/amazon_review_datasets/Cell_Phones_and_Accessories/sequential_train_test/urels.search.test.tsv\n",
      "0\tQ0\t306281\t1\n",
      "1\tQ0\t50876\t1\n",
      "====================================================================================================\n",
      "215338 /home/jupyter/unity_jointly_rec_and_search/datasets/amazon_review_datasets/Cell_Phones_and_Accessories/sequential_train_test/search_sequential.test.json\n",
      "{\"uid\":0,\"query_ids\":[533872,533879,533872,533872],\"context_key_ids\":[533872,533872,533879,533872],\"context_value_ids\":[36258,181323,273874,317230]}\n",
      "{\"uid\":1,\"query_ids\":[533872,533871,533884,533871,533871],\"context_key_ids\":[533883,533872,533871,533884,533871],\"context_value_ids\":[29109,123568,266605,60890,193511]}\n",
      "====================================================================================================\n",
      "=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.=.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ncheck_dir = os.path.join(out_dir, \"without_context\")\\n\\nfor fn in os.listdir(check_dir):\\n    fn = os.path.join(check_dir, fn)\\n    \\n    ! wc -l $fn \\n    ! head -n 2 $fn\\n    print(100*\"=\")\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_dir = os.path.join(out_dir, \"hlen_4_bm25\")\n",
    "\"\"\"\n",
    "for fn in os.listdir(check_dir):\n",
    "    fn = os.path.join(check_dir, fn)\n",
    "    if \".test.json\" in fn:\n",
    "        continue\n",
    "    if \"search_sequential\" not in fn:\n",
    "        continue\n",
    "    ! wc -l $fn \n",
    "    ! head -n 2 $fn\n",
    "    print(100*\"=\")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for fn in os.listdir(out_dir):\n",
    "    if \"search\" not in fn or \"small\" in fn:\n",
    "        continue \n",
    "    fn = os.path.join(out_dir, fn)\n",
    "    \n",
    "    ! wc -l $fn \n",
    "    ! head -n 2 $fn\n",
    "    print(100*\"=\")\n",
    "\n",
    "print(100*\"=.\")\n",
    "\"\"\"\n",
    "check_dir = os.path.join(out_dir, \"without_context\")\n",
    "\n",
    "for fn in os.listdir(check_dir):\n",
    "    fn = os.path.join(check_dir, fn)\n",
    "    \n",
    "    ! wc -l $fn \n",
    "    ! head -n 2 $fn\n",
    "    print(100*\"=\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uid': 148, 'query_ids': [533888, 533871, 533891, 533874, 533879, 533875, 533879, 533879, 533893, 533882, 533904, 533902, 533882, 533879, 533894, 533894], 'context_key_ids': [533888, 533888, 533871, 533891, 533874, 533879, 533875, 533879, 533879, 533893, 533882, 533904, 533902, 533882, 533879, 533894], 'context_value_ids': [192667, 216313, 220932, 470258, 386194, 435309, 435610, 396247, 426200, 488058, 424134, 503340, 295929, 320473, 499373, 375161]}\n",
      "query: query: cell phones accessories cases holsters sleeves\n",
      "target_item: product: iPhone 5S Case, Caseology [Fusion Series] Scratch-Resistant Clearback Cover [Black] [Dual Bumper] for Apple iPhone 5S / 5 (2013) &amp; iPhone SE (2016) - Black ; Best brand for cases! This case is not only nice looking but it does what it says it does!! I had it for a year and my phone is still the same as when I put the case on. I have drop it so many times from different heights and different surfaces and my phone was always protected!! It is so nice that I bou the same brand for my iPhone 6 plus!!\n",
      "neg_item: product: Rottweiler Basket Dog Muzzle Playful Samsung Galaxy S5 Sm-G900 Flip Cover Case With Card Holder Customized Made To Order Support Ready Premium Deluxe Pu Leather 5 13/16 Inch (148mm) X 2 1/8 Inch (80mm) X 5/8 Inch (16mm) Liil S V S 5 Professional Cases Accessories Open Camera Headphone Port Lcd Graphic Background Covers Designed Model Folio Sleeve Hd Template Designed Wallpaper Photo Jacket Protector Micro Sd Wireless Cellphone Cell Phone ; No problems with this one.\n",
      "\n",
      "===========================================================================\n",
      "query: query: cell phones accessories screen protectors\n",
      "target_item: product: Generic Diamond Finishing Screen Protector for iPhone 4/4s, 1-Pack - Non-Retail Packaging - Clear ; Worked as described\n",
      "neg_item: product: Green Phone Case Cover Cell Phone Accessory + Yellow Pry Tool + Screen Protector + Stylus Pen + EXTREME Band for EXTREME Band for Alcatel One Touch M'Pop M POP OT-5020 OT5020 ; she liked the color\n",
      "\n",
      "===========================================================================\n",
      "query: query: cell phones accessories cases holsters sleeves\n",
      "target_item: product: MyBat SAMSUNG Galaxy Note 4 TUFF Hybrid Phone Protector Cover with Stand - Retail Packaging - Black ; These cases are just as good as otterbox, for a fraction of the price. I had one on my galaxy s2, my note 3, and now my note 4. I routinely drop this poor thing, but have yet to do any damage with this case on. I usually buy a couple, since they usually don't carry my particular favorite color combination... But at under $10 a piece, I can get several and truly customize the look.\n",
      "neg_item: product: New York Brooklyn Bridge Night Time Samsung Galaxy S4 Flip Cover Case with Card Holder Customized Made to Order Support Ready Premium Deluxe Pu Leather 5 1/2 inch (140mm) x 3 1/4 inch (80mm) x 9/16 inch (14mm) MSD S IV S 4 Professional Cases Accessories Open Camera Headphone Port I9500 LCD Graphic Background Covers Designed Model Folio Sleeve HD Template Designed Wallpaper Photo Jacket Wifi 16gb 32gb 64gb Luxury Protector Micro SD Wireless Cellphone Cell Phone\n",
      "\n",
      "===========================================================================\n",
      "query: query: cell phones accessories cases holsters sleeves\n",
      "target_item: product: MyBat SAMSUNG Galaxy Note 4 TUFF Hybrid Phone Protector Cover - Retail Packaging - Ivory/Teal/White\n",
      "neg_item: product: Call Capture Mobile Cell Booster for Analog, PCS Phones\n",
      "\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "uid = 148\n",
    "for example in test_search_examples:\n",
    "    if example[\"uid\"] == uid:\n",
    "        print(example)\n",
    "query_ids = [533872,533879,533872,533872]\n",
    "target_vids = [181323,273874,317230, 306281]\n",
    "neg_vids = [241366,207444,185388, 2222]\n",
    "\n",
    "for qid, tvid, nvid in zip(query_ids, target_vids, neg_vids):\n",
    "    print(\"query: {}\\ntarget_item: {}\\nneg_item: {}\\n\".format(eid_to_text[qid], eid_to_text[tvid], eid_to_text[nvid]))\n",
    "    print(75*\"=\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-9.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9c9569641937ce4addf5496cbe81a89e9d276a9857e7a9967fd6589fdce30733"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
